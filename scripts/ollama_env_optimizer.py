#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–∞—Å—Ç—Ä–æ–µ–∫ Ollama —Å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –≤ .env —Ñ–∞–π–ª
"""

import sys
from pathlib import Path


def estimate_model_layers(model_size_gb):
    """–û—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–ª–æ–µ–≤ –≤ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–∞–∑–º–µ—Ä–∞"""
    if model_size_gb <= 1:
        return 12
    elif model_size_gb <= 4:
        return 24
    elif model_size_gb <= 8:
        return 32
    elif model_size_gb <= 16:
        return 40
    elif model_size_gb <= 32:
        return 48
    else:
        return 60


def calculate_context_overhead(context_size):
    """–†–∞—Å—á–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø–∞–º—è—Ç–∏ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
    return (context_size / 1024) * 0.01


def optimize_ollama_config(model_size_gb, available_vram_gb, ram_gb=None):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º –≤—Å–µ—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤"""
    total_layers = estimate_model_layers(model_size_gb)
    usable_vram = available_vram_gb - 2.0

    if usable_vram <= 0:
        usable_vram = available_vram_gb * 0.5

    # –ü–æ–ª–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –≤ GPU
    if model_size_gb <= usable_vram * 0.75:
        context_size = 16384 if usable_vram > 10 else 8192
        context_overhead = calculate_context_overhead(context_size)

        if model_size_gb + context_overhead <= usable_vram:
            return {
                'gpu_layers': -1,
                'context_size': context_size,
                'strategy': 'full_gpu',
                'num_gpu': 1,
                'cpu_threads': 8,
                'keep_alive': '30m',
                'low_vram': False,
                'flash_attention': True,
                'numa': False,
                'parallel': 1,
                'max_loaded': 1
            }

    # –ì–∏–±—Ä–∏–¥–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
    if model_size_gb <= usable_vram * 1.8:
        available_for_layers = usable_vram * 0.8
        layers_ratio = available_for_layers / model_size_gb
        gpu_layers = max(5, int(total_layers * layers_ratio))
        context_size = 8192 if usable_vram > 6 else 4096

        return {
            'gpu_layers': min(gpu_layers, total_layers - 2),
            'context_size': context_size,
            'strategy': 'hybrid',
            'num_gpu': 1,
            'cpu_threads': 12,
            'keep_alive': '15m',
            'low_vram': True,
            'flash_attention': False,
            'numa': True,
            'parallel': 1,
            'max_loaded': 1
        }

    # CPU-—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞
    gpu_layers = min(8, max(2, int(usable_vram / model_size_gb * total_layers)))
    context_size = 4096 if ram_gb and ram_gb >= 16 else 2048

    return {
        'gpu_layers': gpu_layers,
        'context_size': context_size,
        'strategy': 'cpu_focused',
        'num_gpu': 0,
        'cpu_threads': 16,
        'keep_alive': '5m',
        'low_vram': True,
        'flash_attention': False,
        'numa': True,
        'parallel': 1,
        'max_loaded': 1
    }


def find_env_file():
    """–ü–æ–∏—Å–∫ .env —Ñ–∞–π–ª–∞ –≤ —Ç–µ–∫—É—â–µ–π –∏ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö –ø–∞–ø–∫–∞—Ö"""
    current_path = Path.cwd()

    # –ò—â–µ–º .env –≤ —Ç–µ–∫—É—â–µ–π –ø–∞–ø–∫–µ –∏ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö
    for path in [current_path] + list(current_path.parents):
        env_file = path / '.env'
        if env_file.exists():
            return env_file

    # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –≤ —Ç–µ–∫—É—â–µ–π –ø–∞–ø–∫–µ
    return current_path / '.env'


def read_env_file(env_path):
    """–ß–∏—Ç–∞–µ—Ç .env —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫"""
    if env_path.exists():
        with open(env_path, 'r', encoding='utf-8') as f:
            return f.readlines()
    return []


def update_env_file(env_path, config):
    """–û–±–Ω–æ–≤–ª—è–µ—Ç .env —Ñ–∞–π–ª —Å –Ω–æ–≤—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ Ollama"""
    lines = read_env_file(env_path)

    # –°–æ–∑–¥–∞–µ–º –∫–∞—Ä—Ç—É –Ω–∞—Å—Ç—Ä–æ–µ–∫ Ollama
    ollama_settings = {
        'OLLAMA_USE_PARAMS': 'true',
        'OLLAMA_GPU_LAYERS': str(config['gpu_layers']),
        'OLLAMA_CONTEXT_SIZE': str(config['context_size']),
        'OLLAMA_NUM_GPU': str(config['num_gpu']),
        'OLLAMA_CPU_THREADS': str(config['cpu_threads']),
        'OLLAMA_KEEP_ALIVE': config['keep_alive'],
        'OLLAMA_LOW_VRAM': str(config['low_vram']).lower(),
        'OLLAMA_FLASH_ATTENTION': str(config['flash_attention']).lower(),
        'OLLAMA_USE_NUMA': str(config['numa']).lower(),
        'OLLAMA_NUM_PARALLEL': str(config['parallel']),
        'OLLAMA_MAX_LOADED_MODELS': str(config['max_loaded'])
    }

    # –ù–∞—Ö–æ–¥–∏–º —Å–µ–∫—Ü–∏—é Ollama –∏–ª–∏ —Å–æ–∑–¥–∞–µ–º –µ—ë
    ollama_section_start = -1
    ollama_section_end = -1

    for i, line in enumerate(lines):
        if '=== –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø OLLAMA ===' in line or 'OLLAMA_USE_PARAMS' in line:
            ollama_section_start = i
        if ollama_section_start != -1 and line.strip() == '' and i > ollama_section_start + 1:
            ollama_section_end = i
            break

    # –ï—Å–ª–∏ —Å–µ–∫—Ü–∏—è –Ω–∞–π–¥–µ–Ω–∞, –æ–±–Ω–æ–≤–ª—è–µ–º –µ—ë
    if ollama_section_start != -1:
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        updated_vars = set()

        for i in range(ollama_section_start, len(lines)):
            line = lines[i].strip()
            if '=' in line and not line.startswith('#'):
                var_name = line.split('=')[0].strip()
                if var_name in ollama_settings:
                    lines[i] = f"{var_name}={ollama_settings[var_name]}\n"
                    updated_vars.add(var_name)

            # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º, –µ—Å–ª–∏ –¥–æ—Å—Ç–∏–≥–ª–∏ –∫–æ–Ω—Ü–∞ —Å–µ–∫—Ü–∏–∏
            if line == '' and i > ollama_section_start + 3:
                ollama_section_end = i
                break

        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        missing_vars = set(ollama_settings.keys()) - updated_vars
        if missing_vars:
            insert_pos = ollama_section_end if ollama_section_end != -1 else len(lines)
            for var_name in missing_vars:
                lines.insert(insert_pos, f"{var_name}={ollama_settings[var_name]}\n")
                insert_pos += 1

    else:
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å–µ–∫—Ü–∏—é Ollama –≤ –∫–æ–Ω—Ü–µ —Ñ–∞–π–ª–∞
        if lines and not lines[-1].endswith('\n'):
            lines.append('\n')

        lines.append('\n')
        lines.append('# === –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò –°–ì–ï–ù–ï–†–ò–†–û–í–ê–ù–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò OLLAMA ===\n')
        lines.append('OLLAMA_USE_PARAMS=true\n')

        for var_name, value in ollama_settings.items():
            if var_name != 'OLLAMA_USE_PARAMS':  # –£–∂–µ –¥–æ–±–∞–≤–ª–µ–Ω–æ –≤—ã—à–µ
                lines.append(f"{var_name}={value}\n")

        lines.append('\n')

    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
    with open(env_path, 'w', encoding='utf-8') as f:
        f.writelines(lines)

    return ollama_settings


def generate_detailed_config(model_size_gb, available_vram_gb, ram_gb=None, model_name="unknown"):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏"""
    config = optimize_ollama_config(model_size_gb, available_vram_gb, ram_gb)

    output = []
    output.append("=" * 70)
    output.append(f"–û–ü–¢–ò–ú–ê–õ–¨–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –î–õ–Ø –ú–û–î–ï–õ–ò: {model_name}")
    output.append("=" * 70)
    output.append(f"–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: {model_size_gb:.1f} –ì–ë")
    output.append(f"–î–æ—Å—Ç—É–ø–Ω–∞—è VRAM: {available_vram_gb:.1f} –ì–ë")
    if ram_gb:
        output.append(f"–î–æ—Å—Ç—É–ø–Ω–∞—è RAM: {ram_gb:.1f} –ì–ë")
    output.append("")

    # –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    output.append("–û–°–ù–û–í–ù–´–ï –ü–ê–†–ê–ú–ï–¢–†–´:")
    output.append(f"‚îú‚îÄ –°—Ç—Ä–∞—Ç–µ–≥–∏—è –∑–∞–≥—Ä—É–∑–∫–∏: {config['strategy'].upper()}")
    output.append(f"‚îú‚îÄ –°–ª–æ–µ–≤ –≤ GPU: {config['gpu_layers']}")
    output.append(f"‚îú‚îÄ –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {config['context_size']:,}")
    output.append(f"‚îú‚îÄ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU: {config['num_gpu']}")
    output.append(f"‚îú‚îÄ –ü–æ—Ç–æ–∫–æ–≤ CPU: {config['cpu_threads']}")
    output.append(f"‚îî‚îÄ Keep Alive: {config['keep_alive']}")
    output.append("")

    # ENV –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
    output.append("–ü–ï–†–ï–ú–ï–ù–ù–´–ï .ENV (–ë–£–î–£–¢ –û–ë–ù–û–í–õ–ï–ù–´):")
    output.append("```")
    output.append("OLLAMA_USE_PARAMS=true")
    output.append(f"OLLAMA_GPU_LAYERS={config['gpu_layers']}")
    output.append(f"OLLAMA_CONTEXT_SIZE={config['context_size']}")
    output.append(f"OLLAMA_NUM_GPU={config['num_gpu']}")
    output.append(f"OLLAMA_CPU_THREADS={config['cpu_threads']}")
    output.append(f"OLLAMA_KEEP_ALIVE={config['keep_alive']}")
    output.append(f"OLLAMA_LOW_VRAM={str(config['low_vram']).lower()}")
    output.append(f"OLLAMA_FLASH_ATTENTION={str(config['flash_attention']).lower()}")
    output.append(f"OLLAMA_USE_NUMA={str(config['numa']).lower()}")
    output.append(f"OLLAMA_NUM_PARALLEL={config['parallel']}")
    output.append(f"OLLAMA_MAX_LOADED_MODELS={config['max_loaded']}")
    output.append("```")
    output.append("")

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    output.append("–†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–ò:")

    if config['strategy'] == 'full_gpu':
        output.append("‚úÖ –ú–æ–¥–µ–ª—å –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ VRAM - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å")
        output.append("‚úÖ –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª—å—à–æ–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á")
        output.append("‚ö†Ô∏è  –°–ª–µ–¥–∏—Ç–µ –∑–∞ —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–æ–π GPU –ø—Ä–∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç–µ")
    elif config['strategy'] == 'hybrid':
        output.append("‚ö° –ì–∏–±—Ä–∏–¥–Ω—ã–π —Ä–µ–∂–∏–º - —Ö–æ—Ä–æ—à–∏–π –±–∞–ª–∞–Ω—Å —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –ø–∞–º—è—Ç–∏")
        output.append("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–≤–µ–ª–∏—á–∏—Ç—å gpu_layers, –µ—Å–ª–∏ –µ—Å—Ç—å —Å–≤–æ–±–æ–¥–Ω–∞—è VRAM")
        output.append("üí° –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —É–º–µ–Ω—å—à–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è")
    else:
        output.append("üêå CPU-—Ä–µ–∂–∏–º - –º–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –±–æ–ª—å—à–∏–º–∏ –º–æ–¥–µ–ª—è–º–∏")
        output.append("üí° –£–±–µ–¥–∏—Ç–µ—Å—å –≤ –Ω–∞–ª–∏—á–∏–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –æ–±—ä–µ–º–∞ RAM")
        output.append("üí° –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ SSD –¥–ª—è swap, –µ—Å–ª–∏ RAM –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ")
        output.append("üí° –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é –º–æ–¥–µ–ª–∏ (Q4_K_M, Q5_K_M)")

    output.append("")
    output.append("=" * 70)

    return "\n".join(output), config


def interactive_script():
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≤–≤–æ–¥–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è .env"""
    print("üîß –û–ü–¢–ò–ú–ò–ó–ê–¢–û–† –ù–ê–°–¢–†–û–ï–ö OLLAMA –° –ò–ù–¢–ï–ì–†–ê–¶–ò–ï–ô .ENV")
    print("=" * 55)

    try:
        # –í–≤–æ–¥ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏
        print("\nüìè –í–≤–µ–¥–∏—Ç–µ —Ä–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏:")
        print("–ü—Ä–∏–º–µ—Ä—ã: 3.8 (–¥–ª—è 4B –º–æ–¥–µ–ª–∏), 7.2 (–¥–ª—è 7B), 13.5 (–¥–ª—è 13B)")
        model_size_input = input("–†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏ –≤ –ì–ë: ").strip()
        model_size_gb = float(model_size_input)

        # –í–≤–æ–¥ –≤–∏–¥–µ–æ–ø–∞–º—è—Ç–∏
        print("\nüéÆ –í–≤–µ–¥–∏—Ç–µ –æ–±—ä–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ–π –≤–∏–¥–µ–æ–ø–∞–º—è—Ç–∏:")
        print("–ü—Ä–∏–º–µ—Ä—ã: 8 (RTX 3070), 12 (RTX 4070 Ti), 24 (RTX 4090)")
        vram_input = input("–í–∏–¥–µ–æ–ø–∞–º—è—Ç—å –≤ –ì–ë: ").strip()
        available_vram_gb = float(vram_input)

        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –≤–≤–æ–¥ RAM
        print("\nüíæ –í–≤–µ–¥–∏—Ç–µ –æ–±—ä–µ–º RAM (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, Enter –¥–ª—è –ø—Ä–æ–ø—É—Å–∫–∞):")
        ram_input = input("RAM –≤ –ì–ë: ").strip()
        ram_gb = float(ram_input) if ram_input else None

        # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        print("\nüè∑Ô∏è  –í–≤–µ–¥–∏—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ):")
        model_name = input("–ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: ").strip()
        if not model_name:
            model_name = "unknown"

        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        print("\n‚öôÔ∏è  –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏...")
        config_output, config = generate_detailed_config(model_size_gb, available_vram_gb, ram_gb, model_name)

        print("\n" + config_output)

        # –ü–æ–∏—Å–∫ .env —Ñ–∞–π–ª–∞
        env_file = find_env_file()
        print(f"\nüìÑ –ù–∞–π–¥–µ–Ω .env —Ñ–∞–π–ª: {env_file}")

        # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –æ–±–Ω–æ–≤–∏—Ç—å .env
        print(f"\nüîÑ –û–±–Ω–æ–≤–∏—Ç—å —Ñ–∞–π–ª {env_file} —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏?")
        print("   –≠—Ç–æ –∏–∑–º–µ–Ω–∏—Ç —Å–µ–∫—Ü–∏—é OLLAMA –≤ –≤–∞—à–µ–º .env —Ñ–∞–π–ª–µ")
        update_env = input("–û–±–Ω–æ–≤–∏—Ç—å .env? (y/N): ").lower().strip().startswith('y')

        if update_env:
            try:
                updated_settings = update_env_file(env_file, config)
                print(f"\n‚úÖ –§–∞–π–ª {env_file} —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω!")
                print("\nüìù –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ:")
                for key, value in updated_settings.items():
                    print(f"   {key}={value}")

                print(f"\nüöÄ –¢–µ–ø–µ—Ä—å –≤–∞—à OllamaClient –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏!")
                print(f"üí° –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –≤–∞—à–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π.")

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ .env —Ñ–∞–π–ª–∞: {e}")
        else:
            print("‚è≠Ô∏è  .env —Ñ–∞–π–ª –Ω–µ –∏–∑–º–µ–Ω–µ–Ω")

        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ —Ñ–∞–π–ª
        print("\nüíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—Ç—á–µ—Ç –æ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –≤ —Ñ–∞–π–ª? (y/N): ", end="")
        save_input = input().lower().strip()
        save_to_file = save_input.startswith('y')

        if save_to_file:
            safe_model_name = model_name.replace(':', '_').replace('/', '_').replace(' ', '_')
            filename = f"ollama_config_{safe_model_name}.txt"

            try:
                with open(filename, 'w', encoding='utf-8') as f:
                    f.write(config_output)
                print(f"‚úÖ –û—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {filename}")
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ñ–∞–π–ª–∞: {e}")

    except ValueError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤–≤–æ–¥–∞: {e}")
        print("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–æ–¥–∏—Ç–µ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è.")
        return False
    except KeyboardInterrupt:
        print("\nüëã –í—ã—Ö–æ–¥ –∏–∑ –ø—Ä–æ–≥—Ä–∞–º–º—ã.")
        return False
    except Exception as e:
        print(f"‚ùå –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        return False

    return True


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã"""
    try:
        interactive_script()
    except Exception as e:
        print(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
