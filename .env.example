# ===================================================================
#             ЕДИНЫЙ ФАЙЛ КОНФИГУРАЦИИ ДЛЯ AI-REASONING-LAB
# ===================================================================

# --- ЭТАП 1: ПОДГОТОВКА МОДЕЛЕЙ (для скрипта create_model.py) ---
# Сначала запустите `python scripts/create_model.py`, чтобы скачать
# и создать указанные ниже модели в Ollama.

# --- Модель №0 для слабых ПК ---
PREPARE_MODEL_0_ENABLED="true"
PREPARE_MODEL_0_NAME="jan-long-q4" # Это имя мы будем использовать в тестах ниже
PREPARE_MODEL_0_GGUF_URL="https://huggingface.co/janhq/jan-v1-4b-GGUF/resolve/main/jan-v1-4b.Q4_K_M.gguf"
PREPARE_MODEL_0_PARAMS_NUM_CTX="262144"
PREPARE_MODEL_0_PARAMS_TEMPERATURE="0.1"
PREPARE_MODEL_0_SYSTEM_PROMPT="Ты — точный и педантичный ассистент по извлечению информации..."

# --- Модель №1 для мощных ПК (Windows/Linux) ---
PREPARE_MODEL_1_ENABLED="true"
PREPARE_MODEL_1_NAME="jan-long-q8" # Это имя мы будем использовать в тестах ниже
PREPARE_MODEL_1_GGUF_URL="https://huggingface.co/janhq/jan-v1-4b-GGUF/resolve/main/jan-v1-4b.Q8_0.gguf"
PREPARE_MODEL_1_PARAMS_NUM_CTX="262144"
PREPARE_MODEL_1_PARAMS_TEMPERATURE="0.1"
PREPARE_MODEL_1_SYSTEM_PROMPT="Ты — точный и педантичный ассистент по извлечению информации..."

GEMINI_API_KEY=""
# ===================================================================
#          ЭТАП 2: ТЕСТИРОВАНИЕ (для run_baselogic_benchmark.py)
# ===================================================================
# Этот раздел использует модели, подготовленные на ЭТАПЕ 1.

# --- Общие параметры тестирования ---
# --- Общие параметры тестирования ---
BC_RUNS_PER_TEST=2
BC_SHOW_PAYLOAD=false
BC_RUNS_RAW_SAVE="false" # true/false Сохранять результаты или нет

# --- Набор тестов для запуска ---
BC_TESTS_TO_RUN=["t15_multi_hop_reasoning", "t16_counterfactual_reasoning", "t17_proof_verification", "t18_constrained_optimization"]
# Все доступные тесты:
# Базовые: t01_simple_logic, t02_instructions, t03_code_gen, t04_data_extraction, t05_summarization, t06_mathematics
# Продвинутые: t07_accuracy_ideal, t08_accuracy_flawed, t09_verbosity_ideal, t10_verbosity_verbose, t11_positional_first, t12_positional_second
# Экспертные: t13_grandmaster_judge_evaluator, t14_image_recognition
# Комплексный код: t_neural_labyrinth
# Новые расширенные: t15_multi_hop_reasoning, t16_counterfactual_reasoning, t17_proof_verification, t18_constrained_optimization
# Плагины: custom_logic, t_context_stress, t_base_check, t_text_classifier, t_support_request_classifier, t_complex_code_problem, t_context_stress_advanced,t_mvc_complex_code_problem

# --- Настройки логирования ---
BC_LOGGING_LEVEL="INFO"
BC_LOGGING_FORMAT="DETAILED"
BC_LOGGING_DIRECTORY="logs"

# --- Список моделей для тестирования ---
# --- Модель №0 ---
BC_MODELS_0_NAME="jan-long-q4"
BC_MODELS_0_CLIENT_TYPE="ollama" # ollama, lmstudio, jan, openai_compatible
BC_MODELS_0_API_BASE="http://127.0.0.1:11434/v1" # ИЗМЕНИТЕ для openai_compatible

# Общие опции, которые читает TestRunner/Adapter
BC_MODELS_0_OPTIONS_QUERY_TIMEOUT="600" # Таймаут на весь запрос
BC_MODELS_0_INFERENCE_STREAM="false"    # true/false потоковый режим
BC_MODELS_0_INFERENCE_THINK="true"      # Включить/выключить режим chain_of_thought <think>

# Опции, которые передаются напрямую в API модели
BC_MODELS_0_PROMPTING_SYSTEM_PROMPT="Ты — точный и педантичный ассистент..."
BC_MODELS_0_GENERATION_TEMPERATURE="0.1"
BC_MODELS_0_GENERATION_NUM_CTX="4096"     # Устанавливаем размер контекста
BC_MODELS_0_GENERATION_MAX_TOKENS="2500"
BC_MODELS_0_GENERATION_TOP_P="0.9"
BC_MODELS_0_GENERATION_REPEAT_PENALTY="1.1"
BC_MODELS_0_GENERATION_NUM_GPU="1"        # Указываем, что использовать 1 GPU
BC_MODELS_0_GENERATION_NUM_THREAD="16"     # Указываем количество потоков CPU
BC_MODELS_0_GENERATION_MAX_TOKENS="512"
# Опционально: используйте стоп-токены, если уверены, что сервер их поддерживает
# BC_MODELS_0_GENERATION_STOP="<|im_end|>" # Пример одного стоп-токена в виде строки

# --- Стресс-тест контекста (для плагина t_context_stress) ---
CST_CONTEXT_LENGTHS_K="8,16,32,64,128,256,512,1024" # BC_RUNS_PER_TEST ставится CST_CONTEXT_LENGTHS_K * CST_NEEDLE_DEPTH_PERCENTAGES
CST_NEEDLE_DEPTH_PERCENTAGES="10,50,90"

OLLAMA_USE_PARAMS=true #  true/false использовать параметры из env
# === ОПТИМИЗАЦИЯ OLLAMA === НЕ УБИРАТЬ!!!
OLLAMA_NUM_PARALLEL=1
OLLAMA_MAX_LOADED_MODELS=1
OLLAMA_CPU_THREADS=10
OLLAMA_FLASH_ATTENTION=false
OLLAMA_KEEP_ALIVE=5m
OLLAMA_LOW_VRAM=true
OLLAMA_NUM_GPU="0"

