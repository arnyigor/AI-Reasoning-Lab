## Техническое Задание: Платформа "Базовый Контроль" для Тестирования LLM (Уровень 1)

**Версия:** 1.0
**Дата:** 12.08.2024

### 1. Введение и Цели

**1.1. Назначение Системы:**
Разработать на языке Python автоматизированную платформу для проведения набора фундаментальных тестов ("Базовый Контроль") для оценки ключевых компетенций больших языковых моделей (LLM).

**1.2. Бизнес-цели:**
*   Создать стандартизированный "входной фильтр" для быстрой оценки новых или малоизвестных LLM.
*   Получать объективные, количественные и сравнимые метрики производительности различных моделей.
*   Сформировать основу для более сложных систем тестирования (Уровень 2 и Уровень 3).

**1.3. Технические цели:**
*   Реализовать систему, способную процедурно генерировать уникальные тестовые задания в рамках заданных категорий.
*   Обеспечить 100% автоматическую и детерминированную проверку результатов ("железобетонная верификация").
*   Создавать наглядные и машиночитаемые отчеты по итогам тестирования.

### 2. Ключевые Принципы Архитектуры

*   **Модульность:** Система должна быть разделена на независимые логические блоки: Ядро (Runner), Клиент LLM, Генераторы тестов, Верификаторы, Репортеры.
*   **Расширяемость:** Добавление новой категории тестов не должно требовать изменения ядра системы. Достаточно добавить новый файл с генератором и верификатором.
*   **Конфигурируемость:** Модели для тестирования, наборы тестов и параметры (например, количество итераций) должны задаваться через внешний конфигурационный файл.
*   **Детерминизм:** Тесты должны быть воспроизводимы. Взаимодействие с LLM должно происходить с `temperature = 0` для минимизации случайности.
*   **Изоляция:** Взаимодействие с LLM должно быть вынесено в отдельный класс-клиент, чтобы в будущем можно было легко заменить `ollama` на другой источник моделей (API, Hugging Face, и т.д.).

### 3. Структура Проекта (Файловая Система)

```
llm-benchmark-suite/
├── main.py                     # Главный исполняемый файл, оркестратор
├── config.yaml                 # Конфигурация: список моделей, тестов, параметры
├── core/
│   ├── __init__.py
│   ├── llm_client.py           # Абстракция для работы с Ollama
│   ├── test_runner.py          # Ядро, отвечающее за запуск тестов
│   ├── verifier.py             # Диспетчер верификаторов
│   └── reporter.py             # Модуль генерации отчетов
│
├── tests/
│   ├── __init__.py
│   ├── abstract_test_generator.py # Абстрактный базовый класс для всех генераторов
│   ├── t01_simple_logic.py     # Генератор и верификатор для тестов на простую логику
│   ├── t02_instructions.py     # Генератор и верификатор для тестов на следование инструкциям
│   ├── t03_code_gen.py         # Генератор и верификатор для генерации кода
│   ├── t04_data_extraction.py  # Генератор и верификатор для извлечения данных
│   ├── t05_summarization.py    # Генератор и верификатор для суммаризации
│   └── t06_mathematics.py      # Генератор и верификатор для арифмет. задач
│
└── results/
    ├── raw/                    # Сырые результаты каждого запуска в JSON
    │   └── llama3_8b_20240812_183000.json
    └── reports/                # Сгенерированные отчеты в Markdown
        └── report_20240812_183000.md
```

### 4. Определение Тестовых Категорий, Генераторов и Верификаторов

Для каждой категории должен быть реализован класс, наследуемый от `AbstractTestGenerator`, который имеет два метода:
1.  `generate() -> dict`: Создает один экземпляр теста.
2.  `verify(llm_output: str) -> bool`: Проверяет ответ модели.

| Категория | Цель | Метод генерации | Метод верификации |
| :--- | :--- | :--- | :--- |
| **1. Простая логика** | Проверка базовой дедукции | Создается шаблонная задача с 2-3 сущностями и 2-3 фактами. (`[Имя1] [свойство] [Имя2]`, `[Имя3] не [свойство]`). Генератор подставляет случайные имена/свойства и вычисляет правильный ответ. | **Жесткая:** Поиск точного совпадения имени правильного ответа в ответе модели (в нижнем регистре). |
| **2. Следование инструкциям** | Способность выполнять последовательность атомарных команд и форматировать вывод. | Генерируется случайное предложение и случайный набор из 2-4 команд (написать задом наперед, посчитать гласные, обернуть в теги `<data>`, написать в ВЕРХНЕМ РЕГИСТРЕ). Генератор сам выполняет эти команды для получения `expected_output`. | **Жесткая:** Посимвольное сравнение строки ответа модели с эталонной строкой `expected_output` после нормализации пробелов. |
| **3. Генерация кода** | Проверка знания синтаксиса языка и базовых операций. | Генерируется ТЗ на простую функцию (например, `fun max(a: Int, b: Int): Int` на Kotlin или `def is_positive(n):` на Python). Генератор также создает набор из 3-5 unit-тестов (`assert max(2,3) == 3`). | **Жесткая (для Python):** Код, сгенерированный моделью, выполняется через `exec()`. Затем к нему применяются сгенерированные unit-тесты. Успех = все тесты пройдены. |
| **4. Извлечение данных** | Распознавание стандартных сущностей (NER) в тексте. | Генерируется текст, в который вставляется случайное количество (2-5) email'ов, номеров телефонов и дат в разных форматах из заранее подготовленного списка. Генератор знает, какие именно сущности он вставил. | **Жесткая:** Из ответа модели извлекаются все сущности (например, с помощью регулярных выражений). Полученное множество сущностей сравнивается с эталонным множеством, которое хранится в сгенерированном тесте. |
| **5. Суммаризация** | Способность выделять ключевую информацию (в ограниченной форме). | **Метод "извлекающей суммаризации":** Генерируется абзац из 3-5 предложений, одно из которых помечается как ключевое. Промпт просит модель "суммировать текст, выбрав из него ОДНО наиболее важное предложение". | **Жесткая:** Проверяется, что ответ модели содержит эталонное "ключевое предложение". Можно использовать метрику семантической близости для более мягкой проверки, но для начала — проверка на точное вхождение. |
| **6. Арифметика** | Проверка способности решать простые математические задачи, соблюдая порядок операций. | Генерируется арифметическое выражение из 3-4 чисел и 2-3 операторов (`+`, `-`, `*`), включая скобки. Пример: `(A + B) * C`. Генератор вычисляет правильный ответ с помощью `eval()`. | **Жесткая:** Из ответа модели извлекается числовой результат. Сравнение числового ответа модели с эталонным. Допустима небольшая погрешность для чисел с плавающей точкой. |

### 5. Форматы Данных и Отчетов

**5.1. Конфигурационный файл (`config.yaml`)**
```yaml
models_to_test:
  - llama3:8b
  - phi3
  - janhq/Jan-v1-4B-GGUF

tests_to_run:
  - t01_simple_logic
  - t02_instructions
  - t03_code_gen
  - t04_data_extraction
  - t05_summarization
  - t06_mathematics

runs_per_test: 10 # Сколько раз генерировать задачу для каждой категории
```

**5.2. Формат сырого результата (`results/raw/*.json`)**
Это будет массив JSON-объектов, где каждый объект — результат одного запуска.
```json
[
  {
    "test_id": "t01_simple_logic_1723487400_1",
    "model_name": "llama3:8b",
    "prompt": "В комнате Анна, Борис и Вера...",
    "llm_response": "Самый высокий - Вера.",
    "expected_output": "Вера",
    "is_correct": true,
    "execution_time_ms": 1540
  },
  ...
]
```

**5.3. Формат финального отчета (`results/reports/*.md`)**
```markdown
# Отчет о Тестировании LLM

**Дата и время:** 2024-08-12 18:30:00
**Протестированные модели:** `llama3:8b`, `phi3`, `janhq/Jan-v1-4B-GGUF`

## Сводная Таблица Результатов

| Модель / Категория | Логика | Инструкции | Код | Извлечение | Суммаризация | Арифметика | **ИТОГО** |
|:-------------------|:------:|:----------:|:---:|:----------:|:------------:|:----------:|:---------:|
| **llama3:8b**      | 100%   | 90%        | 100%| 100%       | 70%          | 100%       | **93.3%** |
| **phi3**           | 100%   | 100%       | 80% | 90%        | 60%          | 90%        | **86.7%** |
| **Jan-v1-4B-GGUF** | 80%    | 70%        | 40% | 80%        | 50%          | 70%        | **65.0%** |

## Детальный Анализ (TODO)

*   **Сильные стороны модели X:** ...
*   **Типичные ошибки модели Y:** ...
```

### 6. План Реализации

1.  **Этап 1: Каркас Системы (20%)**
    *   Создать файловую структуру проекта.
    *   Реализовать `llm_client.py` с функцией для вызова модели `ollama`.
    *   Создать пустые классы-заглушки для всех генераторов и `core` модулей.
    *   Настроить `config.yaml` и чтение из него.

2.  **Этап 2: Первая Сквозная Реализация (30%)**
    *   Полностью реализовать генератор и верификатор для одной категории, например, `t06_mathematics` (самая простая для верификации).
    *   Реализовать базовую логику `test_runner.py` для запуска этого одного теста.
    *   Реализовать сохранение сырого JSON-результата.

3.  **Этап 3: Наполнение Контентом (40%)**
    *   Последовательно реализовать генераторы и верификаторы для всех остальных 5 категорий.
    *   Для каждой новой категории дорабатывать `test_runner.py` для ее поддержки.

4.  **Этап 4: Отчетность и Завершение (10%)**
    *   Реализовать `reporter.py`, который будет анализировать JSON-файлы из `results/raw/` и генерировать итоговый Markdown-отчет.
    *   Провести рефакторинг, добавить docstrings и type hints.


# Стратегия валидации AI-Reasoning-Lab платформы

## 1. Самотестирование платформы (Self-Testing)

### 1.1 Синтетические эталонные тесты
```python
# Создание тестов с заведомо известными ответами
synthetic_tests = {
    "arithmetic": {
        "question": "2 + 2 = ?",
        "expected": "4",
        "category": "exact_match"
    },
    "logic": {
        "question": "Если все A - B, и X - A, то X - B?",
        "expected": "да",
        "category": "logical_reasoning"
    }
}
```

### 1.2 Тесты корректности метрик
```python
def test_wilson_score_calculation():
    # Проверяем расчет Wilson Score на известных данных
    successes, total = 85, 100
    expected_lower = 0.766  # известное значение
    actual_lower, _ = wilson_score_interval(successes, total)
    assert abs(actual_lower - expected_lower) < 0.001
```

### 1.3 Регрессионные тесты
```python
# Создание snapshot тестов для предотвращения регрессий
def test_reporter_output_stability():
    # Фиксированные входные данные
    test_data = load_test_dataset("regression_test_v1.json")
    report = generate_report(test_data)
    
    # Сравнение с эталонным снимком
    assert report_matches_snapshot(report, "baseline_report_v1.md")
```

## 2. Внешняя валидация через API

### 2.1 Структура валидационного API
```bash
# Отправка результатов тестирования на внешний валидатор
curl -X POST https://validation-api.example.com/validate \
  -H "Content-Type: application/json" \
  -d '{
    "platform": "AI-Reasoning-Lab",
    "version": "0.2.0",
    "test_results": {
      "wilson_scores": [...],
      "accuracy_calculations": [...],
      "ranking_logic": [...]
    }
  }'
```

### 2.2 Схема валидационного запроса
```json
{
  "validation_request": {
    "timestamp": "2025-08-19T14:51:00Z",
    "platform_info": {
      "name": "AI-Reasoning-Lab",
      "version": "0.2.0",
      "base_logic_version": "1.0"
    },
    "test_data": {
      "raw_results": "results/raw/validation_test.json",
      "calculated_metrics": {
        "model_rankings": [...],
        "wilson_scores": [...],
        "trust_scores": [...]
      }
    },
    "validation_scope": [
      "metric_accuracy",
      "ranking_consistency", 
      "statistical_validity"
    ]
  }
}
```

## 3. Многослойная стратегия валидации

### 3.1 Уровень 1: Unit Testing
- Тестирование отдельных функций (wilson_score_interval, _calculate_leaderboard)
- Проверка edge cases (пустые данные, NaN значения)
- Валидация типов данных и форматирования

### 3.2 Уровень 2: Integration Testing
- Тестирование взаимодействия между компонентами
- Проверка корректности pipeline: raw data → processing → report
- Валидация сохранения и загрузки истории

### 3.3 Уровень 3: End-to-End Testing
- Полный цикл: запуск теста → сохранение результатов → генерация отчета
- Проверка консистентности между разными запусками
- Валидация пользовательского workflow

### 3.4 Уровень 4: External Validation
- Отправка результатов на внешние валидаторы
- Сравнение с другими бенчмарк платформами
- Независимая проверка статистических расчетов

## 4. Конкретные тесты для AI-Reasoning-Lab

### 4.1 Валидация Wilson Score
```python
def validate_wilson_score_externally():
    # Отправляем известные данные на внешний статистический сервис
    test_cases = [
        {"successes": 85, "total": 100},
        {"successes": 3, "total": 5},
        {"successes": 0, "total": 10}
    ]
    
    for case in test_cases:
        internal_result = wilson_score_interval(case["successes"], case["total"])
        
        # Отправка на внешний валидатор
        external_result = validate_via_api({
            "method": "wilson_score",
            "data": case,
            "confidence": 0.95
        })
        
        assert_close(internal_result, external_result, tolerance=0.001)
```

### 4.2 Валидация рейтинговой системы
```python
def validate_ranking_logic():
    # Создаем синтетические данные с известным правильным порядком
    synthetic_models = [
        {"name": "perfect_model", "accuracy": 1.0, "total_runs": 100},
        {"name": "good_model", "accuracy": 0.85, "total_runs": 100}, 
        {"name": "poor_model", "accuracy": 0.3, "total_runs": 100}
    ]
    
    ranking = calculate_leaderboard(synthetic_models)
    expected_order = ["perfect_model", "good_model", "poor_model"]
    
    assert ranking["Модель"].tolist() == expected_order
```

### 4.3 Стресс-тестирование системы данных
```python
def stress_test_data_handling():
    # Генерируем большое количество синтетических результатов
    large_dataset = generate_synthetic_results(10000)
    
    # Проверяем что система корректно обрабатывает большие объемы
    start_time = time.time()
    report = Reporter(large_dataset).generate_report()
    processing_time = time.time() - start_time
    
    # Валидация производительности и корректности
    assert processing_time < 60  # не более минуты
    assert "Trust Score" in report
    assert len(report.split("\n")) > 50  # отчет содержательный
```

## 5. Внешние валидаторы

### 5.1 Статистические сервисы
- **R Online**: Проверка статистических расчетов через R API
- **SciPy Service**: Валидация через Python научные библиотеки
- **Wolfram Alpha API**: Независимая проверка математических расчетов

### 5.2 AI-специфичные валидаторы
- **HuggingFace Evaluate**: Проверка метрик через стандартные библиотеки
- **MLflow**: Валидация экспериментального трекинга
- **Weights & Biases**: Сравнение с эталонными бенчмарками

### 5.3 Бенчмарк кросс-валидация
```python
def cross_validate_with_external_benchmarks():
    # Запускаем те же модели на других платформах
    models_to_test = ["gpt-4", "claude-2", "gemini-pro"]
    
    for model in models_to_test:
        # Результаты из AI-Reasoning-Lab  
        our_results = get_results_for_model(model)
        
        # Результаты из внешних бенчмарков
        external_results = {
            "lm_eval_harness": get_lm_eval_results(model),
            "bigbench": get_bigbench_results(model), 
            "openai_evals": get_openai_evals_results(model)
        }
        
        # Проверяем корреляцию результатов
        validate_result_consistency(our_results, external_results)
```

## 6. Автоматизированный пайплайн валидации

### 6.1 CI/CD интеграция
```yaml
# .github/workflows/validation.yml
name: Platform Validation
on: [push, pull_request]

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Run internal validation tests
        run: python -m pytest tests/validation/
      - name: Send results to external validator
        run: python scripts/external_validation.py
      - name: Compare with baseline
        run: python scripts/regression_check.py
```

### 6.2 Мониторинг валидации
- **Дашборд валидации**: Отслеживание успешности проверок
- **Алерты**: Уведомления при обнаружении расхождений
- **Трендовый анализ**: Мониторинг изменений в валидационных метриках

## 7. Рекомендуемая последовательность внедрения

1. **Фаза 1**: Создание unit и integration тестов для критичных компонентов
2. **Фаза 2**: Разработка синтетических эталонных тестов
3. **Фаза 3**: Интеграция с внешними валидационными сервисами
4. **Фаза 4**: Создание автоматизированного пайплайна валидации
5. **Фаза 5**: Непрерывный мониторинг и улучшение системы валидации

Эта многоуровневая стратегия обеспечит высокий уровень доверия к результатам AI-Reasoning-Lab платформы.