Отлично, прекрасный подход. Создание фундаментальной, автоматизированной и расширяемой тестовой платформы — это правильный путь, который принесет огромную пользу в долгосрочной перспективе.

Вот подробное Техническое Задание (ТЗ) на разработку платформы "Базовый Контроль". Оно спроектировано так, чтобы служить четким планом действий.

---

## Техническое Задание: Платформа "Базовый Контроль" для Тестирования LLM (Уровень 1)

**Версия:** 1.0
**Дата:** 12.08.2024

### 1. Введение и Цели

**1.1. Назначение Системы:**
Разработать на языке Python автоматизированную платформу для проведения набора фундаментальных тестов ("Базовый Контроль") для оценки ключевых компетенций больших языковых моделей (LLM).

**1.2. Бизнес-цели:**
*   Создать стандартизированный "входной фильтр" для быстрой оценки новых или малоизвестных LLM.
*   Получать объективные, количественные и сравнимые метрики производительности различных моделей.
*   Сформировать основу для более сложных систем тестирования (Уровень 2 и Уровень 3).

**1.3. Технические цели:**
*   Реализовать систему, способную процедурно генерировать уникальные тестовые задания в рамках заданных категорий.
*   Обеспечить 100% автоматическую и детерминированную проверку результатов ("железобетонная верификация").
*   Создавать наглядные и машиночитаемые отчеты по итогам тестирования.

### 2. Ключевые Принципы Архитектуры

*   **Модульность:** Система должна быть разделена на независимые логические блоки: Ядро (Runner), Клиент LLM, Генераторы тестов, Верификаторы, Репортеры.
*   **Расширяемость:** Добавление новой категории тестов не должно требовать изменения ядра системы. Достаточно добавить новый файл с генератором и верификатором.
*   **Конфигурируемость:** Модели для тестирования, наборы тестов и параметры (например, количество итераций) должны задаваться через внешний конфигурационный файл.
*   **Детерминизм:** Тесты должны быть воспроизводимы. Взаимодействие с LLM должно происходить с `temperature = 0` для минимизации случайности.
*   **Изоляция:** Взаимодействие с LLM должно быть вынесено в отдельный класс-клиент, чтобы в будущем можно было легко заменить `ollama` на другой источник моделей (API, Hugging Face, и т.д.).

### 3. Структура Проекта (Файловая Система)

```
llm-benchmark-suite/
├── main.py                     # Главный исполняемый файл, оркестратор
├── config.yaml                 # Конфигурация: список моделей, тестов, параметры
├── core/
│   ├── __init__.py
│   ├── llm_client.py           # Абстракция для работы с Ollama
│   ├── test_runner.py          # Ядро, отвечающее за запуск тестов
│   ├── verifier.py             # Диспетчер верификаторов
│   └── reporter.py             # Модуль генерации отчетов
│
├── tests/
│   ├── __init__.py
│   ├── abstract_test_generator.py # Абстрактный базовый класс для всех генераторов
│   ├── t01_simple_logic.py     # Генератор и верификатор для тестов на простую логику
│   ├── t02_instructions.py     # Генератор и верификатор для тестов на следование инструкциям
│   ├── t03_code_gen.py         # Генератор и верификатор для генерации кода
│   ├── t04_data_extraction.py  # Генератор и верификатор для извлечения данных
│   ├── t05_summarization.py    # Генератор и верификатор для суммаризации
│   └── t06_mathematics.py      # Генератор и верификатор для арифмет. задач
│
└── results/
    ├── raw/                    # Сырые результаты каждого запуска в JSON
    │   └── llama3_8b_20240812_183000.json
    └── reports/                # Сгенерированные отчеты в Markdown
        └── report_20240812_183000.md
```

### 4. Определение Тестовых Категорий, Генераторов и Верификаторов

Для каждой категории должен быть реализован класс, наследуемый от `AbstractTestGenerator`, который имеет два метода:
1.  `generate() -> dict`: Создает один экземпляр теста.
2.  `verify(llm_output: str) -> bool`: Проверяет ответ модели.

| Категория | Цель | Метод генерации | Метод верификации |
| :--- | :--- | :--- | :--- |
| **1. Простая логика** | Проверка базовой дедукции | Создается шаблонная задача с 2-3 сущностями и 2-3 фактами. (`[Имя1] [свойство] [Имя2]`, `[Имя3] не [свойство]`). Генератор подставляет случайные имена/свойства и вычисляет правильный ответ. | **Жесткая:** Поиск точного совпадения имени правильного ответа в ответе модели (в нижнем регистре). |
| **2. Следование инструкциям** | Способность выполнять последовательность атомарных команд и форматировать вывод. | Генерируется случайное предложение и случайный набор из 2-4 команд (написать задом наперед, посчитать гласные, обернуть в теги `<data>`, написать в ВЕРХНЕМ РЕГИСТРЕ). Генератор сам выполняет эти команды для получения `expected_output`. | **Жесткая:** Посимвольное сравнение строки ответа модели с эталонной строкой `expected_output` после нормализации пробелов. |
| **3. Генерация кода** | Проверка знания синтаксиса языка и базовых операций. | Генерируется ТЗ на простую функцию (например, `fun max(a: Int, b: Int): Int` на Kotlin или `def is_positive(n):` на Python). Генератор также создает набор из 3-5 unit-тестов (`assert max(2,3) == 3`). | **Жесткая (для Python):** Код, сгенерированный моделью, выполняется через `exec()`. Затем к нему применяются сгенерированные unit-тесты. Успех = все тесты пройдены. |
| **4. Извлечение данных** | Распознавание стандартных сущностей (NER) в тексте. | Генерируется текст, в который вставляется случайное количество (2-5) email'ов, номеров телефонов и дат в разных форматах из заранее подготовленного списка. Генератор знает, какие именно сущности он вставил. | **Жесткая:** Из ответа модели извлекаются все сущности (например, с помощью регулярных выражений). Полученное множество сущностей сравнивается с эталонным множеством, которое хранится в сгенерированном тесте. |
| **5. Суммаризация** | Способность выделять ключевую информацию (в ограниченной форме). | **Метод "извлекающей суммаризации":** Генерируется абзац из 3-5 предложений, одно из которых помечается как ключевое. Промпт просит модель "суммировать текст, выбрав из него ОДНО наиболее важное предложение". | **Жесткая:** Проверяется, что ответ модели содержит эталонное "ключевое предложение". Можно использовать метрику семантической близости для более мягкой проверки, но для начала — проверка на точное вхождение. |
| **6. Арифметика** | Проверка способности решать простые математические задачи, соблюдая порядок операций. | Генерируется арифметическое выражение из 3-4 чисел и 2-3 операторов (`+`, `-`, `*`), включая скобки. Пример: `(A + B) * C`. Генератор вычисляет правильный ответ с помощью `eval()`. | **Жесткая:** Из ответа модели извлекается числовой результат. Сравнение числового ответа модели с эталонным. Допустима небольшая погрешность для чисел с плавающей точкой. |

### 5. Форматы Данных и Отчетов

**5.1. Конфигурационный файл (`config.yaml`)**
```yaml
models_to_test:
  - llama3:8b
  - phi3
  - janhq/Jan-v1-4B-GGUF

tests_to_run:
  - t01_simple_logic
  - t02_instructions
  - t03_code_gen
  - t04_data_extraction
  - t05_summarization
  - t06_mathematics

runs_per_test: 10 # Сколько раз генерировать задачу для каждой категории
```

**5.2. Формат сырого результата (`results/raw/*.json`)**
Это будет массив JSON-объектов, где каждый объект — результат одного запуска.
```json
[
  {
    "test_id": "t01_simple_logic_1723487400_1",
    "model_name": "llama3:8b",
    "prompt": "В комнате Анна, Борис и Вера...",
    "llm_response": "Самый высокий - Вера.",
    "expected_output": "Вера",
    "is_correct": true,
    "execution_time_ms": 1540
  },
  ...
]
```

**5.3. Формат финального отчета (`results/reports/*.md`)**
```markdown
# Отчет о Тестировании LLM

**Дата и время:** 2024-08-12 18:30:00
**Протестированные модели:** `llama3:8b`, `phi3`, `janhq/Jan-v1-4B-GGUF`

## Сводная Таблица Результатов

| Модель / Категория | Логика | Инструкции | Код | Извлечение | Суммаризация | Арифметика | **ИТОГО** |
|:-------------------|:------:|:----------:|:---:|:----------:|:------------:|:----------:|:---------:|
| **llama3:8b**      | 100%   | 90%        | 100%| 100%       | 70%          | 100%       | **93.3%** |
| **phi3**           | 100%   | 100%       | 80% | 90%        | 60%          | 90%        | **86.7%** |
| **Jan-v1-4B-GGUF** | 80%    | 70%        | 40% | 80%        | 50%          | 70%        | **65.0%** |

## Детальный Анализ (TODO)

*   **Сильные стороны модели X:** ...
*   **Типичные ошибки модели Y:** ...
```

### 6. План Реализации

1.  **Этап 1: Каркас Системы (20%)**
    *   Создать файловую структуру проекта.
    *   Реализовать `llm_client.py` с функцией для вызова модели `ollama`.
    *   Создать пустые классы-заглушки для всех генераторов и `core` модулей.
    *   Настроить `config.yaml` и чтение из него.

2.  **Этап 2: Первая Сквозная Реализация (30%)**
    *   Полностью реализовать генератор и верификатор для одной категории, например, `t06_mathematics` (самая простая для верификации).
    *   Реализовать базовую логику `test_runner.py` для запуска этого одного теста.
    *   Реализовать сохранение сырого JSON-результата.

3.  **Этап 3: Наполнение Контентом (40%)**
    *   Последовательно реализовать генераторы и верификаторы для всех остальных 5 категорий.
    *   Для каждой новой категории дорабатывать `test_runner.py` для ее поддержки.

4.  **Этап 4: Отчетность и Завершение (10%)**
    *   Реализовать `reporter.py`, который будет анализировать JSON-файлы из `results/raw/` и генерировать итоговый Markdown-отчет.
    *   Провести рефакторинг, добавить docstrings и type hints.

---

Это ТЗ представляет собой четкую дорожную карту. Теперь можно брать первый пункт из плана реализации и начинать кодировать. Начнем?