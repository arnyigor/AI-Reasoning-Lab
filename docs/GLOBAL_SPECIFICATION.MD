# Техническое Задание: Поэтапная Разработка и Развитие Платформы AI-Reasoning-Lab

## Раздел 0: Введение и Руководящие Принципы

### 0.1. Назначение Документа
Настоящее Техническое Задание (ТЗ) является единым, исчерпывающим и авторитетным источником истины для проектирования, разработки и поэтапного внедрения платформы AI-Reasoning-Lab. Документ предназначен для одного разработчика, работающего в тесном взаимодействии с ассистентом на базе искусственного интеллекта, что требует максимальной степени детализации, однозначности и предписывающего характера изложения.

Цель данного ТЗ — синтезировать и гармонизировать требования из четырех исходных документов: стратегического плана , описания проекта , первоначального ТЗ на базовую логику  и стратегии обеспечения целостности. Результатом является целостный, непротиворечивый и практически реализуемый инженерный план, который обеспечивает плавное, инкрементальное развитие платформы без нарушения существующей функциональности на каждом новом этапе.   

### 0.2. Философия Проектирования
В основе архитектуры AI-Reasoning-Lab лежат три фундаментальных принципа, которые должны направлять каждое техническое решение на всех этапах разработки.

**Модульность и Расширяемость (Modularity-First):** Платформа проектируется не как монолитный инструмент, а как экосистема независимых, слабо связанных и взаимозаменяемых модулей. В основе этого подхода лежит принцип "открытости/закрытости": основные компоненты системы должны быть открыты для расширения (например, добавления новых типов тестов или моделей-оценщиков), но закрыты для модификации. Это минимизирует риски регрессии и обеспечивает долгосрочную поддерживаемость и масштабируемость.   

**Доказуемость и Воспроизводимость:** Каждый результат, полученный в рамках платформы, должен быть методологически корректным и полностью воспроизводимым. Для детерминированных тестов это означает получение идентичных результатов при идентичных входных данных. Для стохастических оценок это означает точное версионирование всех компонентов (кода, данных, моделей, конфигураций) для возможности проведения аудита и повторного анализа любого тестового запуска.   

**Безопасность по Принципу "Нулевого Доверия" (Zero Trust):** В условиях растущего кризиса доверия к бенчмаркам ИИ, платформа должна рассматривать каждый внешний компонент, данные и тестируемую модель как потенциально враждебные. Доверие никогда не предполагается по умолчанию; оно должно постоянно проверяться на каждом этапе конвейера обработки данных. Этот принцип является основой для защиты от манипуляций, подмены данных и "обучения на тесте".   

### 0.3. Сводная Дорожная Карта Реализации
Разработка будет вестись в пять последовательных этапов, каждый из которых представляет собой функционально завершенный и стабильный инкремент.

*   **Этап 1: Фундаментальная Архитектура и Ядро "Базового Контроля".** Создание архитектурного скелета и реализация набора базовых функциональных тестов.
*   **Этап 2: Реализация Модуля Продвинутых Рассуждений (Уровень 2).** Разработка ключевого компонента — процедурного генератора уникальных логических задач.
*   **Этап 3: Модули Качественной Оценки и Ответственного ИИ (Уровень 3).** Внедрение инструментов для тонкой оценки, включая "LLM-as-a-Judge", и тестов на безопасность, справедливость и токсичность.
*   **Этап 4: Сегмент Тестирования Производительности на Устройствах.** Расширение платформы для тестирования локальных моделей на платформе Android.
*   **Этап 5: MLOps, Инфраструктура и Обеспечение Целостности.** Обертывание фреймворка в промышленную MLOps-инфраструктуру для автоматизации, безопасности и воспроизводимости.

## Раздел 1: Этап 1 - Фундаментальная Архитектура и Ядро "Базового Контроля"

### 1.1. Цель Этапа
Основной целью данного этапа является создание прочного и расширяемого архитектурного фундамента для всей платформы и реализация на его основе базового, но полнофункционального набора тестов Уровня 1 ("Базовый Контроль"). Этот этап не создает временный прототип, а сразу реализует простейшую функциональность из первоначального ТЗ  в рамках стратегической, долговечной архитектуры. Такой подход гарантирует, что последующие этапы будут расширять систему, а не требовать ее кардинальной и рискованной переработки.   

### 1.2. Формализация Четырехуровневой Архитектуры
Система должна быть строго структурирована в соответствии с четырехуровневой архитектурой, обеспечивающей четкое разделение ответственности.   

#### 1.2.1. Уровень Утилит и Ядра (Utility & Core Layer)
Этот уровень является фундаментом, предоставляющим общие сервисы для всей платформы.

*   **Модуль core.config_manager:** Должен быть реализован с использованием шаблона проектирования Одиночка (Singleton). Это обеспечит единую, глобальную и согласованную точку доступа к настройкам из конфигурационного файла config.yaml для всех компонентов системы, предотвращая конфликты и дублирование конфигураций.   
*   **Модуль core.logger:** Также должен быть реализован как Одиночка (Singleton) для централизованного управления логированием. Он должен поддерживать различные уровни логирования (DEBUG, INFO, WARNING, ERROR) и вывод в консоль и/или файл.

#### 1.2.2. Сервисный Уровень (Service Layer)
Этот уровень инкапсулирует взаимодействие с внешними системами, в данном случае — с моделями ИИ.

*   **Класс LLMClient:** Должен быть спроектирован с использованием шаблона Фабрика (Factory Pattern). Будет создан абстрактный базовый класс AbstractLLMClient и фабричный метод, который на основе конфигурации в config.yaml создает конкретный экземпляр клиента (например, OllamaClient). На данном этапе будет реализован только один "продукт" — OllamaClient, взаимодействующий с локальным сервером Ollama. Однако архитектура должна быть готова к добавлению OpenAIClient, GeminiClient или HuggingFaceClient в будущем без необходимости изменять код, который использует фабрику. Это отделяет логику тестов от специфики API конкретных провайдеров.   

#### 1.2.3. Уровень Бизнес-логики (Business Logic Layer)
Этот уровень содержит логику оркестрации тестовых процессов.

*   **Класс TestRunner:** Является центральным оркестратором выполнения тестов. Его реализация должна следовать шаблону Стратегия (Strategy Pattern). TestRunner не должен содержать логику генерации или верификации конкретных тестов. Вместо этого он будет получать на вход список объектов-стратегий, каждый из которых реализует общий интерфейс ITestGenerator. Его задача — итерироваться по этим стратегиям, вызывать их методы для генерации и верификации, собирать результаты и измерять время выполнения.   

#### 1.2.4. Уровень Тестов (Test Layer)
Самый высокий уровень, содержащий определения конкретных тестовых случаев.

*   **Абстрактный базовый класс AbstractTestGenerator:** Этот класс формализует интерфейс ITestGenerator, который должны реализовывать все тестовые стратегии. Он должен определять следующие абстрактные методы :   
    *   `generate() -> dict`: Генерирует один экземпляр теста, возвращая словарь, содержащий как минимум `prompt` и `expected_output`.
*   `verify(llm_response: str, expected_output: any) -> dict`: Проверяет ответ LLM на корректность, сравнивая его с `expected_output`. Возвращает словарь с результатом проверки, например, `{'is_correct': bool, 'details': str}`.

### 1.3. Реализация Тестовых Категорий Уровня 1
На базе определенной архитектуры должны быть реализованы все шесть категорий тестов из первоначального ТЗ. Каждая категория реализуется как отдельный класс-стратегия, наследующий  `AbstractTestGenerator`.

*   **SimpleLogicTest:** Генерирует простые дедуктивные задачи с 2-3 сущностями и фактами. Верификация — поиск точного совпадения имени правильного ответа в выводе модели.
*   **InstructionsTest:** Генерирует задачу на выполнение последовательности из 2-4 команд над текстом (например, реверс, подсчет гласных, смена регистра). Верификация — посимвольное сравнение с эталонной строкой.
*   **CodeGenTest:** Генерирует ТЗ на простую функцию на Python и набор unit-тестов. Верификация — выполнение сгенерированного моделью кода через `exec()` и запуск сгенерированных unit-тестов.
*   **DataExtractionTest:** Генерирует текст со вставленными в него сущностями (email, телефон, дата). Верификация — извлечение сущностей из ответа модели и сравнение полученного множества с эталонным.
*   **SummarizationTest:** Генерирует абзац, где одно предложение помечено как ключевое. Промпт просит выбрать одно наиболее важное предложение. Верификация — проверка на точное вхождение эталонного предложения в ответ модели.
*   **MathematicsTest:** Генерирует арифметическое выражение со скобками. Верификация — извлечение числового ответа из вывода модели и сравнение с результатом, вычисленным через `eval()`.

### 1.4. Спецификация Форматов Данных

#### 1.4.1. config.yaml
Файл конфигурации должен иметь следующую структуру, расширяя первоначальную спецификацию  для поддержки фабрики клиентов:   

```yaml
# Секция для конфигурации клиентов LLM (для Factory Pattern)
llm_clients:
  default: ollama
  providers:
    ollama:
      # Параметры для Ollama, если нужны
      base_url: "http://localhost:11434"

# Список моделей для тестирования
models_to_test:
  - llama3:8b
  - phi3

# Список тестовых модулей для запуска
tests_to_run:
  - t01_simple_logic
  - t02_instructions
  - t03_code_gen
  - t04_data_extraction
  - t05_summarization
  - t06_mathematics

# Количество генераций задачи для каждой категории
runs_per_test: 10
```

#### 1.4.2. results/raw/*.json
Схема для сырых результатов должна быть расширяемой. Каждый файл представляет собой массив JSON-объектов :   

```json
[
  {
    "timestamp": "2024-05-20T10:00:00Z",
    "model_name": "llama3:8b",
    "test_name": "t01_simple_logic",
    "run_id": 1,
    "input_data": {
      "prompt": "..."
    },
    "raw_output": "...",
    "verification_result": {
      "is_correct": true
    },
    "performance_metrics": {
      "latency_ms": 1200
    }
  }
]
```

#### 1.4.3. results/reports/*.md
Модуль `Reporter` должен быть реализован с использованием шаблона Декоратор (Decorator Pattern).

*   **Базовый компонент BaseReporter:** Его единственная ответственность — загрузить и агрегировать данные из сырых JSON-файлов в промежуточную структуру данных (например, pandas DataFrame).
*   **Декоратор MarkdownReportDecorator:** Принимает объект BaseReporter, получает от него агрегированные данные и форматирует их в виде сводной Markdown-таблицы, как указано в.   
*   **Декоратор JSONReportDecorator:** (Для будущего использования) Может быть добавлен для вывода агрегированных результатов в структурированном JSON-формате.

Такой подход позволяет гибко добавлять новые форматы отчетов (например, HTML, CSV) в будущем, не изменяя логику агрегации данных.   

### 1.5. Ключевые Результаты Этапа
По завершении Этапа 1 будет получена полностью функционирующая система, способная выполнять все тесты из первоначального ТЗ. Однако, в отличие от простой реализации, эта система будет построена на надежном, модульном и расширяемом архитектурном фундаменте , полностью готовом к безболезненной интеграции более сложных модулей на последующих этапах.   

## Раздел 2: Этап 2 - Реализация Модуля Продвинутых Рассуждений (Уровень 2)

### 2.1. Цель Этапа
Целью данного этапа является разработка и интеграция центрального, наиболее инновационного компонента платформы — модуля процедурной генерации уникальных и сложных логических задач (логических гридов). Этот модуль является не просто функцией для создания тестов, а ключевой стратегической защитой от экзистенциальной угрозы "обучения на тесте" (data contamination), которая подрывает доверие к статичным бенчмаркам. Создавая на лету практически бесконечный поток новых задач, которых по определению нет ни в одном обучающем наборе, платформа обеспечивает методологически корректную и надежную оценку истинных когнитивных способностей LLM.   

### 2.2. Проектирование Модуля baselogic (Ядро Рассуждений)
Этот модуль предоставляет низкоуровневые примитивы для представления и решения логических головоломок.

#### 2.2.1. Модели Домена
Необходимо определить набор классов для представления структуры головоломки :   

*   **Domain:** Контейнер для всей задачи, определяющий ее размер и геометрию (например, linear).
*   **Category:** Представляет категорию атрибутов (например, "Имя студента", "Цвет сумки").
*   **Attribute:** Конкретное значение внутри категории (например, "Анна", "Зеленый").
*   **Grid:** Представляет сетку решения, состоящую из слотов.
*   **Slot:** Позиция в сетке (например, index: 1).
*   **Assignment:** Структура данных, хранящая текущее состояние решения (какие атрибуты назначены каким слотам).

#### 2.2.2. DSL Ограничений
Необходимо реализовать классы для каждого типа правил, описанных в. Каждый класс должен реализовывать общий интерфейс `IConstraint` с методом `is_satisfied(assignment: Assignment) -> bool`.

*   **ImplicationConstraint:** "Если A, то B".
*   **XORConstraint:** "Верно либо A, либо B, но не оба".
*   **OrderConstraint:** "A находится левее B".
*   **AdjacencyConstraint:** "A находится рядом с B".
*   **SumConstraint:** "Сумма индексов позиций A и B равна N".
*   **IffConstraint (Bi-implication):** "A истинно тогда и только тогда, когда истинно B".

Эта богатая DSL (Domain-Specific Language) позволяет создавать комбинаторно сложные задачи, требующие многошаговых дедуктивных рассуждений.   

#### 2.2.3. Решатель (Solver)
Решатель является сердцем модуля baselogic. Он должен быть реализован как решатель Задач удовлетворения ограничений (Constraint Satisfaction Problem, CSP). Рекомендуемый алгоритм — поиск с возвратом (backtracking) с применением следующих эвристик и оптимизаций для повышения производительности :   

*   **Эвристики выбора переменных:** MRV (Minimum Remaining Values), Degree Heuristic.
*   **Распространение ограничений:** Forward Checking или более сложный Arc Consistency (AC-3) для раннего отсечения невалидных ветвей поиска.

#### 2.2.4. Валидатор Уникальности
Критически важная функция `has_unique_solution(constraints: list[IConstraint], domain: Domain) -> bool`. Эта функция использует Solver для поиска решений. Она должна быть оптимизирована для быстрого завершения после нахождения второго решения. Если решатель находит более одного решения, функция немедленно возвращает `False`. Если после полного обхода пространства поиска найдено ровно одно решение, она возвращает `True`. Производительность этого компонента напрямую влияет на скорость генерации задач.

### 2.3. Проектирование Модуля grandmaster (Генератор Задач)
Этот высокоуровневый модуль использует примитивы из baselogic для создания готовых к использованию головоломок.

#### 2.3.1. Алгоритм Генерации
Генерация должна следовать трехфазному "субтрактивному" алгоритму, описанному в  и :   

1.  **Фаза 1: `generate_full_solution()`:** Алгоритмически создается полная, валидная сетка решения (например, случайным образом назначаются все атрибуты всем слотам).
2.  **Фаза 2: `enumerate_all_clues(solution: Assignment)`:** На основе сгенерированного решения создается исчерпывающий набор всех возможных истинных утверждений (подсказок) всех типов, поддерживаемых DSL.
3.  **Фаза 3: `minimize_clues(all_clues: list, solution: Assignment)`:** Это основной итеративный цикл минимизации:
    a. Подсказки из `all_clues` случайным образом перемешиваются.
    b. В цикле итеративно удаляется одна подсказка.
    c. После каждого удаления вызывается `baselogic.has_unique_solution()` для проверки, что оставшийся набор подсказок все еще приводит к единственному решению.
    d. Если решение остается уникальным, подсказка удаляется навсегда. Если удаление приводит к множественности или отсутствию решений, подсказка возвращается в набор.
    e. Процесс продолжается до тех пор, пока не будет предпринята попытка удалить каждую подсказку.

Результатом является минимальный, но достаточный набор подсказок, гарантирующий единственное решение.

#### 2.3.2. Параметризация Сложности
Класс генератора Grandmaster должен принимать на вход конфигурационный объект, позволяющий контролировать сложность генерируемых задач :   

*   **grid_size:** `int` (например, 5, 6, 7).
*   **num_categories:** `int`.
*   **allowed_constraint_types:** `list[str]`.
*   **seed:** `int` для воспроизводимости генерации.

### 2.4. Интеграция в TestRunner
Модуль процедурной генерации интегрируется в существующую архитектуру через шаблон Стратегия.

*   Создается новый класс `ProceduralLogicTest(AbstractTestGenerator)`.
*   Метод `generate()` этого класса будет инстанцировать и вызывать `grandmaster.Grandmaster` с заданными параметрами сложности для создания новой, уникальной задачи. Он вернет словарь, содержащий сгенерированный `prompt` (набор подсказок в текстовом виде) и `expected_output` (полное решение в структурированном виде).
*   Метод `verify()` будет парсить структурированный ответ от LLM (например, таблицу в формате Markdown или JSON) и сравнивать его с эталонным решением, сгенерированным на лету.

Важно отметить, что сгенерированные задачи должны быть эфемерными. Они создаются, используются для одного тестового прогона и затем удаляются. В логах сохраняются только `seed` и параметры генерации для обеспечения воспроизводимости, но не сами тексты задач. Это предотвращает возможность утечки и последующего использования уникальных тестовых случаев для загрязнения обучающих данных.

### 2.5. Ключевые Результаты Этапа
По завершении этапа платформа AI-Reasoning-Lab приобретает свою ключевую отличительную особенность: способность проводить тестирование на бесконечном потоке свежих, методологически выверенных и сложных логических задач. Это не только позволяет оценивать глубокие когнитивные способности LLM, но и обеспечивает надежную защиту от "обучения на тесте", укрепляя доверие к результатам оценки.

## Раздел 3: Этап 3 - Модули Качественной Оценки и Ответственного ИИ (Уровень 3)

### 3.1. Цель Этапа
Цель данного этапа — расширить возможности платформы за пределы бинарных оценок "правильно/неправильно" и внедрить инструменты для проведения тонкой, нюансированной качественной оценки, а также для всестороннего тестирования аспектов Ответственного ИИ (Responsible AI), включая безопасность, справедливость и токсичность. Это позволит формировать комплексное представление о поведении модели, что критически важно для ее применения в реальных условиях.   

### 3.2. Разработка Модуля "LLM-as-a-Judge"
Этот модуль использует мощную LLM для автоматизации качественной оценки ответов тестируемых моделей.

#### 3.2.1. Архитектура
Модуль должен состоять из трех ключевых компонентов :   

*   **PromptManager:** Отвечает за управление версионированными шаблонами промптов для LLM-судьи. Промпты должны храниться в отдельных файлах (например, `.jinja2`) и содержать плейсхолдеры для исходного запроса, ответа тестируемой модели и критериев оценки из рубрики.
*   **JudgeInferenceEngine:** Слой абстракции для взаимодействия с LLM-судьей. Он использует существующую `LLMClient` Factory для отправки запросов к модели-судье (например, GPT-4o или Claude 3 Opus), обрабатывает повторные попытки при сбоях и обеспечивает получение ответа в строго структурированном формате (например, JSON).
*   **RubricParser:** Компонент, который принимает JSON-ответ от судьи, валидирует его по схеме и преобразует в структурированные, количественные оценки и текстовые обоснования для сохранения в результатах.

#### 3.2.2. Система Рубрик
Эффективность LLM-судьи напрямую зависит от качества рубрики. Необходимо создать механизм для определения и версионирования рубрик в формате YAML. Каждая рубрика должна содержать :   

*   **Название критерия** (например, "Креативность", "Связность").
*   **Шкалу оценки** (рекомендуется простая шкала от 1 до 5).
*   **Подробные дескрипторы** для каждого балла по шкале, включая конкретные положительные и отрицательные примеры.

В качестве эталона должна быть реализована рубрика для оценки "Творческого письма", приведенная в Таблице 3 документа.   

#### 3.2.3. Реализация Стратегий Снижения Предвзятости
LLM-судьи подвержены различным видам предвзятости. Архитектура должна включать встроенные механизмы для их смягчения :   

*   **Разнообразие Оценщиков:** `JudgeInferenceEngine` должен реализовывать политику выбора судьи из другого семейства моделей, чем тестируемая модель (например, использовать модель от Anthropic для оценки модели от Google). Это настраивается в `config.yaml`.
*   **Нейтрализация Позиционной Предвзятости:** Для задач, требующих парного сравнения двух ответов (A и B), система должна автоматически проводить оценку дважды в случайном порядке (A, B, а затем B, A) и агрегировать результаты, чтобы нейтрализовать склонность моделей предпочитать первый или последний вариант.
*   **Борьба с Предвзятостью к Многословию:** Рубрики и промпты для судьи должны содержать явные инструкции поощрять краткость и штрафовать за излишнюю многословность. Длина ответа должна логироваться как отдельная метрика для последующего анализа корреляции с оценками.

### 3.3. Интеграция Инструментов Ответственного ИИ
Платформа должна быть расширена новыми типами тестов (стратегиями), нацеленными на оценку справедливости и токсичности.

#### 3.3.1. Модуль Справедливости
Создается класс `FairnessTest(AbstractTestGenerator)`.

*   Этот модуль будет использовать библиотеку `fairlearn` для измерения групповой справедливости.
*   Тесты будут направлены на выявление вреда от распределения (allocation harms) и вреда от качества обслуживания (quality-of-service harms).
*   Ключевые метрики для расчета и логирования: Демографический паритет (Demographic Parity) и Равные шансы (Equalized Odds).   

#### 3.3.2. Модуль Токсичности
Создается класс `ToxicityTest(AbstractTestGenerator)`.

*   Модуль будет использовать библиотеку `evaluate` от Hugging Face, в частности ее модуль `toxicity`.
*   Тест будет подавать на вход модели набор состязательных промптов и оценивать сгенерированные ответы с помощью предобученной модели-классификатора (например, `roberta-hate-speech-dynabench-r4-target`).
*   Ключевые метрики: оценка токсичности (0-1) для каждого ответа и общий коэффициент токсичности (`toxicity_ratio`) для тестового запуска.   

### 3.4. Реализация Автоматизированного Red-Teaming
Для проактивного выявления уязвимостей безопасности будет интегрирован модуль автоматизированного Red-Teaming, основанный на принципах OWASP Top 10 для LLM и использующий инструментарий, вдохновленный Microsoft PyRIT.   

#### 3.4.1. Интеграция PyRIT
Будут созданы классы-обертки для взаимодействия с ключевыми компонентами PyRIT, такими как `PromptConverter` и `AttackOrchestrator`.

#### 3.4.2. Тестовые Стратегии для OWASP
Будут реализованы следующие классы-стратегии:

*   **LLM01_PromptInjectionTest:** Использует PyRIT для генерации промптов, нацеленных на обход системных инструкций модели. Верификация оценивает, удалось ли инъекции добиться нежелательного поведения.
*   **LLM04_DenialOfServiceTest:** Генерирует ресурсоемкие запросы (например, глубоко рекурсивные задачи) и измеряет время ответа. Верификация срабатывает, если время ответа превышает заданный порог, что указывает на потенциальную уязвимость к отказу в обслуживании.
*   **LLM06_SensitiveDataDisclosureTest:** Использует промпты, специально разработанные для извлечения потенциально конфиденциальной информации (PII), к которой модель могла иметь доступ во время обучения.

### 3.5. Внутренняя Петля Валидации "Судья должен быть судим"
Доверие к платформе зависит от доверия к ее компонентам. LLM-судья сам является системой ИИ и должен подвергаться проверке. Для этого необходимо реализовать внутренний конвейер обратной связи.   

*   **Спецификация конвейера:** После каждого тестового запуска, в котором использовался `LLM-as-a-Judge`, система должна автоматически формировать случайную выборку (например, 10%) текстовых обоснований (justification), сгенерированных судьей.
*   **Мета-оценка:** Эта выборка должна программно подаваться на вход модулям `ToxicityTest` и `FairnessTest`.
*   **Логирование:** Результаты этой мета-оценки (например, `judge_toxicity_score`, `judge_fairness_disparity`) должны сохраняться в сыром JSON-файле вместе с основными результатами теста.

Эта петля обратной связи превращает оценочный фреймворк из "черного ящика" в прозрачную, самоконтролируемую систему. Она позволяет количественно измерять и отслеживать надежность и беспристрастность самого процесса оценки, что является мощным отличительным фактором и ключевым элементом для построения доверия к платформе.

### 3.6. Ключевые Результаты Этапа
По завершении этого этапа AI-Reasoning-Lab превращается в комплексную платформу для многоаспектной оценки ИИ, способную измерять не только корректность и когнитивные способности, но и критически важные качественные характеристики: справедливость, безопасность, устойчивость к атакам и общее качество генерируемого контента.

## Раздел 4: Этап 4 - Сегмент Тестирования Производительности на Устройствах

### 4.1. Цель Этапа
Целью данного этапа является расширение функциональности AI-Reasoning-Lab для охвата нового, критически важного домена: тестирования малых, локальных моделей ИИ (on-device AI) на платформе Android. Основное внимание уделяется измерению количественных показателей производительности, таких как задержка, энергопотребление и использование памяти, которые являются определяющими для пользовательского опыта на устройствах с ограниченными ресурсами.   

### 4.2. Архитектура Модуля Android
Модуль будет иметь гибридную архитектуру для достижения максимальной точности измерений.

#### 4.2.1. C++ NDK Компонент
Ядро модуля для выполнения бенчмарков должно быть реализовано на C++ с использованием Android NDK. Это позволяет обойти оверхед виртуальной машины Java/Kotlin и выполнять код вывода моделей напрямую, обеспечивая наиболее точные измерения производительности. Этот компонент будет использовать:

*   **LiteRT (преемник TensorFlow Lite)** как основную среду выполнения.
*   **ONNX Runtime** для поддержки моделей из этой экосистемы.
    Он также должен включать конфигурации для явного включения и тестирования аппаратных ускорителей через делегаты GPU и NNAPI.   

#### 4.2.2. JNI-мост
Для связи между управляющим кодом и нативным компонентом должен быть определен и реализован Java Native Interface (JNI). Через этот мост управляющий код будет передавать нативному компоненту модель, входные данные и параметры бенчмарка, а обратно получать результаты измерений.

#### 4.2.3. Оркестратор на Kotlin/Java
Будет создано специализированное Android-приложение, выполняющее роль оркестратора. Его задачи:

*   Получать тестовые задания от основного Python-фреймворка (например, через ADB или локальный HTTP-сервер).
*   Управлять жизненным циклом нативного C++ компонента через JNI.
*   Программно запускать и останавливать сбор системных трассировок.
*   Агрегировать результаты от нативного компонента и системных профилировщиков и отправлять их обратно в основной фреймворк.

### 4.3. Спецификация Конвейера Бенчмаркинга
Конвейер должен собирать полный набор метрик производительности, используя комбинацию инструментов.   

#### 4.3.1. Автоматизация TFLite Benchmark Tool
Нативный компонент должен интегрировать и программно вызывать функциональность, аналогичную консольной утилите `benchmark_model` от TFLite. Это позволит измерять гранулярные метрики:

*   **Задержка вывода (Inference Latency):** Среднее, медианное, максимальное время выполнения одного вывода.
*   **Время до первого токена (Time to First Token):** Для генеративных моделей.
*   **Скорость декодирования (Decoding Speed):** В токенах/секунду для генеративных моделей.

#### 4.3.2. Сбор Трассировок
Оркестратор на Kotlin/Java должен реализовывать функционал для программного запуска и сбора данных из системных профилировщиков Android Studio (или их консольных аналогов, таких как `perfetto`):

*   **Использование CPU/GPU/NPU:** Процент загрузки во времени.
*   **Использование памяти (Memory Footprint):** Пиковое использование нативной и Java-кучи (RSS).
*   **Энергопотребление:** Потребление энергии ключевыми компонентами SoC (ЦП, ГП, память) на поддерживаемых устройствах.

Эта дальновидная возможность позволит тестировать не только пиковую, но и "устойчивую" производительность, что критически важно для генеративных моделей на устройствах, работающих в течение длительного времени.   

### 4.4. Конвейер Оптимизации и Валидации
Одной из ключевых задач при разработке on-device AI является компромисс между производительностью и точностью. Модуль должен включать автоматизированный конвейер для оценки этого компромисса. Скрипт на стороне основного Python-фреймворка должен последовательно выполнять следующие шаги:   

1.  **Базовый замер:** Запустить полный набор бенчмарков производительности и точности на исходной модели с плавающей запятой (FP32).
2.  **Прунинг (Pruning):** Применить прунинг по величине с использованием TensorFlow Model Optimization Toolkit (`tfmot`) для создания разреженной версии модели.
3.  **Квантование (Quantization):** Применить пост-тренировочное квантование (PTQ) для снижения точности весов до INT8.
4.  **Повторный замер:** Автоматически запустить полный набор бенчмарков на разреженной и квантованной моделях.
5.  **Генерация отчета:** Сформировать итоговый сравнительный отчет, наглядно представляющий компромиссы по четырем осям: Точность vs. Задержка vs. Размер модели vs. Энергопотребление.

### 4.5. Ключевые Результаты Этапа
По завершении этого этапа AI-Reasoning-Lab станет комплексным, сквозным фреймворком, способным проводить всестороннее тестирование как мощных облачных моделей, так и компактных моделей на устройствах. Это значительно расширит область применения платформы и ее ценность для разработчиков, работающих над всем спектром приложений ИИ.

## Раздел 5: Этап 5 - MLOps, Инфраструктура и Обеспечение Целостности

### 5.1. Цель Этапа
Целью заключительного этапа является преобразование разработанного фреймворка из набора локальных скриптов в промышленную, автоматизированную и защищенную платформу. Для этого необходимо внедрить надежную MLOps-инфраструктуру, которая обеспечит автоматизацию, воспроизводимость, масштабируемость и, что наиболее важно, реализует на практике архитектуру безопасности по принципу "Нулевого Доверия".   

### 5.2. Настройка Технологического Стека MLOps
Будет развернут и интегрирован следующий стек технологий :   

#### 5.2.1. Контейнеризация
*   **Dockerfile:** Будет создан `Dockerfile`, который инкапсулирует всю среду выполнения фреймворка, включая Python 3.11+, все зависимости из `requirements.txt`, а также необходимые системные библиотеки и, при необходимости, Android SDK/NDK. Это гарантирует согласованность среды между локальной разработкой и выполнением в CI/CD.

#### 5.2.2. Отслеживание Экспериментов
*   **MLflow:** Платформа будет глубоко интегрирована с MLflow. Класс `TestRunner` должен быть модифицирован для автоматического логирования каждого тестового запуска как отдельного "run" в MLflow. Должны логироваться:
    *   **Параметры:** имя модели, тип теста, `seed` генератора, параметры сложности.
    *   **Метрики:** точность, оценка токсичности, задержка, оценка от LLM-судьи и т.д.
    *   **Артефакты:** сырые JSON-результаты, сгенерированные Markdown-отчеты и любые другие релевантные файлы (например, логи).

#### 5.2.3. Версионирование Данных
*   **DVC (Data Version Control):** Будет интегрирован с Git для версионирования больших активов, таких как статические наборы данных (например, для тестов на справедливость) и веса тестируемых моделей. Это обеспечивает полную воспроизводимость любого исторического тестового запуска.   

### 5.3. Реализация Архитектуры Безопасности "Нулевого Доверия"
Для обеспечения целостности и защиты от манипуляций будут реализованы следующие архитектурные компоненты, основанные на принципах из.   

#### 5.3.1. Шлюз Приема Данных
*   **API-эндпоинт на FastAPI:** Будет разработан легковесный API-сервер на FastAPI, который будет служить единой точкой входа для запуска тестов.
*   **Криптографическое хеширование:** При получении любых входных данных (промпты, конфигурации) они должны немедленно хешироваться с использованием алгоритма SHA-256. Хеш должен сохраняться в логах MLflow вместе с остальными параметрами запуска. Любой последующий доступ к этим данным должен сопровождаться проверкой хеша для гарантии их неизменности.

#### 5.3.2. Изолированные "Песочницы"
*   **Выполнение в контейнерах:** `TestRunner` должен быть модифицирован для запуска каждого тестового прогона (или даже каждого отдельного теста) в отдельном, эфемерном Docker-контейнере. Это полностью изолирует выполнение тестов друг от друга и от основной системы.
*   **Принцип наименьших привилегий:** Контейнеры должны запускаться с максимально ограниченными правами: без доступа к сети (если это не требуется для теста), с доступом только для чтения к необходимым данным (через монтирование томов) и с мониторингом потребления ресурсов для предотвращения атак типа "Отказ в обслуживании модели" (LLM04).   

#### 5.3.3. Криптографическая Подпись Результатов
*   **Аттестация результатов:** Модуль `Reporter` должен быть расширен функциональностью для криптографической подписи сгенерированных отчетов (например, с использованием стандарта JWT или PGP). Эта подпись позволит любому внешнему потребителю отчета верифицировать, что он был действительно сгенерирован платформой AI-Reasoning-Lab и не был изменен после создания.   

### 5.4. CI/CD для Тестирования ИИ
Будет создан шаблон конвейера CI/CD (например, для GitHub Actions), который автоматизирует жизненный цикл тестирования. Конвейер должен автоматически запускать соответствующий набор регрессионных тестов при наступлении следующих событий :   

*   **Изменение в коде фреймворка:** Запуск unit-тестов и базового набора функциональных тестов для проверки самого фреймворка.
*   **Регистрация новой версии целевой модели в MLflow Model Registry:** Автоматический запуск полного набора бенчмарков для новой версии модели.
*   **Обновление набора данных в DVC:** Автоматический перезапуск тестов, которые зависят от этого набора данных.
*   **Запланированные ночные запуски:** Регулярный запуск полного регрессионного набора тестов на эталонных моделях для мониторинга стабильности.

### 5.5. Ключевые Результаты Этапа
По завершении этого этапа проект AI-Reasoning-Lab эволюционирует из мощного, но локального инструмента в промышленную, автоматизированную и защищенную платформу для оценки ИИ. Она будет готова к развертыванию в качестве внутреннего сервиса ("Framework-as-a-Service") или даже к открытой публикации, обеспечивая высочайший уровень воспроизводимости, надежности и доверия к результатам.   

## Раздел 6: Приложения и Сводные Спецификации

### 6.1. Спецификация REST API
Для программного взаимодействия с фреймворком будет предоставлен следующий минимальный набор эндпоинтов на базе FastAPI.

**`POST /v1/benchmark_runs`**

*   **Назначение:** Асинхронно запускает новый тестовый прогон.
*   **Тело запроса (JSON):**
    ```json
    {
      "model_name": "llama3:8b",
      "tests_to_run": ["t01_simple_logic", "t06_mathematics"],
      "runs_per_test": 20,
      "custom_config": {}
    }
    ```
*   **Ответ (JSON):**
    ```json
    {
      "run_id": "mlflow_run_uuid_12345",
      "status": "QUEUED",
      "message": "Benchmark run has been queued."
    }
    ```

**`GET /v1/benchmark_runs/{run_id}`**

*   **Назначение:** Получает статус и результаты завершенного тестового прогона.
*   **Ответ (JSON):**
    ```json
    {
      "run_id": "mlflow_run_uuid_12345",
      "status": "COMPLETED",
      "results_summary": {
        "t01_simple_logic": {"accuracy": 0.95},
        "t06_mathematics": {"accuracy": 1.0}
      },
      "artifacts_url": "/path/to/mlflow/artifacts"
    }
    ```

### 6.2. Полная Схема config.yaml
Финальная, полная спецификация конфигурационного файла, объединяющая настройки всех этапов.

```yaml
# Конфигурация клиентов LLM (для Factory Pattern)
llm_clients:
  default: ollama
  providers:
    ollama:
      base_url: "http://localhost:11434"
    openai:
      api_key: "${OPENAI_API_KEY}" # Поддержка переменных окружения
      model: "gpt-4o"

# Конфигурация LLM-as-a-Judge
judge_config:
  provider: openai # Использовать GPT-4o в качестве судьи
  rubrics_path: "./rubrics/"

# Список моделей для тестирования
models_to_test:
  - provider: ollama
    model_name: "llama3:8b"
  - provider: ollama
    model_name: "phi3"

# Список тестовых модулей для запуска
tests_to_run:
  # Уровень 1
  - t01_simple_logic
  - t02_instructions
  - t03_code_gen
  - t04_data_extraction
  - t05_summarization
  - t06_mathematics
  # Уровень 2
  - t07_procedural_logic
  # Уровень 3
  - t08_toxicity
  - t09_fairness
  - t10_redteam_prompt_injection

# Параметры для конкретных тестов
test_parameters:
  t07_procedural_logic:
    grid_size: 5
    num_categories: 5
    runs_per_test: 5
  default:
    runs_per_test: 10

# Настройки безопасности
security:
  enable_sandboxing: true
  sign_results: true
  signing_key_path: "/path/to/private.key"
```

### 6.3. Сводная Таблица Зависимостей
| Библиотека/Инструмент | Версия | Назначение |
| --- | --- | --- |
| Python | 3.11+ | Основной язык разработки |
| FastAPI | 0.110+ | Бэкенд API |
| MLflow | 2.12+ | Отслеживание экспериментов |
| DVC | 3.0+ | Версионирование данных |
| Docker | 20.10+ | Контейнеризация среды |
| fairlearn | 0.10+ | Оценка справедливости (Уровень 3) |
| evaluate (Hugging Face) | 0.4+ | Оценка токсичности (Уровень 3) |
| Microsoft PyRIT | 0.1+ | Автоматизированный Red-Teaming (Уровень 3) |
| z3-solver | 4.12+ | (Опционально) Бэкенд для решателя baselogic |
| TensorFlow Lite | 2.15+ | Среда выполнения на Android |
| TensorFlow Model Optimization | 0.7+ | Прунинг и квантование моделей |

### 6.4. Таблица: Архитектурные Компоненты и Шаблоны Проектирования
Данная таблица служит "архитектурным контрактом" и должна использоваться как постоянный ориентир в процессе разработки для обеспечения согласованности и соблюдения заложенных принципов проектирования. Она связывает ключевые компоненты системы с предписанными шаблонами проектирования и обосновывает их выбор на основе стратегических целей.   

| Компонент/Модуль | Основная ответственность | Рекомендуемый шаблон(ы) | Обоснование |
| --- | --- | --- | --- |
| Движок выполнения тестов (TestRunner) | Оркестрация запуска наборов тестов и сбор результатов. | Стратегия (Strategy) | Позволяет динамически выбирать и применять различные типы тестов (Уровень 1, 2, 3) без изменения ядра. |
| Менеджер целевых моделей (LLMClient) | Управление подключениями к различным моделям ИИ. | Фабрика (Factory) | Отделяет фреймворк от конкретных SDK; позволяет добавлять новые модели без изменения клиентского кода. |
| Сервис конфигурации (ConfigManager) | Предоставление глобального доступа к настройкам. | Одиночка (Singleton) | Обеспечивает единый, согласованный источник конфигурации для всех модулей. |
| Конвейер отчетности (Reporter) | Форматирование и вывод результатов тестов. | Декоратор (Decorator) | Позволяет гибко добавлять форматы отчетов (Markdown, JSON, MLflow) без создания подклассов. |
| Генератор тестовых данных (AbstractTestGenerator) | Создание и предоставление входных данных для тестов. | Строитель (Builder) | Упрощает создание сложных, многокомпонентных входных данных, улучшая читаемость тестов. |
