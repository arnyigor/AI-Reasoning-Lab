from typing import Dict, Any, TypedDict
import json
from baselogic.tests.abstract_test_generator import AbstractTestGenerator


class VerbosityIdealExpectedOutput(TypedDict):
    expected_range: tuple  # Ожидаемый диапазон оценок


class VerbosityIdealTestGenerator(AbstractTestGenerator):
    """
    Тестирует оценку лаконичного резюме - не должно быть штрафа за краткость.
    """

    def __init__(self, test_id: str):
        super().__init__(test_id)
        self.article_text = """Соловьиные вечера–2025: вместе сохраним гармонию природы!

Цель акции: привлечение внимания общественности к охране пернатых друзей и популяризации научных исследований среди жителей городов.

Уже много лет подряд эта акция помогает оценить состояние популяций соловьев и служит важным инструментом народного мониторинга экологии городской среды.

Даты проведения мероприятия: 17 и 18 мая 2025г.

Цель мероприятия «Соловьиные вечера–2025»

Ежегодная акция имеет двойственную природу:

Научная: Позволяет определить численность соловьев в различных частях города, выявить изменения в местах обитания и оперативно реагировать на негативные тенденции в состоянии зеленых зон.

Эколого-просветительская: Привлекает внимание жителей к важности сохранения природных ресурсов, знакомит население с проблемами охраны окружающей среды и пробуждает интерес к природе родного края.

Соловей является своеобразным барометром экологической обстановки в городах. Ухудшение условий жизни в парках и скверах ведет к сокращению числа гнездящихся особей, что сигнализирует о возможных проблемах, связанных с загрязнением воздуха, вырубкой деревьев и кустарников, нарушением микроклимата территорий. Поэтому наблюдение за этими маленькими птицами играет важную роль в сохранении здоровой экосистемы."""

        # Короткое, но точное резюме
        self.summary_brief = "Акция «Соловьиные вечера–2025» в мае помогает оценить численность соловьев в городах и выявить экологические изменения. Это важный инструмент мониторинга и экологического просвещения жителей."

    def generate(self) -> Dict[str, Any]:
        prompt_template = """[INST]
Ты — дотошный, беспристрастный и объективный аналитик-оценщик. Твоя единственная задача — сравнивать два текста и выносить вердикт строго по заданным критериям.

Тебе будут предоставлены оригинальная статья и ее резюме. Ты должен оценить резюме по одному единственному критерию: **фактическая состоятельность**.

**Шкала оценки:**
- **5 (Идеально):** Резюме полностью и без искажений отражает все ключевые факты оригинальной статьи.
- **4 (Хорошо):** Резюме фактически корректно, но может упускать второстепенные детали или использовать слегка упрощенные формулировки.
- **3 (Удовлетворительно):** В основном соответствует статье, но содержит незначительные неточности, двусмысленности или спорные интерпретации.
- **2 (Плохо):** Содержит одну или несколько существенных фактических ошибок, которые противоречат оригинальной статье.
- **1 (Неприемлемо):** Резюме содержит грубые фактические ошибки, вымыслы или полностью искажает смысл оригинала.

**Инструкции по выполнению:**
Твой ответ должен быть представлен в формате JSON и содержать два поля: 'reasoning' (краткое текстовое объяснение твоего выбора на 1-2 предложения) и 'score' (одно целое число от 1 до 5).

**Оригинальная статья:**
\"\"\"
{article_text}
\"\"\"

**Резюме для оценки:**
\"\"\"
{summary_text}
\"\"\"

[/INST]"""

        expected_output: VerbosityIdealExpectedOutput = {
            "expected_range": (4, 5)  # Короткое резюме должно быть высоко оценено
        }

        return {
            "prompt": prompt_template.format(
                article_text=self.article_text,
                summary_text=self.summary_brief
            ),
            "expected_output": expected_output,
            "test_name": "verbosity_ideal"
        }

    def verify(self, llm_output: str, expected_output: VerbosityIdealExpectedOutput) -> Dict[str, Any]:
        """
        Проверяет что краткое резюме получает справедливую оценку.
        """
        try:
            parsed_output = json.loads(llm_output)

            if "score" not in parsed_output:
                return {
                    "is_correct": False,
                    "details": {"reason": "Отсутствует поле 'score' в ответе"}
                }

            actual_score = parsed_output["score"]
            min_score, max_score = expected_output["expected_range"]

            is_correct = min_score <= actual_score <= max_score

            return {
                "is_correct": is_correct,
                "actual_score": actual_score,
                "expected_range": expected_output["expected_range"],
                "reasoning": parsed_output.get("reasoning", ""),
                "test_type": "brief",
                "json_valid": True,
                "details": {
                    "reason": "OK" if is_correct else f"Оценка {actual_score} вне ожидаемого диапазона {expected_output['expected_range']}"
                }
            }

        except json.JSONDecodeError:
            return {
                "is_correct": False,
                "json_valid": False,
                "details": {"reason": "Невалидный JSON в ответе модели"}
            }
        except Exception as e:
            return {
                "is_correct": False,
                "details": {"reason": f"Ошибка при обработке: {str(e)}"}
            }
