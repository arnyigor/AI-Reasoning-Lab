import logging
import time
from pathlib import Path

import pandas as pd

from baselogic.core.reporter import Reporter

log = logging.getLogger(__name__)


class JudgeReporter(Reporter):
    """
    –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ Reporter –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ LLM-—Å—É–¥–µ–π.
    –í–∫–ª—é—á–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏, —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏, —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç–∏ –∫ —Å–º–µ—â–µ–Ω–∏—è–º.
    """

    def __init__(self, results_dir: Path):
        super().__init__(results_dir)
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–µ—Å—Ç–æ–≤ —Å—É–¥–µ–π
        self.judge_results = self._filter_judge_results()

    def _filter_judge_results(self) -> pd.DataFrame:
        """–§–∏–ª—å—Ç—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –æ—Ç–Ω–æ—Å—è—â–∏–µ—Å—è –∫ —Ç–µ—Å—Ç–∞–º —Å—É–¥–µ–π."""
        if self.all_results.empty:
            return pd.DataFrame()

        # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ —Ç–µ—Å—Ç—ã —Å—É–¥–µ–π –∏–º–µ—é—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        judge_categories = ['accuracy_test', 'verbosity_bias_test', 'positional_bias_test']
        judge_data = self.all_results[
            self.all_results['category'].isin(judge_categories)
        ].copy()

        log.info(f"–ù–∞–π–¥–µ–Ω–æ {len(judge_data)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—É–¥–µ–π")
        return judge_data

    def _calculate_accuracy_score(self, df: pd.DataFrame) -> pd.Series:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç Accuracy Score - —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–∞–∑–ª–∏—á–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏ –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ä–µ–∑—é–º–µ.
        –§–æ—Ä–º—É–ª–∞: (Avg_Score_Ideal - Avg_Score_Flawed) / 4
        """
        accuracy_scores = {}

        for model in df['model_name'].unique():
            model_data = df[df['model_name'] == model]

            # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —Ç–µ—Å—Ç—ã —Å –∏–¥–µ–∞–ª—å–Ω—ã–º–∏ –∏ –æ—à–∏–±–æ—á–Ω—ã–º–∏ —Ä–µ–∑—é–º–µ
            ideal_tests = model_data[model_data['test_variant'] == 'ideal']
            flawed_tests = model_data[model_data['test_variant'] == 'flawed']

            if not ideal_tests.empty and not flawed_tests.empty:
                avg_ideal = ideal_tests['score'].mean()
                avg_flawed = flawed_tests['score'].mean()
                accuracy_score = max(0, (avg_ideal - avg_flawed) / 4.0)
            else:
                accuracy_score = 0.0

            accuracy_scores[model] = accuracy_score

        return pd.Series(accuracy_scores, name="Accuracy_Score")

    def _calculate_stability_score(self, df: pd.DataFrame) -> pd.Series:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç Stability Score - —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–æ–∫ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—É—Å–∫–∞—Ö.
        –§–æ—Ä–º—É–ª–∞: 1 - (StdDev_Ideal + StdDev_Flawed) / 2
        """
        stability_scores = {}

        for model in df['model_name'].unique():
            model_data = df[df['model_name'] == model]

            ideal_tests = model_data[model_data['test_variant'] == 'ideal']
            flawed_tests = model_data[model_data['test_variant'] == 'flawed']

            if len(ideal_tests) > 1 and len(flawed_tests) > 1:
                std_ideal = ideal_tests['score'].std()
                std_flawed = flawed_tests['score'].std()
                avg_std = (std_ideal + std_flawed) / 2
                stability_score = max(0, 1 - avg_std / 4.0)  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ —à–∫–∞–ª–µ 0-1
            else:
                stability_score = 0.0

            stability_scores[model] = stability_score

        return pd.Series(stability_scores, name="Stability_Score")

    def _calculate_positional_resistance(self, df: pd.DataFrame) -> pd.Series:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–º—É —Å–º–µ—â–µ–Ω–∏—é.
        –§–æ—Ä–º—É–ª–∞: Correct_Content_Choices / Total_Comparisons
        """
        positional_scores = {}

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–µ—Å—Ç—ã –Ω–∞ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ —Å–º–µ—â–µ–Ω–∏–µ
        pos_tests = df[df['category'] == 'positional_bias_test']

        for model in pos_tests['model_name'].unique():
            model_data = pos_tests[pos_tests['model_name'] == model]

            run_a_choices = model_data[model_data['test_variant'] == 'run_A']['choice'].tolist()
            run_b_choices = model_data[model_data['test_variant'] == 'run_B']['choice'].tolist()

            correct_choices = 0
            total_pairs = min(len(run_a_choices), len(run_b_choices))

            for choice_a, choice_b in zip(run_a_choices, run_b_choices):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≤—ã–±–∏—Ä–∞–µ—Ç –ª–∏ –º–æ–¥–µ–ª—å –∫–æ–Ω—Ç–µ–Ω—Ç, –∞ –Ω–µ –ø–æ–∑–∏—Ü–∏—é
                if (choice_a == 'A' and choice_b == 'B') or (choice_a == 'B' and choice_b == 'A'):
                    correct_choices += 1

            if total_pairs > 0:
                positional_scores[model] = correct_choices / total_pairs
            else:
                positional_scores[model] = 0.0

        return pd.Series(positional_scores, name="Positional_Resistance")

    def _calculate_verbosity_resistance(self, df: pd.DataFrame) -> pd.Series:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Å–º–µ—â–µ–Ω–∏—é –º–Ω–æ–≥–æ—Å–ª–æ–≤–∏—è.
        –§–æ—Ä–º—É–ª–∞: 1 - abs(Avg_Score_Ideal - Avg_Score_Verbose) / 4
        """
        verbosity_scores = {}

        for model in df['model_name'].unique():
            model_data = df[df['model_name'] == model]

            ideal_tests = model_data[model_data['test_variant'] == 'ideal']
            verbose_tests = model_data[model_data['test_variant'] == 'verbose']

            if not ideal_tests.empty and not verbose_tests.empty:
                avg_ideal = ideal_tests['score'].mean()
                avg_verbose = verbose_tests['score'].mean()
                score_difference = abs(avg_ideal - avg_verbose)
                verbosity_resistance = max(0, 1 - score_difference / 4.0)
            else:
                verbosity_resistance = 0.0

            verbosity_scores[model] = verbosity_resistance

        return pd.Series(verbosity_scores, name="Verbosity_Resistance")

    def _calculate_format_adherence(self, df: pd.DataFrame) -> pd.Series:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö JSON-–æ—Ç–≤–µ—Ç–æ–≤.
        –§–æ—Ä–º—É–ª–∞: Valid_JSON_Responses / Total_Responses
        """
        format_scores = {}

        for model in df['model_name'].unique():
            model_data = df[df['model_name'] == model]

            total_responses = len(model_data)
            valid_responses = model_data['json_valid'].sum() if 'json_valid' in model_data.columns else total_responses

            if total_responses > 0:
                format_scores[model] = valid_responses / total_responses
            else:
                format_scores[model] = 0.0

        return pd.Series(format_scores, name="Format_Adherence")

    def _calculate_judge_rating(self, metrics: pd.DataFrame) -> pd.DataFrame:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–π—Ç–∏–Ω–≥ —Å—É–¥–µ–π —Å –≤–µ—Å–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏.
        """
        # –í–µ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏ (—Å—É–º–º–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å 1.0)
        weights = {
            'Accuracy_Score': 0.35,  # –°–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ - —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–∞–∑–ª–∏—á–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ
            'Stability_Score': 0.25,  # –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–æ–∫
            'Positional_Resistance': 0.15,  # –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–º—É —Å–º–µ—â–µ–Ω–∏—é
            'Verbosity_Resistance': 0.10,  # –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –º–Ω–æ–≥–æ—Å–ª–æ–≤–∏—é
            'Format_Adherence': 0.15  # –°–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç—É
        }

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π –∏—Ç–æ–≥–æ–≤—ã–π –±–∞–ª–ª
        metrics['Judge_Rating'] = 0.0
        for metric, weight in weights.items():
            if metric in metrics.columns:
                metrics['Judge_Rating'] += metrics[metric] * weight

        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏—Ç–æ–≥–æ–≤–æ–º—É —Ä–µ–π—Ç–∏–Ω–≥—É
        metrics.sort_values('Judge_Rating', ascending=False, inplace=True)
        metrics.insert(0, 'Rank', range(1, len(metrics) + 1))

        return metrics

    def generate_judge_leaderboard(self) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç—á–µ—Ç-—Ä–µ–π—Ç–∏–Ω–≥ –¥–ª—è LLM-—Å—É–¥–µ–π.
        """
        if self.judge_results.empty:
            return "# üèõÔ∏è –†–µ–π—Ç–∏–Ω–≥ LLM-–°—É–¥–µ–π\n\n–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å—É–¥–µ–π."

        timestamp = time.strftime('%Y-%m-%d %H:%M:%S')

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏
        accuracy_scores = self._calculate_accuracy_score(self.judge_results)
        stability_scores = self._calculate_stability_score(self.judge_results)
        positional_resistance = self._calculate_positional_resistance(self.judge_results)
        verbosity_resistance = self._calculate_verbosity_resistance(self.judge_results)
        format_adherence = self._calculate_format_adherence(self.judge_results)

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ –æ–¥–Ω—É —Ç–∞–±–ª–∏—Ü—É
        metrics = pd.DataFrame({
            'Model': accuracy_scores.index,
            'Accuracy_Score': accuracy_scores.values,
            'Stability_Score': stability_scores.reindex(accuracy_scores.index, fill_value=0).values,
            'Positional_Resistance': positional_resistance.reindex(accuracy_scores.index, fill_value=0).values,
            'Verbosity_Resistance': verbosity_resistance.reindex(accuracy_scores.index, fill_value=0).values,
            'Format_Adherence': format_adherence.reindex(accuracy_scores.index, fill_value=0).values
        }).set_index('Model')

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–π—Ç–∏–Ω–≥
        final_metrics = self._calculate_judge_rating(metrics)

        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        display_df = pd.DataFrame()
        display_df['üèÜ –†–∞–Ω–≥'] = final_metrics['Rank']
        display_df['ü§ñ –ú–æ–¥–µ–ª—å'] = final_metrics.index
        display_df['‚≠ê –†–µ–π—Ç–∏–Ω–≥'] = final_metrics['Judge_Rating'].map(lambda x: f"{x:.3f}")
        display_df['üéØ –¢–æ—á–Ω–æ—Å—Ç—å'] = final_metrics['Accuracy_Score'].map(lambda x: f"{x:.3f}")
        display_df['üìä –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å'] = final_metrics['Stability_Score'].map(lambda x: f"{x:.3f}")
        display_df['üîÑ –ê–Ω—Ç–∏-–ø–æ–∑–∏—Ü–∏—è'] = final_metrics['Positional_Resistance'].map(lambda x: f"{x:.3f}")
        display_df['üìù –ê–Ω—Ç–∏-–±–æ–ª—Ç–æ–≤–Ω—è'] = final_metrics['Verbosity_Resistance'].map(lambda x: f"{x:.3f}")
        display_df['‚úÖ –§–æ—Ä–º–∞—Ç'] = final_metrics['Format_Adherence'].map(lambda x: f"{x:.1%}")

        display_df.set_index('üèÜ –†–∞–Ω–≥', inplace=True)

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç—á–µ—Ç
        report_md = f"# üèõÔ∏è –†–µ–π—Ç–∏–Ω–≥ LLM-–°—É–¥–µ–π: –ö—Ç–æ –ª—É—á—à–∏–π –∞—Ä–±–∏—Ç—Ä?\n\n"
        report_md += f"*–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {timestamp}*\n\n"

        # –û–±—ä—è—Å–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
        report_md += "## üìã –ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏ —Å—É–¥–µ–π\n\n"
        report_md += "| –ú–µ—Ç—Ä–∏–∫–∞ | –í–µ—Å | –û–ø–∏—Å–∞–Ω–∏–µ |\n"
        report_md += "|---------|-----|----------|\n"
        report_md += "| üéØ **–¢–æ—á–Ω–æ—Å—Ç—å** | 35% | –°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–∞–∑–ª–∏—á–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∏ –Ω–µ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã. –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –∫–∞–∫ —Ä–∞–∑–Ω–æ—Å—Ç—å —Å—Ä–µ–¥–Ω–∏—Ö –æ—Ü–µ–Ω–æ–∫ –¥–ª—è –∏–¥–µ–∞–ª—å–Ω—ã—Ö –∏ –æ—à–∏–±–æ—á–Ω—ã—Ö —Ä–µ–∑—é–º–µ. |\n"
        report_md += "| üìä **–°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å** | 25% | –ö–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–æ–∫ –ø—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—É—Å–∫–∞—Ö –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ —Ç–µ—Å—Ç–∞. –ò–∑–º–µ—Ä—è–µ—Ç—Å—è —á–µ—Ä–µ–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ. |\n"
        report_md += "| üîÑ **–ê–Ω—Ç–∏-–ø–æ–∑–∏—Ü–∏—è** | 15% | –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–º—É —Å–º–µ—â–µ–Ω–∏—é (–ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—é –ø–µ—Ä–≤–æ–≥–æ/–ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≤–∞—Ä–∏–∞–Ω—Ç–∞ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è). |\n"
        report_md += "| üìù **–ê–Ω—Ç–∏-–±–æ–ª—Ç–æ–≤–Ω—è** | 10% | –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ —Å–º–µ—â–µ–Ω–∏—é –≤ –ø–æ–ª—å–∑—É –º–Ω–æ–≥–æ—Å–ª–æ–≤–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞–¥ –ª–∞–∫–æ–Ω–∏—á–Ω—ã–º–∏ –ø—Ä–∏ —Ä–∞–≤–Ω–æ–º –∫–∞—á–µ—Å—Ç–≤–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è. |\n"
        report_md += "| ‚úÖ **–§–æ—Ä–º–∞—Ç** | 15% | –ü—Ä–æ—Ü–µ–Ω—Ç –æ—Ç–≤–µ—Ç–æ–≤ –≤ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–º JSON-—Ñ–æ—Ä–º–∞—Ç–µ. –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å —Å—É–¥—å–∏. |\n\n"

        # –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
        report_md += "## üèÜ –¢–∞–±–ª–∏—Ü–∞ –ª–∏–¥–µ—Ä–æ–≤\n\n"
        report_md += "> _–ß–µ–º –≤—ã—à–µ –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–π—Ç–∏–Ω–≥, —Ç–µ–º –Ω–∞–¥–µ–∂–Ω–µ–µ –º–æ–¥–µ–ª—å –≤ —Ä–æ–ª–∏ –æ–±—ä–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Å—É–¥—å–∏. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª: 1.000_\n\n"
        report_md += self._to_markdown_table(display_df)

        # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if not final_metrics.empty:
            best_judge = final_metrics.index[0]
            best_score = final_metrics.iloc['Judge_Rating']

            report_md += f"\n### ü•á –õ–∏–¥–µ—Ä: {best_judge}\n\n"
            report_md += f"**–ò—Ç–æ–≥–æ–≤—ã–π —Ä–µ–π—Ç–∏–Ω–≥:** {best_score:.3f}/1.000\n\n"

            # –ê–Ω–∞–ª–∏–∑ —Å–∏–ª—å–Ω—ã—Ö —Å—Ç–æ—Ä–æ–Ω –ª–∏–¥–µ—Ä–∞
            leader_metrics = final_metrics.iloc[0]
            strengths = []

            if leader_metrics['Accuracy_Score'] > 0.8:
                strengths.append("–æ—Ç–ª–∏—á–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–∞–∑–ª–∏—á–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤")
            if leader_metrics['Stability_Score'] > 0.8:
                strengths.append("–≤—ã—Å–æ–∫–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–æ–∫")
            if leader_metrics['Positional_Resistance'] > 0.7:
                strengths.append("—É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å –∫ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–º —Å–º–µ—â–µ–Ω–∏—è–º")
            if leader_metrics['Format_Adherence'] > 0.9:
                strengths.append("–Ω–∞–¥–µ–∂–Ω–æ–µ —Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º")

            if strengths:
                report_md += f"**–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:** {', '.join(strengths)}.\n\n"

        # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        report_md += "## üìà –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n"

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Ç–µ—Å—Ç–∞–º
        if not self.judge_results.empty:
            test_stats = self.judge_results.groupby(['model_name', 'category']).agg({
                'is_correct': ['count', 'sum', 'mean'],
                'execution_time_ms': 'mean'
            }).round(3)

            test_stats.columns = ['–¢–µ—Å—Ç–æ–≤', '–£—Å–ø–µ—à–Ω–æ', '–¢–æ—á–Ω–æ—Å—Ç—å', '–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (–º—Å)']
            test_stats['–¢–æ—á–Ω–æ—Å—Ç—å'] = test_stats['–¢–æ—á–Ω–æ—Å—Ç—å'].map(lambda x: f"{x:.1%}")
            test_stats['–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (–º—Å)'] = test_stats['–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è (–º—Å)'].map(lambda x: f"{x:,.0f}")

            report_md += self._to_markdown_table(test_stats)

        report_md += "\n---\n"
        report_md += "*–≠—Ç–æ—Ç —Ä–µ–π—Ç–∏–Ω–≥ –ø–æ–º–æ–≥–∞–µ—Ç –≤—ã–±—Ä–∞—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ —Ç–µ–∫—Å—Ç–æ–≤.*\n"

        return report_md

    def generate_comprehensive_report(self) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç, –≤–∫–ª—é—á–∞—é—â–∏–π –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π –ª–∏–¥–µ—Ä–±–æ—Ä–¥, —Ç–∞–∫ –∏ —Ä–µ–π—Ç–∏–Ω–≥ —Å—É–¥–µ–π.
        """
        main_report = self.generate_leaderboard_report()
        judge_report = self.generate_judge_leaderboard()

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ—Ç—á–µ—Ç—ã
        comprehensive_report = main_report + "\n\n" + "=" * 50 + "\n\n" + judge_report

        return comprehensive_report
