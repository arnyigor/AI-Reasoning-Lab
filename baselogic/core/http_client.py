import requests
import json
import logging
from typing import Dict, Any, Optional

from .interfaces import LLMClientError, LLMTimeoutError, LLMConnectionError, LLMResponseError
from .base_client import BaseLLMClient
from .types import ModelOptions


class OpenAICompatibleClient(BaseLLMClient):
    """
    HTTP-–∫–ª–∏–µ–Ω—Ç –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –ª—é–±—ã–º —Å–µ—Ä–≤–µ—Ä–æ–º, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—â–∏–º
    OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç /chat/completions.
    (–Ω–∞–ø—Ä–∏–º–µ—Ä, LM Studio, Jan, vLLM).
    """
    def __init__(self, model_name: str, api_base: str, api_key: Optional[str] = None, model_options: Optional[ModelOptions] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç HTTP-–∫–ª–∏–µ–Ω—Ç.

        Args:
            model_name (str): –ò–º—è –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–æ–µ –±—É–¥–µ—Ç –ø–µ—Ä–µ–¥–∞–Ω–æ –≤ —Ç–µ–ª–µ –∑–∞–ø—Ä–æ—Å–∞.
            api_base (str): –ë–∞–∑–æ–≤—ã–π URL —Å–µ—Ä–≤–µ—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "http://localhost:1234/v1").
            api_key (Optional[str]): –ù–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –∫–ª—é—á API.
            model_options (Optional[ModelOptions]): –°–ª–æ–≤–∞—Ä—å —Å –æ–ø—Ü–∏—è–º–∏.
        """
        super().__init__(model_name, model_options)
        
        self.api_url = f"{api_base.rstrip('/')}/chat/completions"
        self.api_key = api_key
        
        self.logger.info("‚úÖ OpenAICompatibleClient –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è '%s' –ø–æ –∞–¥—Ä–µ—Å—É: %s", model_name, self.api_url)

    def _execute_query(self, user_prompt: str) -> str:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç HTTP POST-–∑–∞–ø—Ä–æ—Å –Ω–∞ —Å–µ—Ä–≤–µ—Ä –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç.
        """
        headers = {
            "Content-Type": "application/json",
        }
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"

        messages = self._prepare_messages(user_prompt)

        # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–ª–æ –∑–∞–ø—Ä–æ—Å–∞ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–µ–π OpenAI
        payload = {
            "model": self.model_name,
            "messages": messages,
            "stream": False,
            **self.generation_opts # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –æ–ø—Ü–∏–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: temp, stop, etc.
        }

        # --- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ ---
        log_message = (
            f"REQUEST (HTTP):\n"
            f"  URL: {self.api_url}\n"
            f"  Headers: {headers}\n"
            f"  Payload: {json.dumps(payload, indent=2, ensure_ascii=False)}\n\n"
        )
        self.logger.debug(log_message)

        try:
            self.logger.info("    üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ API (—Ç–∞–π–º–∞—É—Ç: %d—Å)...", self.query_timeout)
            response = requests.post(self.api_url, headers=headers, json=payload, timeout=self.query_timeout)
            response.raise_for_status()  # –í—ã–∑–æ–≤–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –∫–æ–¥–æ–≤ 4xx/5xx

            data = response.json()

            if not data.get('choices'):
                raise ValueError("–û—Ç–≤–µ—Ç API –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á 'choices'")

            full_response_text = data['choices'][0]['message']['content']

            log_message += f"RESPONSE (Success):\n{full_response_text}"
            self.logger.info("    ‚úÖ –û—Ç–≤–µ—Ç –æ—Ç API –ø–æ–ª—É—á–µ–Ω.")
            self.logger.debug(log_message)
            return self._validate_response(full_response_text)

        except requests.exceptions.Timeout:
            error_details = "RESPONSE (HTTP Timeout Error): –°–µ—Ä–≤–µ—Ä –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –∑–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è."
            self.logger.error(error_details)
            raise LLMTimeoutError(error_details)
        except requests.exceptions.RequestException as e:
            error_details = f"RESPONSE (HTTP Request Error):\n{e}"
            log_message += error_details
            self.logger.error(log_message, exc_info=True)
            raise LLMConnectionError(f"HTTP_ERROR: {e}") from e
        except (ValueError, KeyError, IndexError) as e:
            raw_response = response.text if 'response' in locals() else "No response received"
            error_details = f"RESPONSE (JSON Parsing Error):\n{e}\nRaw Response: {raw_response}"
            log_message += error_details
            self.logger.error(log_message, exc_info=True)
            raise LLMResponseError(f"JSON_PARSING_ERROR: {e}") from e

    def get_model_info(self) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏.
        –î–ª—è HTTP-–∫–ª–∏–µ–Ω—Ç–æ–≤ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.
        """
        self.logger.info("    ‚öôÔ∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π –¥–ª—è API-–º–æ–¥–µ–ª–∏: %s", self.model_name)
        
        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        base_info = super().get_model_info()
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—É—é –¥–ª—è HTTP –∫–ª–∏–µ–Ω—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        http_info = {
            "client_type": "openai_compatible",
            "api_url": self.api_url,
            "modelfile": "N/A (API)",
            "parameters": "N/A (API)",
            "template": self.prompting_opts.get('template', 'N/A (API)'),
            "details": {
                "family": "api",  # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
                "parameter_size": "N/A",
                "quantization_level": "API",
                "format": "api"
            },
            "object": "model"
        }
        
        base_info.update(http_info)
        return base_info

