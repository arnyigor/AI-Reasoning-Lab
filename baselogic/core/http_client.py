import requests
import json
import logging
from typing import Dict, Any, Optional

from .interfaces import LLMClientError, LLMTimeoutError, LLMConnectionError, LLMResponseError
from .base_client import BaseLLMClient
from .types import ModelOptions


class OpenAICompatibleClient(BaseLLMClient):
    """
    HTTP-–∫–ª–∏–µ–Ω—Ç –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –ª—é–±—ã–º —Å–µ—Ä–≤–µ—Ä–æ–º, –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—â–∏–º
    OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —ç–Ω–¥–ø–æ–∏–Ω—Ç /chat/completions.
    (–Ω–∞–ø—Ä–∏–º–µ—Ä, LM Studio, Jan, vLLM).
    """
    def __init__(self, model_name: str, api_base: str, api_key: Optional[str] = None, model_options: Optional[ModelOptions] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç HTTP-–∫–ª–∏–µ–Ω—Ç.

        Args:
            model_name (str): –ò–º—è –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–æ–µ –±—É–¥–µ—Ç –ø–µ—Ä–µ–¥–∞–Ω–æ –≤ —Ç–µ–ª–µ –∑–∞–ø—Ä–æ—Å–∞.
            api_base (str): –ë–∞–∑–æ–≤—ã–π URL —Å–µ—Ä–≤–µ—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "http://localhost:1234/v1").
            api_key (Optional[str]): –ù–µ–æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–π –∫–ª—é—á API.
            model_options (Optional[ModelOptions]): –°–ª–æ–≤–∞—Ä—å —Å –æ–ø—Ü–∏—è–º–∏.
        """
        super().__init__(model_name, model_options)
        
        self.api_url = f"{api_base.rstrip('/')}/chat/completions"
        self.api_key = api_key
        
        self.logger.info("‚úÖ OpenAICompatibleClient –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è '%s' –ø–æ –∞–¥—Ä–µ—Å—É: %s", model_name, self.api_url)

    def _execute_query(self, user_prompt: str) -> Dict[str, Any]:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç HTTP POST-–∑–∞–ø—Ä–æ—Å –Ω–∞ OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å–µ—Ä–≤–µ—Ä,
        –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Å—Ç—Ä–∏–º–∏–Ω–≥ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç.
        """
        headers = {
            "Content-Type": "application/json",
        }
        if self.api_key:
            headers["Authorization"] = f"Bearer {self.api_key}"

        messages = self._prepare_messages(user_prompt)

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º –æ–ø—Ü–∏—é —Å—Ç—Ä–∏–º–∏–Ω–≥–∞, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–¥–∞–≤–∞—Ç—å –µ–µ –¥–≤–∞–∂–¥—ã
        # .pop() —É–¥–∞–ª—è–µ—Ç –∫–ª—é—á –∏–∑ —Å–ª–æ–≤–∞—Ä—è, —á—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –æ—à–∏–±–∫–∏
        use_stream = self.generation_opts.pop('stream', False)

        payload = {
            "model": self.model_name,
            "messages": messages,
            "stream": use_stream,
            **self.generation_opts  # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ –æ–ø—Ü–∏–∏: temp, stop, etc.
        }

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 'stream' –æ–±—Ä–∞—Ç–Ω–æ –≤ –æ–ø—Ü–∏–∏ –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è, –µ—Å–ª–∏ —ç—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ
        self.generation_opts['stream'] = use_stream

        log_message = (
            f"REQUEST (HTTP):\n"
            f"  URL: {self.api_url}\n"
            f"  Payload: {json.dumps(payload, indent=2, ensure_ascii=False)}\n\n"
        )
        self.logger.debug(log_message)

        try:
            self.logger.info("    üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ API (stream=%s, timeout=%d—Å)...", use_stream, self.query_timeout)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º
            with requests.post(self.api_url, headers=headers, json=payload, timeout=self.query_timeout, stream=use_stream) as response:
                response.raise_for_status() # –í—ã–∑–æ–≤–µ—Ç –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–ª—è –∫–æ–¥–æ–≤ 4xx/5xx

                if use_stream:
                    # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ ---
                    full_response_content = []
                    print("    [STREAM] –û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: ", end='', flush=True)
                    for line in response.iter_lines():
                        if line:
                            decoded_line = line.decode('utf-8')
                            if decoded_line.startswith('data: '):
                                json_str = decoded_line[6:]
                                if json_str.strip() == '[DONE]':
                                    break
                                try:
                                    chunk = json.loads(json_str)
                                    content_part = chunk.get('choices', [{}])[0].get('delta', {}).get('content', '')
                                    if content_part:
                                        print(content_part, end='', flush=True)
                                        full_response_content.append(content_part)
                                except json.JSONDecodeError:
                                    self.logger.debug("–ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–µ-JSON —Å—Ç—Ä–æ–∫–∞ –≤ —Å—Ç—Ä–∏–º–µ: %s", decoded_line)
                    print() # –ü–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏ –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Å—Ç—Ä–∏–º–∞
                    final_content = "".join(full_response_content)
                else:
                    # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–±—ã—á–Ω–æ–≥–æ (–±–ª–æ–∫–∏—Ä—É—é—â–µ–≥–æ) –æ—Ç–≤–µ—Ç–∞ ---
                    data = response.json()
                    if not data.get('choices'):
                        raise ValueError("–û—Ç–≤–µ—Ç API –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á 'choices'")
                    final_content = data['choices']['message']['content']

            self.logger.info("    ‚úÖ –û—Ç–≤–µ—Ç –æ—Ç API —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω.")

            validated_response = self._validate_response(final_content)

            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–ª–æ–≤–∞—Ä—å, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π –Ω–æ–≤–æ–º—É –∫–æ–Ω—Ç—Ä–∞–∫—Ç—É ILLMClient
            return {
                "thinking_response": "", # OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ –∫–ª–∏–µ–Ω—Ç—ã –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç "thinking"
                "llm_response": validated_response
            }

        except requests.exceptions.Timeout:
            error_details = "RESPONSE (HTTP Timeout Error): –°–µ—Ä–≤–µ—Ä –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª –∑–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è."
            self.logger.error(error_details)
            raise LLMTimeoutError(error_details)
        except requests.exceptions.RequestException as e:
            error_details = f"RESPONSE (HTTP Request Error):\n{e}"
            self.logger.error(error_details, exc_info=True)
            raise LLMConnectionError(f"HTTP_ERROR: {e}") from e
        except (ValueError, KeyError, IndexError, json.JSONDecodeError) as e:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –æ—Ç–≤–µ—Ç–∞
            raw_response_text = response.text if 'response' in locals() and hasattr(response, 'text') else "No response received"
            error_details = f"RESPONSE (Parsing Error):\n{e}\nRaw Response: {raw_response_text[:500]}"
            self.logger.error(error_details, exc_info=True)
            raise LLMResponseError(f"PARSING_ERROR: {e}") from e

    def get_model_info(self) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏.
        –î–ª—è HTTP-–∫–ª–∏–µ–Ω—Ç–æ–≤ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é.
        """
        self.logger.info("    ‚öôÔ∏è –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –¥–µ—Ç–∞–ª–µ–π –¥–ª—è API-–º–æ–¥–µ–ª–∏: %s", self.model_name)
        
        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        base_info = super().get_model_info()
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—É—é –¥–ª—è HTTP –∫–ª–∏–µ–Ω—Ç–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        http_info = {
            "client_type": "openai_compatible",
            "api_url": self.api_url,
            "modelfile": "N/A (API)",
            "parameters": "N/A (API)",
            "template": self.prompting_opts.get('template', 'N/A (API)'),
            "details": {
                "family": "api",  # –ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏
                "parameter_size": "N/A",
                "quantization_level": "API",
                "format": "api"
            },
            "object": "model"
        }
        
        base_info.update(http_info)
        return base_info

