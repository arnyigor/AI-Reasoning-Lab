import threading
import threading
import time
from typing import Dict, Any, Optional

import ollama
import requests

from .base_client import BaseLLMClient
from .interfaces import LLMResponseError
from .logger import llm_logger
from .types import ModelOptions


class OllamaClient(BaseLLMClient):
    """
    –ö–ª–∞—Å—Å-–∫–ª–∏–µ–Ω—Ç –¥–ª—è –∏–Ω–∫–∞–ø—Å—É–ª—è—Ü–∏–∏ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å API Ollama.
    """

    def __init__(self, model_name: str, model_options: Optional[ModelOptions] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∫–ª–∏–µ–Ω—Ç.
        """
        super().__init__(model_name, model_options)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å, –Ω–æ –ù–ï –∑–∞–≤–µ—Ä—à–∞–µ–º –ø—Ä–æ–≥—Ä–∞–º–º—É
        skip_validation = self.model_options.get('skip_model_validation', False)
        if not skip_validation:
            self._check_model_exists()
        else:
            self.logger.info("‚ö†Ô∏è –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–æ–ø—É—â–µ–Ω–∞")

    def _check_model_exists(self):
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –º–æ–¥–µ–ª—å –±–µ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–∞–º–º—ã.
        """
        target_model = self.model_name
        ollama_api_url = "http://127.0.0.1:11434/api/tags"

        try:
            self.logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –º–æ–¥–µ–ª–∏ '%s'...", target_model)
            response = requests.get(ollama_api_url, timeout=5)
            response.raise_for_status()

            data = response.json()
            model_list = data.get('models', [])

            local_models = [
                model_info.get('name') or model_info.get('model', '')
                for model_info in model_list
                if isinstance(model_info, dict)
            ]

            if target_model in local_models:
                self.logger.info("‚úÖ –ú–æ–¥–µ–ª—å '%s' –Ω–∞–π–¥–µ–Ω–∞", target_model)
            else:
                self.logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å '%s' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Å–ø–∏—Å–∫–µ. –î–æ—Å—Ç—É–ø–Ω—ã–µ: %s",
                                    target_model, local_models[:3])

        except Exception as e:
            self.logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –º–æ–¥–µ–ª—å: %s. –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É.", e)

    def get_model_info(self) -> Dict[str, Any]:
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏ –≤ –≤–∏–¥–µ —Å–ª–æ–≤–∞—Ä—è.
        –ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –±–∞–∑–æ–≤—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª–µ–π –æ—Ç Ollama.
        """
        self.logger.info("–ó–∞–ø—Ä–æ—Å –¥–µ—Ç–∞–ª–µ–π –¥–ª—è –º–æ–¥–µ–ª–∏: %s", self.model_name)
        try:
            response = ollama.show(self.model_name)

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ–±—ä–µ–∫—Ç –≤ —á–∏—Å—Ç—ã–π —Å–ª–æ–≤–∞—Ä—å
            details_dict = {
                "model_name": self.model_name,
                "modelfile": response.get("modelfile"),
                "parameters": response.get("parameters"),
                "template": response.get("template"),
                "details": {}
            }

            # –ë–µ–∑–æ–ø–∞—Å–Ω–æ –∏–∑–≤–ª–µ–∫–∞–µ–º details
            if hasattr(response, 'details') and response.details:
                details_obj = response.details
                details_dict["details"] = {
                    "family": getattr(details_obj, 'family', 'N/A'),
                    "parameter_size": getattr(details_obj, 'parameter_size', 'N/A'),
                    "quantization_level": getattr(details_obj, 'quantization_level', 'N/A'),
                    "format": getattr(details_obj, 'format', 'N/A')
                }

            # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –±–∞–∑–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∞
            base_info = super().get_model_info()
            details_dict.update(base_info)

            return details_dict

        except Exception as e:
            error_details = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –¥–µ—Ç–∞–ª–µ–π –º–æ–¥–µ–ª–∏: {e}"
            self.logger.error(error_details)
            raise LLMResponseError(error_details) from e

    def _execute_query(self, user_prompt: str) -> str:
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–±—Ä–∞–Ω–∞ –ø–æ–ø—ã—Ç–∫–∞ –ø–µ—Ä–µ–¥–∞—á–∏ timeout –≤ ollama.chat()
        """
        messages = []
        if self.system_prompt:
            messages.append({'role': 'system', 'content': self.system_prompt})
        messages.append({'role': 'user', 'content': user_prompt})

        llm_logger.info("    üöÄ –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ –º–æ–¥–µ–ª–∏ '%s'...", self.model_name)
        llm_logger.info("    ‚è∞ –¢–∞–π–º–∞—É—Ç: %d —Å–µ–∫—É–Ω–¥", self.query_timeout)

        # –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –ø–æ—Ç–æ–∫–∞
        result = [None]
        error = [None]
        completed = [False]

        def make_request():
            """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ."""
            try:
                # –ò–°–ü–†–ê–í–õ–ï–ù–û: –ù–ï –ø–µ—Ä–µ–¥–∞–µ–º timeout –≤ options
                response = ollama.chat(
                    model=self.model_name,
                    messages=messages,
                    options=self.generation_opts,  # –ë–ï–ó timeout
                    stream=False
                )
                result[0] = response['message']['content'].strip()
                completed[0] = True
            except Exception as e:
                error[0] = e
                completed[0] = True

        # –û—Å—Ç–∞–ª—å–Ω–æ–π –∫–æ–¥ —Å threading –æ—Å—Ç–∞–µ—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π
        start_time = time.time()
        thread = threading.Thread(target=make_request)
        thread.daemon = True
        thread.start()

        elapsed = 0
        while elapsed < self.query_timeout and not completed[0]:
            time.sleep(1)
            elapsed += 1

            if elapsed % 10 == 0:
                llm_logger.info("    ‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞... (%d—Å/%d—Å)", elapsed, self.query_timeout)

        end_time = time.time()
        execution_time = end_time - start_time

        if completed[0]:
            if error[0]:
                if hasattr(error[0], 'error'):
                    llm_logger.error("    üö´ Ollama API Error: %s", error[0].error)
                    return f"API_ERROR: {error[0].error}"
                else:
                    llm_logger.error("    üí• –û—à–∏–±–∫–∞: %s", str(error[0]), exc_info=True)
                    return f"UNEXPECTED_ERROR: {str(error[0])}"
            else:
                llm_logger.info("    ‚úÖ –û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ %.1f—Å", execution_time)
                return result[0]
        else:
            llm_logger.error("    ‚è±Ô∏è –¢–ê–ô–ú–ê–£–¢: –ú–æ–¥–µ–ª—å –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª–∞ –∑–∞ %d —Å–µ–∫—É–Ω–¥", self.query_timeout)
            return f"TIMEOUT_ERROR: –ú–æ–¥–µ–ª—å –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª–∞ –∑–∞ {self.query_timeout} —Å–µ–∫—É–Ω–¥"
