import json
import logging
import time
from typing import List, Dict, Tuple, Optional, Any, Union, Iterable, Generator

import requests

from .interfaces import ProviderClient, LLMResponseError, LLMConnectionError

log = logging.getLogger(__name__)


def _log_chunk_smart(chunk: Any, chunk_number: int, level: str = "INFO") -> None:
    """–£–º–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∞–Ω–∫–æ–≤ —Å –∫–æ–Ω—Ç—Ä–æ–ª–µ–º –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏."""
    try:
        if level == "DEBUG":
            json_str = json.dumps(chunk, indent=2, ensure_ascii=False)
            snippet = (json_str[:1500] + '\n...(–æ–±—Ä–µ–∑–∞–Ω–æ)') if len(json_str) > 1500 else json_str
            log.debug("üì¶ GEMINI CHUNK #%d (FULL):\n%s", chunk_number, snippet)

        elif level == "INFO":
            chunk_type = type(chunk).__name__
            content_preview = ""

            if isinstance(chunk, list) and chunk:
                if isinstance(chunk[0], dict) and "candidates" in chunk[0]:
                    candidates = chunk[0]["candidates"]
                    if candidates and "content" in candidates[0]:
                        parts = candidates[0]["content"].get("parts", [])
                        if parts and "text" in parts[0]:
                            text = parts[0]["text"]
                            content_preview = f" | –¢–µ–∫—Å—Ç: '{text[:50]}...'" if len(text) > 50 else f" | –¢–µ–∫—Å—Ç: '{text}'"

            elif isinstance(chunk, dict) and "candidates" in chunk:
                candidates = chunk["candidates"]
                if candidates and "content" in candidates[0]:
                    parts = candidates[0]["content"].get("parts", [])
                    if parts and "text" in parts[0]:
                        text = parts[0]["text"]
                        content_preview = f" | –¢–µ–∫—Å—Ç: '{text[:50]}...'" if len(text) > 50 else f" | –¢–µ–∫—Å—Ç: '{text}'"

            log.info("üì¶ CHUNK #%d: %s%s", chunk_number, chunk_type, content_preview)

        else:
            log.info("üì¶ CHUNK #%d: %s", chunk_number, type(chunk).__name__)

    except Exception as e:
        log.warning("–û—à–∏–±–∫–∞ –ø—Ä–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–∏ —á–∞–Ω–∫–∞ #%d: %s", chunk_number, e)

class GeminiClient(ProviderClient):
    """
    –ö–ª–∏–µ–Ω—Ç –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å Google Gemini API.
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ –æ–±—ã—á–Ω—ã–π, —Ç–∞–∫ –∏ –ø–æ—Ç–æ–∫–æ–≤—ã–π —Ä–µ–∂–∏–º—ã.
    """

    def __init__(self, api_key: str, base_url: str = "https://generativelanguage.googleapis.com/v1"):
        if not api_key:
            raise ValueError("–î–ª—è GeminiClient —Ç—Ä–µ–±—É–µ—Ç—Å—è api_key.")
        self.api_key = api_key
        self.base_url = base_url.rstrip('/')
        self.session = requests.Session()
        self.session.headers.update({
            "Content-Type": "application/json",
            "x-goog-api-key": api_key  # Gemini –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —ç—Ç–æ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫
        })
        log.info("GeminiClient –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å base_url: %s", self.base_url)

    def _translate_messages_to_gemini(self, messages: List[Dict[str, str]]) -> Tuple[List[Dict], Optional[Dict]]:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç messages –≤ —Ñ–æ—Ä–º–∞—Ç Gemini."""
        gemini_contents = []
        system_instruction = None

        for msg in messages:
            role = msg.get("role")
            content = msg.get("content", "")

            if role == "system":
                system_instruction = {"parts": [{"text": content}]}
                continue
            elif role == "assistant":
                gemini_role = "model"
            else:  # user –∏–ª–∏ –¥—Ä—É–≥–∏–µ —Ä–æ–ª–∏
                gemini_role = "user"

            gemini_contents.append({
                "role": gemini_role,
                "parts": [{"text": content}]
            })

        return gemini_contents, system_instruction

    def prepare_payload(self, messages: List[Dict[str, str]], model: str, *, stream: bool = False, **kwargs: Any) -> \
            Dict[str, Any]:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç payload –¥–ª—è Gemini API."""
        log.debug("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ payload –¥–ª—è Gemini –º–æ–¥–µ–ª–∏ '%s'...", model)

        gemini_contents, system_instruction = self._translate_messages_to_gemini(messages)

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        generation_config = {}
        if "temperature" in kwargs and kwargs["temperature"] is not None:
            generation_config["temperature"] = kwargs["temperature"]
        if "max_tokens" in kwargs and kwargs["max_tokens"] is not None:
            generation_config["maxOutputTokens"] = kwargs["max_tokens"]
        if "top_p" in kwargs and kwargs["top_p"] is not None:
            generation_config["topP"] = kwargs["top_p"]
        if "top_k" in kwargs and kwargs["top_k"] is not None:
            generation_config["topK"] = kwargs["top_k"]
        if "stop" in kwargs and kwargs["stop"] is not None:
            generation_config["stopSequences"] = kwargs["stop"]

        # –ë–∞–∑–æ–≤—ã–π payload
        payload: Dict[str, Any] = {
            "contents": gemini_contents
        }

        if generation_config:
            payload["generationConfig"] = generation_config

        if system_instruction:
            payload["systemInstruction"] = system_instruction

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è send_request
        payload["_model_name"] = model
        payload["_stream_mode"] = stream

        log.debug("Gemini payload –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω –¥–ª—è –º–æ–¥–µ–ª–∏ %s", model)
        return payload

    def send_request(self, payload: Dict[str, Any]) -> Union[Dict[str, Any], Iterable[Dict[str, Any]]]:
        """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ Gemini API."""
        model_name = payload.pop("_model_name")
        is_stream = payload.pop("_stream_mode", False)
        timeout = payload.pop('timeout', 180)

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º endpoint
        action = "streamGenerateContent" if is_stream else "generateContent"
        endpoint = f"{self.base_url}/models/{model_name}:{action}"

        log.info("–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ Gemini endpoint: %s (stream=%s)", endpoint, is_stream)
        log.debug("Gemini payload: %s", json.dumps(payload, indent=2, ensure_ascii=False))

        try:
            resp = self.session.post(
                endpoint,
                json=payload,
                stream=is_stream,
                timeout=timeout,
                params={"key": self.api_key}
            )
            resp.raise_for_status()

            log.info("–ó–∞–ø—Ä–æ—Å –∫ Gemini —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω.")

            if is_stream:
                return self._handle_stream(resp)
            else:
                return resp.json()

        except requests.exceptions.RequestException as e:
            log.error("–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Gemini API: %s", e)

            # –ü–æ–ø—ã—Ç–∫–∞ –∏–∑–≤–ª–µ—á—å –¥–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏
            try:
                if hasattr(e, 'response') and e.response:
                    error_details = e.response.json()
                    log.error("–î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏ –æ—Ç Gemini API: %s", error_details)

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –æ–± –æ—à–∏–±–∫–µ
                    error_message = error_details.get('error', {}).get('message', str(e))
                    raise LLMResponseError(f"Gemini API error: {error_message}") from e
            except (ValueError, json.JSONDecodeError):
                pass

            raise LLMConnectionError(f"–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ Gemini: {e}") from e

    def extract_choices(self, response: Union[Dict[str, Any], List[Any]]) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤ –∏–∑ –æ—Ç–≤–µ—Ç–∞ Gemini —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤."""

        if isinstance(response, dict):
            candidates = response.get("candidates", [])
            if isinstance(candidates, list):
                return candidates

        elif isinstance(response, list):
            # –ï—Å–ª–∏ response —Å–∞–º —è–≤–ª—è–µ—Ç—Å—è —Å–ø–∏—Å–∫–æ–º –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤
            return response

        log.warning("–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç response –≤ extract_choices: %s", type(response))
        return []

    def extract_metadata_from_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –æ—Ç–≤–µ—Ç–∞ Gemini."""
        try:
            metadata = {}

            # –ü—Ä—è–º—ã–µ –ø–æ–ª—è –∏–∑ –æ—Ç–≤–µ—Ç–∞
            direct_fields = [
                "modelVersion",
                "usageMetadata",
                "safetyRatings"
            ]

            for field in direct_fields:
                if field in response:
                    metadata[field] = response[field]

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ –∏–∑ usageMetadata
            usage_metadata = response.get("usageMetadata", {})
            if usage_metadata:
                if "promptTokenCount" in usage_metadata:
                    metadata["prompt_eval_count"] = usage_metadata["promptTokenCount"]
                if "candidatesTokenCount" in usage_metadata:
                    metadata["eval_count"] = usage_metadata["candidatesTokenCount"]
                if "totalTokenCount" in usage_metadata:
                    metadata["total_token_count"] = usage_metadata["totalTokenCount"]

            # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏ –∏–∑ candidates
            candidates = response.get("candidates", [])
            if candidates:
                candidate = candidates[0]
                if "finishReason" in candidate:
                    metadata["finish_reason"] = candidate["finishReason"]
                if "safetyRatings" in candidate:
                    metadata["safety_ratings"] = candidate["safetyRatings"]

            return {k: v for k, v in metadata.items() if v is not None}

        except Exception as e:
            log.warning("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ –æ—Ç–≤–µ—Ç–∞ Gemini: %s", e)
            return {}

    def extract_metadata_from_chunk(self, chunk: Union[Dict[str, Any], List[Any]]) -> Optional[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ —á–∞–Ω–∫–∞ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤."""

        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º chunk –∫ —Å–ª–æ–≤–∞—Ä—é
        normalized_chunk = None

        if isinstance(chunk, dict):
            normalized_chunk = chunk
        elif isinstance(chunk, list) and len(chunk) > 0:
            # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤, —Å–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å
            normalized_chunk = {"candidates": chunk}
        else:
            return None

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —á–∞–Ω–∫ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º
        candidates = normalized_chunk.get("candidates", [])
        if not candidates or not isinstance(candidates, list):
            return None

        candidate = candidates[0]
        if isinstance(candidate, dict) and candidate.get("finishReason"):
            return self.extract_metadata_from_response(normalized_chunk)

        return None

    def _handle_stream(self, response: requests.Response) -> Generator[Dict[str, Any], None, None]:
        """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞."""
        log.info("üîç GEMINI: –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞")

        buffer = ""
        chunk_counter = 0
        start_time = time.time()

        try:
            for line in response.iter_lines(decode_unicode=True):
                if not line:
                    continue

                line = line.strip()

                # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –≤ debug —Ä–µ–∂–∏–º–µ
                if log.isEnabledFor(logging.DEBUG):
                    log.debug("üìÑ RAW LINE: %s", line[:200] + "..." if len(line) > 200 else line)

                if line.startswith('data: '):
                    data = line[6:].strip()
                else:
                    data = line

                if data in ["[DONE]", ""]:
                    log.info("üèÅ GEMINI: –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –ø–æ—Ç–æ–∫–∞")
                    break

                buffer += data

                try:
                    chunk = json.loads(buffer)
                    chunk_counter += 1

                    # –£–º–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —É—Ä–æ–≤–Ω—è
                    if log.isEnabledFor(logging.DEBUG):
                        _log_chunk_smart(chunk, chunk_counter, "DEBUG")
                    elif log.isEnabledFor(logging.INFO):
                        _log_chunk_smart(chunk, chunk_counter, "INFO")

                    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ –æ—Ç–ø—Ä–∞–≤–∫–∞
                    normalized_chunk = self._normalize_chunk_for_adapter(chunk)
                    yield normalized_chunk
                    buffer = ""

                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                    if self._is_final_chunk(chunk):
                        log.info("üèÅ GEMINI: –û–±–Ω–∞—Ä—É–∂–µ–Ω —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —á–∞–Ω–∫")
                        break

                except json.JSONDecodeError as e:
                    if log.isEnabledFor(logging.DEBUG):
                        log.debug("‚ö†Ô∏è JSON ERROR: %s, –±—É—Ñ–µ—Ä: %s...", str(e), buffer[:100])
                    if len(buffer) > 50000:
                        log.warning("üí• BUFFER OVERFLOW: –û—á–∏—Å—Ç–∫–∞ –±—É—Ñ–µ—Ä–∞")
                        buffer = ""
                    continue

        except Exception as e:
            log.error("üí• STREAM ERROR: %s", e, exc_info=True)
            raise

        duration = time.time() - start_time
        log.info("‚úÖ GEMINI: –ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ—Ç–æ–∫–∞ (%d —á–∞–Ω–∫–æ–≤ –∑–∞ %.2f —Å–µ–∫)",
                 chunk_counter, duration)

    def _normalize_chunk_for_adapter(self, chunk: Union[Dict[str, Any], List[Any]]) -> Dict[str, Any]:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á–∞–Ω–∫–∞ —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ø–∏—Å–æ–∫
        if isinstance(chunk, list):
            all_choices = []

            for item_index, item in enumerate(chunk):
                if isinstance(item, dict) and "candidates" in item:
                    candidates = item["candidates"]

                    for cand_index, candidate in enumerate(candidates):
                        content_text = ""
                        if "content" in candidate:
                            parts = candidate["content"].get("parts", [])
                            text_parts = [
                                str(part["text"]) for part in parts
                                if isinstance(part, dict) and "text" in part and part["text"]
                            ]
                            content_text = "".join(text_parts)

                        choice = {
                            "index": len(all_choices),
                            "delta": {"content": content_text}
                        }

                        if "finishReason" in candidate:
                            choice["finish_reason"] = candidate["finishReason"]

                        all_choices.append(choice)

            normalized = {
                "choices": all_choices,
                "id": f"gemini-{int(time.time())}",
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": "gemini-1.5-flash"
            }

            # –õ–æ–≥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
            if all_choices and log.isEnabledFor(logging.DEBUG):
                first_content = all_choices[0]["delta"].get("content", "")
                if first_content:
                    log.debug("‚úÖ NORMALIZED: '%s'", first_content[:100])

            return normalized

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ª–æ–≤–∞—Ä—å
        elif isinstance(chunk, dict):
            if "candidates" in chunk:
                candidates = chunk["candidates"]
                choices = []

                for i, candidate in enumerate(candidates):
                    content_text = ""
                    if "content" in candidate:
                        parts = candidate["content"].get("parts", [])
                        text_parts = [
                            str(part["text"]) for part in parts
                            if isinstance(part, dict) and "text" in part and part["text"]
                        ]
                        content_text = "".join(text_parts)

                    choice = {
                        "index": i,
                        "delta": {"content": content_text}
                    }

                    if "finishReason" in candidate:
                        choice["finish_reason"] = candidate["finishReason"]

                    choices.append(choice)

                normalized = {
                    "choices": choices,
                    "id": chunk.get("id", f"gemini-{int(time.time())}"),
                    "object": "chat.completion.chunk",
                    "created": chunk.get("created", int(time.time())),
                    "model": "gemini-1.5-flash"
                }

                # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
                if "usageMetadata" in chunk:
                    usage = chunk["usageMetadata"]
                    normalized["usage"] = {
                        "prompt_tokens": usage.get("promptTokenCount", 0),
                        "completion_tokens": usage.get("totalTokenCount", 0) - usage.get("promptTokenCount", 0),
                        "total_tokens": usage.get("totalTokenCount", 0)
                    }

                for key in ["modelVersion", "responseId"]:
                    if key in chunk:
                        normalized[key] = chunk[key]

                return normalized
            else:
                return chunk

        # –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø
        else:
            log.warning("‚ö†Ô∏è GEMINI: –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø —á–∞–Ω–∫–∞: %s", type(chunk))
            return {
                "choices": [],
                "id": f"gemini-error-{int(time.time())}",
                "object": "chat.completion.chunk",
                "created": int(time.time()),
                "model": "gemini-1.5-flash"
            }

    def _is_final_chunk(self, chunk: Union[Dict[str, Any], List[Any]]) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —á–∞–Ω–∫ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º."""

        if isinstance(chunk, dict):
            candidates = chunk.get("candidates", [])
            if candidates and isinstance(candidates, list):
                return any(candidate.get("finishReason") for candidate in candidates if isinstance(candidate, dict))

        elif isinstance(chunk, list):
            return any(item.get("finishReason") for item in chunk if isinstance(item, dict))

        return False

    def extract_delta_from_chunk(self, chunk: Union[Dict[str, Any], List[Any]]) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–µ–ª—å—Ç—ã –±–µ–∑ –∏–∑–±—ã—Ç–æ—á–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""

        if isinstance(chunk, dict):
            # OpenAI —Ñ–æ—Ä–º–∞—Ç
            choices = chunk.get("choices", [])
            if choices:
                delta = choices[0].get("delta", {})
                return delta.get("content", "")

            # Gemini —Ñ–æ—Ä–º–∞—Ç
            candidates = chunk.get("candidates", [])
            if candidates:
                return self.extract_content_from_choice(candidates[0])

        elif isinstance(chunk, list) and chunk:
            return self.extract_content_from_choice(chunk[0])

        return ""

    def extract_content_from_choice(self, choice: Dict[str, Any]) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –±–µ–∑ –∏–∑–±—ã—Ç–æ—á–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è."""
        try:
            content = choice.get("content", {})
            if not isinstance(content, dict):
                return ""

            parts = content.get("parts", [])
            if not isinstance(parts, list):
                return ""

            text_parts = [
                str(part["text"]) for part in parts
                if isinstance(part, dict) and "text" in part and part["text"]
            ]

            return "".join(text_parts)

        except Exception as e:
            log.debug("–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–Ω—Ç–∞: %s", e)
            return ""

