import logging
import math
import re
import time
from pathlib import Path
from typing import Tuple
import pandas as pd

log = logging.getLogger(__name__)


def wilson_score_interval(
        successes: int,
        total: int,
        confidence: float = 0.95
) -> Tuple[float, float]:
    if total == 0:
        return 0.0, 1.0
    z = 1.959963984540054
    p_hat = float(successes) / total
    part1 = p_hat + (z * z) / (2 * total)
    part2 = z * math.sqrt((p_hat * (1 - p_hat)) / total + (z * z) / (4 * total * total))
    denominator = 1 + (z * z) / total
    lower_bound = (part1 - part2) / denominator
    upper_bound = (part1 + part2) / denominator
    return lower_bound, upper_bound


class Reporter:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—ã—Ä—ã–µ JSON-—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∏—Ö —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π, —Å–∞–º–æ–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–µ–º—ã–π –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown.
    """

    def __init__(self, results_dir: Path):
        self.results_dir = results_dir
        self.history_path = self.results_dir.parent / "history.json"
        self.all_results: pd.DataFrame = self._load_all_results()

        # >>>>> –ù–û–í–û–ï: –û—Ç–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç–∞ <<<<<
        self.context_stress_results = pd.DataFrame()
        if 'category' in self.all_results.columns and 't_context_stress' in self.all_results['category'].unique():
            self.context_stress_results = self.all_results[self.all_results['category'] == 't_context_stress'].copy()
            log.info(f"–ù–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {len(self.context_stress_results)} –∑–∞–ø–∏—Å–µ–π.")

    def _load_all_results(self) -> pd.DataFrame:
        all_data = []
        json_files = sorted(list(self.results_dir.glob("*.json")))
        log.info("–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ—Ç—á–µ—Ç–∞: %d", len(json_files))
        for json_file in json_files:
            try:
                data = pd.read_json(json_file)
                if not data.empty:
                    all_data.append(data)
            except Exception as e:
                log.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ %s: %s", json_file, e)
        if not all_data:
            log.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞.")
            return pd.DataFrame()
        combined_data = pd.concat(all_data, ignore_index=True)
        log.info("–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: %d", len(combined_data))
        return combined_data

    def _load_history(self) -> pd.DataFrame:
        if not self.history_path.exists():
            log.info("–§–∞–π–ª –∏—Å—Ç–æ—Ä–∏–∏ '%s' –Ω–µ –Ω–∞–π–¥–µ–Ω. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—å—Å—è –Ω–µ –±—É–¥–µ—Ç.", self.history_path.name)
            return pd.DataFrame()
        try:
            history_df = pd.read_json(self.history_path, orient='index')
            log.info("‚úÖ –§–∞–π–ª –∏—Å—Ç–æ—Ä–∏–∏ '%s' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.", self.history_path.name)
            return history_df
        except Exception as e:
            log.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª –∏—Å—Ç–æ—Ä–∏–∏ %s: %s", self.history_path, e)
            return pd.DataFrame()

    def _save_history(self, metrics: pd.DataFrame):
        history_data = metrics[['Trust_Score', 'Accuracy', 'Total_Runs']].copy()
        try:
            history_data.to_json(self.history_path, orient='index', indent=4)
            log.info("‚úÖ –§–∞–π–ª –∏—Å—Ç–æ—Ä–∏–∏ '%s' –æ–±–Ω–æ–≤–ª–µ–Ω –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.", self.history_path.name)
        except Exception as e:
            log.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∞–π–ª –∏—Å—Ç–æ—Ä–∏–∏ %s: %s", self.history_path, e)

    def _to_markdown_table(self, df: pd.DataFrame) -> str:
        if df.empty:
            return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.\n"
        try:
            return df.fillna("N/A").to_markdown() + "\n"
        except ImportError:
            log.error("–î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Markdown-—Ç–∞–±–ª–∏—Ü —Ç—Ä–µ–±—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ 'tabulate'. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–µ: pip install tabulate")
            return "–û—à–∏–±–∫–∞: –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ 'tabulate' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.\n"

    def _calculate_verbosity(self, df: pd.DataFrame) -> pd.Series:
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π "–ò–Ω–¥–µ–∫—Å –ë–æ–ª—Ç–ª–∏–≤–æ—Å—Ç–∏".
        –ú–µ—Ç—Ä–∏–∫–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫—É—é –¥–æ–ª—é –æ—Ç –æ–±—â–µ–≥–æ –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–∏ –∑–∞–Ω–∏–º–∞—é—Ç
        —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è ("–º—ã—Å–ª–∏"), –∞ –Ω–µ –ø—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç.
        –†–∞–±–æ—Ç–∞–µ—Ç, –µ—Å–ª–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –µ—Å—Ç—å –ø–æ–ª–µ 'thinking_response'.
        """
        # –†–∞–±–æ—Ç–∞–µ–º —Å –∫–æ–ø–∏–µ–π, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å SettingWithCopyWarning
        df_copy = df.copy()

        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç, –∏–Ω–∞—á–µ —Å–æ–∑–¥–∞–¥–∏–º –ø—É—Å—Ç—ã–µ
        if 'thinking_response' not in df_copy.columns:
            df_copy['thinking_response'] = ""
        if 'llm_response' not in df_copy.columns:
            df_copy['llm_response'] = ""

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –ø—É—Å—Ç—ã–º–∏ —Å—Ç—Ä–æ–∫–∞–º–∏ –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ .str.len()
        df_copy.loc[:, 'thinking_response'] = df_copy['thinking_response'].fillna("")
        df_copy.loc[:, 'llm_response'] = df_copy['llm_response'].fillna("")

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–ª–∏–Ω—ã
        df_copy['thinking_len'] = df_copy['thinking_response'].str.len()
        df_copy['answer_len'] = df_copy['llm_response'].str.len()
        df_copy['total_len'] = df_copy['thinking_len'] + df_copy['answer_len']

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –∏ —Å—É–º–º–∏—Ä—É–µ–º –¥–ª–∏–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        grouped = df_copy.groupby('model_name')
        sums = grouped[['thinking_len', 'total_len']].sum()

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å –±–æ–ª—Ç–ª–∏–≤–æ—Å—Ç–∏
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º .loc –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–≥–æ –¥–µ–ª–µ–Ω–∏—è, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å
        verbosity_index = pd.Series(0.0, index=sums.index, dtype=float)
        non_zero_total = sums['total_len'] > 0
        verbosity_index.loc[non_zero_total] = sums.loc[non_zero_total, 'thinking_len'] / sums.loc[non_zero_total, 'total_len']

        return verbosity_index.fillna(0).rename("Verbosity_Index")

    def _calculate_comprehensiveness(self, df: pd.DataFrame) -> pd.Series:
        if 'category' not in df.columns or df['category'].nunique() == 0:
            return pd.Series(0.0, index=df['model_name'].unique(), name="Comprehensiveness")

        model_categories_count = df.groupby('model_name')['category'].nunique()
        total_unique_categories = df['category'].nunique()

        comprehensiveness_index = model_categories_count / total_unique_categories

        return comprehensiveness_index.rename("Comprehensiveness")

    # >>>>> –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É <<<<<
    def _generate_context_performance_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç Markdown-–æ—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö."""
        if self.context_stress_results.empty:
            return "" # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç—Ç–æ—Ç –±–ª–æ–∫

        df = self.context_stress_results

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞—Ä–µ–π
        if 'performance_metrics' in df.columns:
            perf_metrics = df['performance_metrics'].apply(pd.Series)
            df = pd.concat([df.drop(['performance_metrics'], axis=1), perf_metrics], axis=1)

        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        pivot = df.pivot_table(
            index=['model_name', 'context_k'],
            values=['is_correct', 'execution_time_ms', 'peak_ram_usage_mb'],
            aggfunc={
                'is_correct': 'mean', # –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ –≤—Å–µ–º –≥–ª—É–±–∏–Ω–∞–º
                'execution_time_ms': 'mean',
                'peak_ram_usage_mb': 'mean'
            }
        ).sort_index()

        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≤—ã–≤–æ–¥–∞
        pivot.rename(columns={
            'is_correct': 'Accuracy',
            'execution_time_ms': 'Avg Time (ms)',
            'peak_ram_usage_mb': 'Avg RAM (MB)'
        }, inplace=True)

        pivot['Accuracy'] = pivot['Accuracy'].map(lambda x: f"{x:.0%}")
        pivot['Avg Time (ms)'] = pivot['Avg Time (ms)'].map(lambda x: f"{x:,.0f}")
        pivot['Avg RAM (MB)'] = pivot['Avg RAM (MB)'].map(lambda x: f"{x:,.1f}")

        report_md = "## üß† –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n\n"
        report_md += "> _–≠—Ç–∞ —Ç–∞–±–ª–∏—Ü–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –º–æ–¥–µ–ª–∏ —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ò–¥–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç 100% —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º —Ä–æ—Å—Ç–µ –≤—Ä–µ–º–µ–Ω–∏ –∏ RAM._\n\n"
        report_md += self._to_markdown_table(pivot)
        return report_md

    # >>>>> –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ "–ü–æ—Ç–µ—Ä—è–Ω–Ω–æ–π —Å–µ—Ä–µ–¥–∏–Ω—ã" <<<<<
    def _generate_heatmap_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç "—Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É" –≤ –≤–∏–¥–µ Markdown —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–±–ª–µ–º—ã '–ø–æ—Ç–µ—Ä—è–Ω–Ω–æ–π —Å–µ—Ä–µ–¥–∏–Ω—ã'."""
        if self.context_stress_results.empty:
            return ""

        df = self.context_stress_results

        # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É: –º–æ–¥–µ–ª–∏/–≥–ª—É–±–∏–Ω–∞ vs —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        heatmap = df.pivot_table(
            index=['model_name', 'depth_percent'],
            columns='context_k',
            values='is_correct',
            aggfunc='mean' # –ë–µ—Ä–µ–º —Å—Ä–µ–¥–Ω–µ–µ, –µ—Å–ª–∏ –±—ã–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã
        )

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ (–µ—Å–ª–∏ –∫–∞–∫–æ–π-—Ç–æ —Ç–µ—Å—Ç –Ω–µ –ø—Ä–æ—à–µ–ª)
        heatmap.fillna(-1, inplace=True)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–∏–º–≤–æ–ª—ã –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        def to_emoji(score):
            if score == 1.0: return "‚úÖ"
            if score == 0.0: return "‚ùå"
            if score == -1: return " N/A " # –¢–µ—Å—Ç –Ω–µ –±—ã–ª –≤—ã–ø–æ–ª–Ω–µ–Ω
            return "‚ö†Ô∏è" # –ß–∞—Å—Ç–∏—á–Ω—ã–π —É—Å–ø–µ—Ö

        heatmap_emoji = heatmap.applymap(to_emoji)
        heatmap_emoji.columns = [f"{col}k" for col in heatmap_emoji.columns]

        report_md = "## üî• –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –≤–Ω–∏–º–∞–Ω–∏—è (Needle in a Haystack)\n\n"
        report_md += "> _–≠—Ç–∞ —Ç–∞–±–ª–∏—Ü–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞ –∫–∞–∫–æ–π –≥–ª—É–±–∏–Ω–µ –∏ –ø—Ä–∏ –∫–∞–∫–æ–º —Ä–∞–∑–º–µ—Ä–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª—å '—Ç–µ—Ä—è–µ—Ç' —Ñ–∞–∫—Ç. ‚úÖ = –ù–∞—à–ª–∞, ‚ùå = –ù–µ –Ω–∞—à–ª–∞, N/A = –¢–µ—Å—Ç –Ω–µ –∑–∞–ø—É—Å–∫–∞–ª—Å—è._\n\n"
        report_md += self._to_markdown_table(heatmap_emoji)
        return report_md

    # >>>>> –í—ã–Ω–æ—Å–∏–º –ª–æ–≥–∏–∫—É —Ä–∞—Å—á–µ—Ç–∞ –ª–∏–¥–µ—Ä–±–æ—Ä–¥–∞ <<<<<
    def _calculate_leaderboard(self, df: pd.DataFrame) -> pd.DataFrame:
        # --- –≠—Ç–∞–ø 1: –ê–≥—Ä–µ–≥–∞—Ü–∏—è –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ ---
        metrics = df.groupby('model_name').agg(
            Successes=('is_correct', 'sum'),
            Total_Runs=('is_correct', 'count'),
            Avg_Time_ms=('execution_time_ms', 'mean')
        )
        verbosity = self._calculate_verbosity(df)
        comprehensiveness = self._calculate_comprehensiveness(df)

        # >>>>> –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º .join() –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –ø–æ –∏–Ω–¥–µ–∫—Å—É <<<<<
        metrics = metrics.join(verbosity).join(comprehensiveness)

        # --- –≠—Ç–∞–ø 2: –†–∞—Å—á–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π ---
        metrics['Accuracy'] = (metrics['Successes'] / metrics['Total_Runs']).fillna(0)
        metrics['Trust_Score'] = metrics.apply(
            lambda row: wilson_score_interval(int(row['Successes']), int(row['Total_Runs']))[0],
            axis=1
        )

        # --- –≠—Ç–∞–ø 3: –†–∞–±–æ—Ç–∞ —Å –∏—Å—Ç–æ—Ä–∏–µ–π ---
        history_df = self._load_history()
        metrics['Accuracy_Change'] = 0.0
        if not history_df.empty:
            metrics = metrics.join(history_df.add_suffix('_prev'))
            metrics['Accuracy_Change'] = (metrics['Accuracy'] - metrics['Accuracy_prev']).fillna(0)

        # --- –≠—Ç–∞–ø 4: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã ---
        def format_with_indicator(value, change, format_str):
            indicator, change_str = "", ""
            threshold = 0.001
            if change > threshold: indicator, change_str = " ‚ñ≤", f" (+{change:.1%})"
            elif change < -threshold: indicator, change_str = " ‚ñº", f" ({change:.1%})"
            elif 'Accuracy_prev' in metrics.columns: indicator = " ‚ñ¨"
            return f"{value:{format_str}}{indicator}{change_str}"

        metrics.sort_values(by='Trust_Score', ascending=False, inplace=True)
        metrics.insert(0, '–†–∞–Ω–≥', range(1, len(metrics) + 1))

        leaderboard_df = pd.DataFrame()
        leaderboard_df['–†–∞–Ω–≥'] = metrics['–†–∞–Ω–≥']
        leaderboard_df['–ú–æ–¥–µ–ª—å'] = metrics.index
        leaderboard_df['Trust Score'] = metrics['Trust_Score'].map(lambda x: f"{x:.3f}")
        leaderboard_df['Accuracy'] = metrics.apply(lambda row: format_with_indicator(row['Accuracy'], row['Accuracy_Change'], '.1%'), axis=1)
        leaderboard_df['Coverage'] = metrics['Comprehensiveness'].map(lambda x: f"{x:.0%}")
        leaderboard_df['Verbosity'] = metrics['Verbosity_Index'].map(lambda x: f"{x:.1%}")
        leaderboard_df['Avg Time'] = metrics['Avg_Time_ms'].map(lambda x: f"{x:,.0f} –º—Å")
        leaderboard_df['Runs'] = metrics['Total_Runs']
        leaderboard_df.set_index('–†–∞–Ω–≥', inplace=True)

        self._save_history(metrics)
        return leaderboard_df

    def generate_leaderboard_report(self) -> str:
        if self.all_results.empty:
            return "# üèÜ –¢–∞–±–ª–∏—Ü–∞ –õ–∏–¥–µ—Ä–æ–≤\n\n–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."

        # --- –≠—Ç–∞–ø 5: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Markdown –û—Ç—á–µ—Ç–∞ ---
        timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
        report_md = f"# üèÜ –¢–∞–±–ª–∏—Ü–∞ –õ–∏–¥–µ—Ä–æ–≤ –ë–µ–Ω—á–º–∞—Ä–∫–∞\n\n"
        report_md += f"*–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {timestamp}*\n\n"

        # >>>>> –ò–ó–ú–ï–ù–ï–ù–ò–ï: –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã <<<<<
        main_results = self.all_results[self.all_results['category'] != 't_context_stress']
        if not main_results.empty:
            # ... (–≤—Å—è –≤–∞—à–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è leaderboard_df, –Ω–æ –Ω–∞ main_results)
            # –¢—É—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –≤—ã—á–∏—Å–ª—è–µ—Ç leaderboard_df
            leaderboard_df = self._calculate_leaderboard(main_results) # –ü—Ä–∏–º–µ—Ä –≤—ã–∑–æ–≤–∞
            report_md += self._to_markdown_table(leaderboard_df)
        else:
            report_md += "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã –ª–∏–¥–µ—Ä–æ–≤.\n"

        report_md += "\n---\n"

        # >>>>> –ù–û–í–´–ô –ë–õ–û–ö: –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—á–µ—Ç—ã –ø–æ —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç—É <<<<<
        context_perf_report = self._generate_context_performance_report()
        if context_perf_report:
            report_md += context_perf_report
            report_md += "\n---\n"

        heatmap_report = self._generate_heatmap_report()
        if heatmap_report:
            report_md += heatmap_report
            report_md += "\n---\n"

        # ... (—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –¥–ª—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ –∏ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏) ...
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –¥–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∂–µ –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç –Ω–∞—à —Å–ø–µ—Ü. —Ç–µ—Å—Ç
        report_md += "## üìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n\n"
        if not main_results.empty:
            test_stats = main_results.groupby(['model_name', 'category'])['is_correct'].agg(['sum', 'count'])
            test_stats['Accuracy'] = (test_stats['sum'] / test_stats['count'])
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏: —Å–Ω–∞—á–∞–ª–∞ –ø–æ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏, –ø–æ—Ç–æ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Ç–æ—á–Ω–æ—Å—Ç–∏
            test_stats.sort_values(by=['model_name', 'Accuracy'], ascending=[True, False], inplace=True)
            test_stats.rename(columns={'sum': '–£—Å–ø–µ—à–Ω–æ', 'count': '–ü–æ–ø—ã—Ç–æ–∫'}, inplace=True)
            test_stats['Accuracy'] = test_stats['Accuracy'].map(lambda x: f"{x:.0%}")
            report_md += self._to_markdown_table(test_stats[['–ü–æ–ø—ã—Ç–æ–∫', '–£—Å–ø–µ—à–Ω–æ', 'Accuracy']])
            report_md += "\n> _–≠—Ç–∞ —Ç–∞–±–ª–∏—Ü–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑—Ä–µ–∑–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π._"
        else:
            report_md += "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏.\n"

        return report_md