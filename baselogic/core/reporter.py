import logging
import math
import re
import time
from pathlib import Path
from typing import Tuple, Dict, Any
import pandas as pd
import pandas as pd

from baselogic.core.system_checker import get_hardware_tier

log = logging.getLogger(__name__)


def wilson_score_interval(
        successes: int,
        total: int,
        confidence: float = 0.95
) -> Tuple[float, float]:
    if total == 0:
        return 0.0, 1.0
    z = 1.959963984540054
    p_hat = float(successes) / total
    part1 = p_hat + (z * z) / (2 * total)
    part2 = z * math.sqrt((p_hat * (1 - p_hat)) / total + (z * z) / (4 * total * total))
    denominator = 1 + (z * z) / total
    lower_bound = (part1 - part2) / denominator
    upper_bound = (part1 + part2) / denominator
    return lower_bound, upper_bound


class Reporter:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—ã—Ä—ã–µ JSON-—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∏—Ö —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π, —Å–∞–º–æ–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–µ–º—ã–π –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown.
    """

    def __init__(self, results_dir: Path):
        self.results_dir = results_dir
        self.history_path = self.results_dir.parent / "history.json"
        self.all_results: pd.DataFrame = self._load_all_results()

        # >>>>> –ù–û–í–û–ï: –û—Ç–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç–∞ <<<<<
        self.context_stress_results = pd.DataFrame()
        if 'category' in self.all_results.columns and 't_context_stress' in self.all_results['category'].unique():
            self.context_stress_results = self.all_results[self.all_results['category'] == 't_context_stress'].copy()
            log.info(f"–ù–∞–π–¥–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {len(self.context_stress_results)} –∑–∞–ø–∏—Å–µ–π.")

    def _load_all_results(self) -> pd.DataFrame:
        all_data = []
        json_files = sorted(list(self.results_dir.glob("*.json")))
        log.info("–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ—Ç—á–µ—Ç–∞: %d", len(json_files))
        for json_file in json_files:
            try:
                data = pd.read_json(json_file)
                if not data.empty:
                    all_data.append(data)
            except Exception as e:
                log.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ %s: %s", json_file, e)
        if not all_data:
            log.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞.")
            return pd.DataFrame()
        combined_data = pd.concat(all_data, ignore_index=True)
        log.info("–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: %d", len(combined_data))
        return combined_data

    def _to_markdown_table(self, df: pd.DataFrame) -> str:
        if df.empty:
            return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.\n"
        try:
            return df.fillna("N/A").to_markdown() + "\n"
        except ImportError:
            log.error("–î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Markdown-—Ç–∞–±–ª–∏—Ü —Ç—Ä–µ–±—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ 'tabulate'. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–µ: pip install tabulate")
            return "–û—à–∏–±–∫–∞: –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ 'tabulate' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.\n"

    def _calculate_verbosity(self, df: pd.DataFrame) -> pd.Series:
        """
        –ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ä–∞—Å—á–µ—Ç –ò–Ω–¥–µ–∫—Å–∞ –ë–æ–ª—Ç–ª–∏–≤–æ—Å—Ç–∏ –¥–ª—è thinking-–º–æ–¥–µ–ª–µ–π.

        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ä–∞—Å—Å—É–∂–¥–µ–Ω–∏—è –∏–∑:
        1. –û—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø–æ–ª—è 'thinking_response'
        2. <think>...</think> –±–ª–æ–∫–æ–≤ –≤–Ω—É—Ç—Ä–∏ 'llm_response'

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ–ª—é thinking —Ç–µ–∫—Å—Ç–∞ –æ—Ç –æ–±—â–µ–≥–æ –æ–±—ä–µ–º–∞ –≤—ã–≤–æ–¥–∞ –º–æ–¥–µ–ª–∏.
        """
        if df.empty:
            return pd.Series(dtype=float, name='Verbosity_Index')

        df_work = df.copy()

        # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        required_columns = ['llm_response', 'thinking_response']
        for col in required_columns:
            if col not in df_work.columns:
                df_work[col] = ""

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –ø—É—Å—Ç—ã–º–∏ —Å—Ç—Ä–æ–∫–∞–º–∏
        df_work['llm_response'] = df_work['llm_response'].fillna("")
        df_work['thinking_response'] = df_work['thinking_response'].fillna("")

        def extract_thinking_and_answer_lengths(row) -> tuple:
            """
            –ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–ª–∏–Ω—ã thinking –∏ —á–∏—Å—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –∏–∑ —Å—Ç—Ä–æ–∫–∏ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞.
            –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (thinking_length, clean_answer_length).
            """
            llm_resp = str(row['llm_response'])
            thinking_resp = str(row['thinking_response'])

            # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Å—å thinking —Ç–µ–∫—Å—Ç
            total_thinking = thinking_resp

            # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ <think>...</think> –±–ª–æ–∫–∏ –≤ llm_response
            think_blocks = re.findall(r'<think>(.*?)</think>', llm_resp, re.DOTALL | re.IGNORECASE)
            for block in think_blocks:
                total_thinking += block

            # –£–¥–∞–ª—è–µ–º thinking –±–ª–æ–∫–∏ –∏–∑ llm_response –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —á–∏—Å—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
            clean_answer = re.sub(r'<think>.*?</think>', '', llm_resp, flags=re.DOTALL | re.IGNORECASE)
            clean_answer = clean_answer.strip()

            return len(total_thinking), len(clean_answer)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∫ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–µ
        lengths = df_work.apply(extract_thinking_and_answer_lengths, axis=1, result_type='expand')
        lengths.columns = ['thinking_len', 'answer_len']

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–ª–∏–Ω–∞–º–∏
        df_work['thinking_len'] = lengths['thinking_len']
        df_work['answer_len'] = lengths['answer_len']
        df_work['total_len'] = df_work['thinking_len'] + df_work['answer_len']

        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –º–æ–¥–µ–ª—è–º –∏ —Å—É–º–º–∏—Ä—É–µ–º –¥–ª–∏–Ω—ã
        model_totals = df_work.groupby('model_name')[['thinking_len', 'total_len']].sum()

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å –±–æ–ª—Ç–ª–∏–≤–æ—Å—Ç–∏ (–∑–∞—â–∏—â–∞–µ–º—Å—è –æ—Ç –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å)
        verbosity_index = pd.Series(0.0, index=model_totals.index, name='Verbosity_Index')

        non_empty_models = model_totals['total_len'] > 0
        if non_empty_models.any():
            verbosity_index.loc[non_empty_models] = (
                    model_totals.loc[non_empty_models, 'thinking_len'] /
                    model_totals.loc[non_empty_models, 'total_len']
            )

        return verbosity_index

    def generate_hardware_compatibility_matrix(self):
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –º–∞—Ç—Ä–∏—Ü—É —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∏ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è.
        """

        compatibility_matrix = {
            'enterprise': {  # H100, A100, 80GB+ VRAM
                'recommended': [
                    'llama-3.1-70b', 'qwen2.5-72b', 'deepseek-v3',
                    'mixtral-8x22b', 'claude-3-opus-20240229'
                ],
                'supported': 'all_models',
                'performance': 'optimal'
            },

            'high_end': {  # RTX 4090, 4080, 16-24GB VRAM
                'recommended': [
                    'llama-3.1-8b', 'qwen2.5-14b', 'mistral-7b-v0.3',
                    'deepseek-coder-v2-16b', 'gemma-2-9b'
                ],
                'supported_with_quantization': [
                    'llama-3.1-70b-q4', 'qwen2.5-72b-q4'
                ],
                'performance': 'high'
            },

            'mid_range': {  # RTX 4070, 3080, 8-16GB VRAM
                'recommended': [
                    'llama-3.1-8b-q4', 'qwen2.5-7b', 'mistral-7b-v0.3-q4',
                    'phi-3.5-mini-3.8b', 'gemma-2-9b-q4'
                ],
                'performance': 'good'
            },

            'entry_level': {  # RTX 4060, 3060, 6-12GB VRAM
                'recommended': [
                    'phi-3.5-mini-3.8b', 'qwen2.5-7b-q4', 'llama-3.2-3b',
                    'tinyllama-1.1b', 'mobilellm-1.5b'
                ],
                'performance': 'acceptable'
            },

            'workstation_cpu': {  # High-end CPU, 64-128GB RAM
                'recommended': [
                    'llama-3.1-8b-q4', 'qwen2.5-7b-q4', 'phi-3.5-mini',
                    'mistral-7b-q4'
                ],
                'note': 'CPU-only inference, slower but possible',
                'performance': 'slow'
            },

            'mobile_cpu': {  # Laptops, <32GB RAM
                'recommended': [
                    'phi-3.5-mini', 'tinyllama-1.1b', 'qwen2.5-1.5b',
                    'mobilellm-1.5b'
                ],
                'performance': 'limited'
            }
        }

        return compatibility_matrix

    # def recommend_model_for_hardware(system_info: Dict[str, Any],
    #                                  use_case: str = 'general') -> Dict[str, Any]:
    #     """
    #     –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏—è.
    #     """
    #     hardware_tier = get_hardware_tier(system_info)
    #     matrix = generate_hardware_compatibility_matrix()
    #
    #     recommendations = matrix.get(hardware_tier, {})
    #
    #     # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ use case
    #     use_case_adjustments = {
    #         'coding': {
    #             'models': ['deepseek-coder-v2', 'codellama', 'phi-3.5-mini'],
    #             'priority': 'code_generation_accuracy'
    #         },
    #         'multilingual': {
    #             'models': ['qwen2.5', 'llama-3.1'],
    #             'priority': 'language_support'
    #         },
    #         'creative': {
    #             'models': ['claude-3', 'llama-3.1', 'mistral'],
    #             'priority': 'creativity_coherence'
    #         }
    #     }
    #
    #     return {
    #         'hardware_tier': hardware_tier,
    #         'recommended_models': recommendations.get('recommended', []),
    #         'performance_expectation': recommendations.get('performance', 'unknown'),
    #         'use_case_optimized': use_case_adjustments.get(use_case, {}),
    #         'system_specs': {
    #             'total_vram_gb': sum(gpu.get('memory_total_gb', 0) for gpu in system_info.get('gpus', [])),
    #             'total_ram_gb': system_info.get('memory', {}).get('total_ram_gb', 0),
    #             'cpu_cores': system_info.get('cpu', {}).get('logical_cores', 0)
    #         }
    #     }

    # >>>>> –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É <<<<<
    def _generate_context_performance_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç Markdown-–æ—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –Ω–∞ –¥–ª–∏–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞—Ö."""
        if self.context_stress_results.empty:
            return "" # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –Ω–µ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç—Ç–æ—Ç –±–ª–æ–∫

        df = self.context_stress_results

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ –≤–ª–æ–∂–µ–Ω–Ω—ã—Ö —Å–ª–æ–≤–∞—Ä–µ–π
        if 'performance_metrics' in df.columns:
            perf_metrics = df['performance_metrics'].apply(pd.Series)
            df = pd.concat([df.drop(['performance_metrics'], axis=1), perf_metrics], axis=1)

        # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
        pivot = df.pivot_table(
            index=['model_name', 'context_k'],
            values=['is_correct', 'execution_time_ms', 'peak_ram_usage_mb'],
            aggfunc={
                'is_correct': 'mean', # –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ –≤—Å–µ–º –≥–ª—É–±–∏–Ω–∞–º
                'execution_time_ms': 'mean',
                'peak_ram_usage_mb': 'mean'
            }
        ).sort_index()

        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –≤—ã–≤–æ–¥–∞
        pivot.rename(columns={
            'is_correct': 'Accuracy',
            'execution_time_ms': 'Avg Time (ms)',
            'peak_ram_usage_mb': 'Avg RAM (MB)'
        }, inplace=True)

        pivot['Accuracy'] = pivot['Accuracy'].map(lambda x: f"{x:.0%}")
        pivot['Avg Time (ms)'] = pivot['Avg Time (ms)'].map(lambda x: f"{x:,.0f}")
        pivot['Avg RAM (MB)'] = pivot['Avg RAM (MB)'].map(lambda x: f"{x:,.1f}")

        report_md = "## üß† –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª–∏–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n\n"
        report_md += "> _–≠—Ç–∞ —Ç–∞–±–ª–∏—Ü–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è —Ç–æ—á–Ω–æ—Å—Ç—å –∏ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤ –º–æ–¥–µ–ª–∏ —Å —É–≤–µ–ª–∏—á–µ–Ω–∏–µ–º —Ä–∞–∑–º–µ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ò–¥–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç 100% —Ç–æ—á–Ω–æ—Å—Ç—å –ø—Ä–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º —Ä–æ—Å—Ç–µ –≤—Ä–µ–º–µ–Ω–∏ –∏ RAM._\n\n"
        report_md += self._to_markdown_table(pivot)
        return report_md

    # >>>>> –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ "–ü–æ—Ç–µ—Ä—è–Ω–Ω–æ–π —Å–µ—Ä–µ–¥–∏–Ω—ã" <<<<<
    def _generate_heatmap_report(self) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç "—Ç–µ–ø–ª–æ–≤—É—é –∫–∞—Ä—Ç—É" –≤ –≤–∏–¥–µ Markdown —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–±–ª–µ–º—ã '–ø–æ—Ç–µ—Ä—è–Ω–Ω–æ–π —Å–µ—Ä–µ–¥–∏–Ω—ã'."""
        if self.context_stress_results.empty:
            return ""

        df = self.context_stress_results

        # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É: –º–æ–¥–µ–ª–∏/–≥–ª—É–±–∏–Ω–∞ vs —Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        heatmap = df.pivot_table(
            index=['model_name', 'depth_percent'],
            columns='context_k',
            values='is_correct',
            aggfunc='mean' # –ë–µ—Ä–µ–º —Å—Ä–µ–¥–Ω–µ–µ, –µ—Å–ª–∏ –±—ã–ª–∏ –¥—É–±–ª–∏–∫–∞—Ç—ã
        )

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ (–µ—Å–ª–∏ –∫–∞–∫–æ–π-—Ç–æ —Ç–µ—Å—Ç –Ω–µ –ø—Ä–æ—à–µ–ª)
        heatmap.fillna(-1, inplace=True)

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —Å–∏–º–≤–æ–ª—ã –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        def to_emoji(score):
            if score == 1.0: return "‚úÖ"
            if score == 0.0: return "‚ùå"
            if score == -1: return " N/A " # –¢–µ—Å—Ç –Ω–µ –±—ã–ª –≤—ã–ø–æ–ª–Ω–µ–Ω
            return "‚ö†Ô∏è" # –ß–∞—Å—Ç–∏—á–Ω—ã–π —É—Å–ø–µ—Ö

        heatmap_emoji = heatmap.applymap(to_emoji)
        heatmap_emoji.columns = [f"{col}k" for col in heatmap_emoji.columns]

        report_md = "## üî• –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –≤–Ω–∏–º–∞–Ω–∏—è (Needle in a Haystack)\n\n"
        report_md += "> _–≠—Ç–∞ —Ç–∞–±–ª–∏—Ü–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞ –∫–∞–∫–æ–π –≥–ª—É–±–∏–Ω–µ –∏ –ø—Ä–∏ –∫–∞–∫–æ–º —Ä–∞–∑–º–µ—Ä–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –º–æ–¥–µ–ª—å '—Ç–µ—Ä—è–µ—Ç' —Ñ–∞–∫—Ç. ‚úÖ = –ù–∞—à–ª–∞, ‚ùå = –ù–µ –Ω–∞—à–ª–∞, N/A = –¢–µ—Å—Ç –Ω–µ –∑–∞–ø—É—Å–∫–∞–ª—Å—è._\n\n"
        report_md += self._to_markdown_table(heatmap_emoji)
        return report_md

    def _calculate_leaderboard(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        –ü–û–õ–ù–ê–Ø –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≤–µ—Ä—Å–∏—è —Ä–∞—Å—á–µ—Ç–∞ –ª–∏–¥–µ—Ä–±–æ—Ä–¥–∞.
        –ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å Coverage, –¥–æ–±–∞–≤–ª—è–µ—Ç —Ä–∞–±–æ—Ç—É —Å –∏—Å—Ç–æ—Ä–∏–µ–π –∏ —É–ª—É—á—à–∞–µ—Ç —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ.
        """

        # --- –≠—Ç–∞–ø 1: –ê–≥—Ä–µ–≥–∞—Ü–∏—è –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ ---
        metrics = df.groupby('model_name').agg(
            Successes=('is_correct', 'sum'),
            Total_Runs=('is_correct', 'count'),
            Avg_Time_ms=('execution_time_ms', 'mean')
        )

        # --- –≠—Ç–∞–ø 2: –†–∞—Å—á–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫ ---
        verbosity = self._calculate_verbosity(df)
        comprehensiveness = self._calculate_comprehensiveness(df)  # –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é

        # –ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É
        metrics = metrics.join(verbosity, how='left').join(comprehensiveness, how='left')

        # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –Ω—É–ª—è–º–∏ –¥–ª—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –º–µ—Ç—Ä–∏–∫
        metrics['Verbosity_Index'] = metrics['Verbosity_Index'].fillna(0.0)
        metrics['Comprehensiveness'] = metrics['Comprehensiveness'].fillna(0.0)

        # --- –≠—Ç–∞–ø 3: –†–∞—Å—á–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π ---
        metrics['Accuracy'] = (metrics['Successes'] / metrics['Total_Runs']).fillna(0)
        metrics['Trust_Score'] = metrics.apply(
            lambda row: wilson_score_interval(int(row['Successes']), int(row['Total_Runs']))[0],
            axis=1
        )

        # --- –≠—Ç–∞–ø 4: –†–∞–±–æ—Ç–∞ —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏ ---
        history_df = self._load_history()
        metrics['Accuracy_Change'] = 0.0

        if not history_df.empty:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
            metrics = metrics.join(history_df.add_suffix('_prev'), how='left')

            # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ —Ç–æ—á–Ω–æ—Å—Ç–∏
            metrics['Accuracy_Change'] = (
                    metrics['Accuracy'] - metrics['Accuracy_prev'].fillna(metrics['Accuracy'])
            ).fillna(0)

        # --- –≠—Ç–∞–ø 5: –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ Trust Score ---
        metrics.sort_values(by='Trust_Score', ascending=False, inplace=True)
        metrics.reset_index(inplace=True)  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ä–∞–Ω–≥–∞
        metrics.insert(0, 'Rank', range(1, len(metrics) + 1))
        metrics.set_index('model_name', inplace=True)

        # --- –≠—Ç–∞–ø 6: –§—É–Ω–∫—Ü–∏—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–π ---
        def format_with_indicator(value, change, format_str):
            """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ —Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è (‚ñ≤‚ñº‚ñ¨)."""
            indicator, change_str = "", ""
            threshold = 0.001

            if change > threshold:
                indicator, change_str = " ‚ñ≤", f" (+{change:.1%})"
            elif change < -threshold:
                indicator, change_str = " ‚ñº", f" ({change:.1%})"
            elif not history_df.empty:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∏—Å—Ç–æ—Ä–∏—è
                indicator = " ‚ñ¨"

            return f"{value:{format_str}}{indicator}{change_str}".strip()

        # --- –≠—Ç–∞–ø 7: –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã –ª–∏–¥–µ—Ä–±–æ—Ä–¥–∞ ---
        leaderboard_df = pd.DataFrame()
        leaderboard_df['–†–∞–Ω–≥'] = metrics['Rank']
        leaderboard_df['–ú–æ–¥–µ–ª—å'] = metrics.index
        leaderboard_df['Trust Score'] = metrics['Trust_Score'].map(lambda x: f"{x:.3f}")
        leaderboard_df['Accuracy'] = metrics.apply(
            lambda row: format_with_indicator(row['Accuracy'], row['Accuracy_Change'], '.1%'),
            axis=1
        )
        leaderboard_df['Coverage'] = metrics['Comprehensiveness'].map(lambda x: f"{x:.0%}")
        leaderboard_df['Verbosity'] = metrics['Verbosity_Index'].map(lambda x: f"{x:.1%}")
        leaderboard_df['Avg Time'] = metrics['Avg_Time_ms'].map(lambda x: f"{x:,.0f} –º—Å")
        leaderboard_df['Runs'] = metrics['Total_Runs'].astype(int)

        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ä–∞–Ω–≥ –∫–∞–∫ –∏–Ω–¥–µ–∫—Å –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        leaderboard_df.set_index('–†–∞–Ω–≥', inplace=True)

        # --- –≠—Ç–∞–ø 8: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –∏—Å—Ç–æ—Ä–∏—é ---
        self._save_history(metrics[['Trust_Score', 'Accuracy', 'Total_Runs']])

        return leaderboard_df

    def _load_history(self) -> pd.DataFrame:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è."""
        if not self.history_path.exists():
            log.info("–§–∞–π–ª –∏—Å—Ç–æ—Ä–∏–∏ '%s' –Ω–µ –Ω–∞–π–¥–µ–Ω. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—å—Å—è –Ω–µ –±—É–¥–µ—Ç.", self.history_path.name)
            return pd.DataFrame()

        try:
            history_df = pd.read_json(self.history_path, orient='index')
            log.info("‚úÖ –§–∞–π–ª –∏—Å—Ç–æ—Ä–∏–∏ '%s' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.", self.history_path.name)
            return history_df
        except Exception as e:
            log.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª –∏—Å—Ç–æ—Ä–∏–∏ %s: %s", self.history_path, e)
            return pd.DataFrame()

    def _save_history(self, metrics: pd.DataFrame):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ç–µ–∫—É—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ —Ñ–∞–π–ª –∏—Å—Ç–æ—Ä–∏–∏."""
        history_data = metrics[['Trust_Score', 'Accuracy', 'Total_Runs']].copy()

        try:
            history_data.to_json(self.history_path, orient='index', indent=4)
            log.info("‚úÖ –§–∞–π–ª –∏—Å—Ç–æ—Ä–∏–∏ '%s' –æ–±–Ω–æ–≤–ª–µ–Ω –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.", self.history_path.name)
        except Exception as e:
            log.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∞–π–ª –∏—Å—Ç–æ—Ä–∏–∏ %s: %s", self.history_path, e)

    def _calculate_comprehensiveness(self, df: pd.DataFrame) -> pd.Series:
        """
        –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≤–µ—Ä—Å–∏—è —Ä–∞—Å—á–µ—Ç–∞ Coverage.
        –¢–µ–ø–µ—Ä—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —É—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞.
        """
        if 'category' not in df.columns or df['category'].nunique() == 0:
            return pd.Series(0.0, index=df['model_name'].unique(), name="Comprehensiveness")

        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°—á–∏—Ç–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –æ—Ç –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞, –∞ –Ω–µ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ
        total_unique_categories = self.all_results['category'].nunique()

        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –£—á–∏—Ç—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –∏–∑ –ø–æ–ª–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
        all_models_coverage = self.all_results.groupby('model_name')['category'].nunique()

        # –ü–æ–ª—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ –∏–∑ —Ç–µ–∫—É—â–µ–π –≤—ã–±–æ—Ä–∫–∏
        filtered_models = df['model_name'].unique()

        # –°–æ–∑–¥–∞–µ–º —Å–µ—Ä–∏—é –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        comprehensiveness_index = pd.Series(0.0, index=filtered_models, name="Comprehensiveness")

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–æ–∫—Ä—ã—Ç–∏–µ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
        for model in filtered_models:
            if model in all_models_coverage.index:
                comprehensiveness_index[model] = all_models_coverage[model] / total_unique_categories
            else:
                comprehensiveness_index[model] = 0.0

        return comprehensiveness_index

    def generate_leaderboard_report(self) -> str:
        if self.all_results.empty:
            return "# üèÜ –¢–∞–±–ª–∏—Ü–∞ –õ–∏–¥–µ—Ä–æ–≤\n\n–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."

        # --- –≠—Ç–∞–ø 5: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Markdown –û—Ç—á–µ—Ç–∞ ---
        timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
        report_md = f"# üèÜ –¢–∞–±–ª–∏—Ü–∞ –õ–∏–¥–µ—Ä–æ–≤ –ë–µ–Ω—á–º–∞—Ä–∫–∞\n\n"
        report_md += f"*–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {timestamp}*\n\n"

        # >>>>> –ò–ó–ú–ï–ù–ï–ù–ò–ï: –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã <<<<<
        main_results = self.all_results[self.all_results['category'] != 't_context_stress']
        if not main_results.empty:
            # ... (–≤—Å—è –≤–∞—à–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è leaderboard_df, –Ω–æ –Ω–∞ main_results)
            # –¢—É—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –≤—ã—á–∏—Å–ª—è–µ—Ç leaderboard_df
            leaderboard_df = self._calculate_leaderboard(main_results) # –ü—Ä–∏–º–µ—Ä –≤—ã–∑–æ–≤–∞
            report_md += self._to_markdown_table(leaderboard_df)
        else:
            report_md += "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã –ª–∏–¥–µ—Ä–æ–≤.\n"

        report_md += "\n---\n"

        # >>>>> –ù–û–í–´–ô –ë–õ–û–ö: –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—á–µ—Ç—ã –ø–æ —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç—É <<<<<
        context_perf_report = self._generate_context_performance_report()
        if context_perf_report:
            report_md += context_perf_report
            report_md += "\n---\n"

        heatmap_report = self._generate_heatmap_report()
        if heatmap_report:
            report_md += heatmap_report
            report_md += "\n---\n"

        # ... (—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∫–æ–¥ –¥–ª—è –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏ –∏ –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏) ...
        # –£–±–µ–¥–∏–º—Å—è, —á—Ç–æ –¥–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–æ–∂–µ –Ω–µ –≤–∫–ª—é—á–∞–µ—Ç –Ω–∞—à —Å–ø–µ—Ü. —Ç–µ—Å—Ç
        report_md += "## üìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n\n"
        if not main_results.empty:
            test_stats = main_results.groupby(['model_name', 'category'])['is_correct'].agg(['sum', 'count'])
            test_stats['Accuracy'] = (test_stats['sum'] / test_stats['count'])
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏: —Å–Ω–∞—á–∞–ª–∞ –ø–æ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏, –ø–æ—Ç–æ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Ç–æ—á–Ω–æ—Å—Ç–∏
            test_stats.sort_values(by=['model_name', 'Accuracy'], ascending=[True, False], inplace=True)
            test_stats.rename(columns={'sum': '–£—Å–ø–µ—à–Ω–æ', 'count': '–ü–æ–ø—ã—Ç–æ–∫'}, inplace=True)
            test_stats['Accuracy'] = test_stats['Accuracy'].map(lambda x: f"{x:.0%}")
            report_md += self._to_markdown_table(test_stats[['–ü–æ–ø—ã—Ç–æ–∫', '–£—Å–ø–µ—à–Ω–æ', 'Accuracy']])
            report_md += "\n> _–≠—Ç–∞ —Ç–∞–±–ª–∏—Ü–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑—Ä–µ–∑–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π._"
        else:
            report_md += "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏.\n"

        return report_md
