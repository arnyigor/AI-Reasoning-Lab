import logging
import math
import re
import time
from pathlib import Path
from typing import Tuple

import pandas as pd

log = logging.getLogger(__name__)


def wilson_score_interval(
        successes: int,
        total: int,
        confidence: float = 0.95
) -> Tuple[float, float]:
    if total == 0:
        return 0.0, 1.0
    z = 1.959963984540054
    p_hat = float(successes) / total
    part1 = p_hat + (z * z) / (2 * total)
    part2 = z * math.sqrt((p_hat * (1 - p_hat)) / total + (z * z) / (4 * total * total))
    denominator = 1 + (z * z) / total
    lower_bound = (part1 - part2) / denominator
    upper_bound = (part1 + part2) / denominator
    return lower_bound, upper_bound


class Reporter:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—ã—Ä—ã–µ JSON-—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∏—Ö —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    –∏ –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π, —Å–∞–º–æ–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–µ–º—ã–π –æ—Ç—á–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Markdown.
    """

    def __init__(self, results_dir: Path):
        self.results_dir = results_dir
        self.history_path = self.results_dir.parent / "history.json"
        self.all_results: pd.DataFrame = self._load_all_results()

    def _load_all_results(self) -> pd.DataFrame:
        all_data = []
        json_files = sorted(list(self.results_dir.glob("*.json")))
        log.info("–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ—Ç—á–µ—Ç–∞: %d", len(json_files))
        for json_file in json_files:
            try:
                data = pd.read_json(json_file)
                if not data.empty:
                    all_data.append(data)
            except Exception as e:
                log.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ %s: %s", json_file, e)
        if not all_data:
            log.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞.")
            return pd.DataFrame()
        combined_data = pd.concat(all_data, ignore_index=True)
        log.info("–í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: %d", len(combined_data))
        return combined_data

    def _load_history(self) -> pd.DataFrame:
        if not self.history_path.exists():
            log.info("–§–∞–π–ª –∏—Å—Ç–æ—Ä–∏–∏ '%s' –Ω–µ –Ω–∞–π–¥–µ–Ω. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ–≤–æ–¥–∏—Ç—å—Å—è –Ω–µ –±—É–¥–µ—Ç.", self.history_path.name)
            return pd.DataFrame()
        try:
            history_df = pd.read_json(self.history_path, orient='index')
            log.info("‚úÖ –§–∞–π–ª –∏—Å—Ç–æ—Ä–∏–∏ '%s' —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è.", self.history_path.name)
            return history_df
        except Exception as e:
            log.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª –∏—Å—Ç–æ—Ä–∏–∏ %s: %s", self.history_path, e)
            return pd.DataFrame()

    def _save_history(self, metrics: pd.DataFrame):
        history_data = metrics[['Trust_Score', 'Accuracy', 'Total_Runs']].copy()
        try:
            history_data.to_json(self.history_path, orient='index', indent=4)
            log.info("‚úÖ –§–∞–π–ª –∏—Å—Ç–æ—Ä–∏–∏ '%s' –æ–±–Ω–æ–≤–ª–µ–Ω –∞–∫—Ç—É–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.", self.history_path.name)
        except Exception as e:
            log.error("–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∞–π–ª –∏—Å—Ç–æ—Ä–∏–∏ %s: %s", self.history_path, e)

    def _to_markdown_table(self, df: pd.DataFrame) -> str:
        if df.empty:
            return "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.\n"
        try:
            return df.fillna("N/A").to_markdown() + "\n"
        except ImportError:
            log.error("–î–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ Markdown-—Ç–∞–±–ª–∏—Ü —Ç—Ä–µ–±—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ 'tabulate'. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ–µ: pip install tabulate")
            return "–û—à–∏–±–∫–∞: –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ 'tabulate' –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.\n"

    def _calculate_verbosity(self, df: pd.DataFrame) -> pd.Series:
        # –†–∞–±–æ—Ç–∞–µ–º —Å –∫–æ–ø–∏–µ–π, —á—Ç–æ–±—ã –Ω–µ –∏–∑–º–µ–Ω—è—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π DataFrame (self.all_results)
        df_copy = df.copy()

        def get_clean_len(text: str) -> int:
            if not isinstance(text, str): return 0
            pattern = re.compile(r"\b–û–ë–†–ê–ë–û–¢–ê–ù–û\b.*?\b–ì–õ–ê–°–ù–´–•\b.*?\d+", re.DOTALL | re.IGNORECASE)
            match = pattern.search(text)
            return len(match.group(0)) if match else 0

        df_copy['raw_len'] = df_copy['llm_response'].str.len().fillna(0)
        df_copy['clean_len'] = df_copy['llm_response'].apply(get_clean_len)

        grouped = df_copy.groupby('model_name')
        sums = grouped[['raw_len', 'clean_len']].sum()

        model_verbosity = (sums['raw_len'] - sums['clean_len']) / sums['raw_len']

        # –Ø–≤–Ω–æ –¥–∞–µ–º –∏–º—è Series, –∫–æ—Ç–æ—Ä–æ–µ —Å—Ç–∞–Ω–µ—Ç –∏–º–µ–Ω–µ–º –∫–æ–ª–æ–Ω–∫–∏
        return model_verbosity.fillna(0).rename("Verbosity_Index")

    def _calculate_comprehensiveness(self, df: pd.DataFrame) -> pd.Series:
        if 'category' not in df.columns or df['category'].nunique() == 0:
            return pd.Series(0.0, index=df['model_name'].unique(), name="Comprehensiveness")

        model_categories_count = df.groupby('model_name')['category'].nunique()
        total_unique_categories = df['category'].nunique()

        comprehensiveness_index = model_categories_count / total_unique_categories

        return comprehensiveness_index.rename("Comprehensiveness")

    def generate_leaderboard_report(self) -> str:
        if self.all_results.empty:
            return "# üèÜ –¢–∞–±–ª–∏—Ü–∞ –õ–∏–¥–µ—Ä–æ–≤\n\n–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞."

        # --- –≠—Ç–∞–ø 1: –ê–≥—Ä–µ–≥–∞—Ü–∏—è –≤—Å–µ—Ö –º–µ—Ç—Ä–∏–∫ ---
        metrics = self.all_results.groupby('model_name').agg(
            Successes=('is_correct', 'sum'),
            Total_Runs=('is_correct', 'count'),
            Avg_Time_ms=('execution_time_ms', 'mean')
        )
        verbosity = self._calculate_verbosity(self.all_results)
        comprehensiveness = self._calculate_comprehensiveness(self.all_results)

        # >>>>> –ò–ó–ú–ï–ù–ï–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑—É–µ–º .join() –¥–ª—è –Ω–∞–¥–µ–∂–Ω–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –ø–æ –∏–Ω–¥–µ–∫—Å—É <<<<<
        metrics = metrics.join(verbosity).join(comprehensiveness)

        # --- –≠—Ç–∞–ø 2: –†–∞—Å—á–µ—Ç –∫–ª—é—á–µ–≤—ã—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π ---
        metrics['Accuracy'] = (metrics['Successes'] / metrics['Total_Runs']).fillna(0)
        metrics['Trust_Score'] = metrics.apply(
            lambda row: wilson_score_interval(int(row['Successes']), int(row['Total_Runs']))[0],
            axis=1
        )

        # --- –≠—Ç–∞–ø 3: –†–∞–±–æ—Ç–∞ —Å –∏—Å—Ç–æ—Ä–∏–µ–π ---
        history_df = self._load_history()
        metrics['Accuracy_Change'] = 0.0
        if not history_df.empty:
            metrics = metrics.join(history_df.add_suffix('_prev'))
            metrics['Accuracy_Change'] = (metrics['Accuracy'] - metrics['Accuracy_prev']).fillna(0)

        # --- –≠—Ç–∞–ø 4: –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã ---
        def format_with_indicator(value, change, format_str):
            indicator, change_str = "", ""
            threshold = 0.001
            if change > threshold: indicator, change_str = " ‚ñ≤", f" (+{change:.1%})"
            elif change < -threshold: indicator, change_str = " ‚ñº", f" ({change:.1%})"
            elif 'Accuracy_prev' in metrics.columns: indicator = " ‚ñ¨"
            return f"{value:{format_str}}{indicator}{change_str}"

        metrics.sort_values(by='Trust_Score', ascending=False, inplace=True)
        metrics.insert(0, '–†–∞–Ω–≥', range(1, len(metrics) + 1))

        leaderboard_df = pd.DataFrame()
        leaderboard_df['–†–∞–Ω–≥'] = metrics['–†–∞–Ω–≥']
        leaderboard_df['–ú–æ–¥–µ–ª—å'] = metrics.index
        leaderboard_df['Trust Score'] = metrics['Trust_Score'].map(lambda x: f"{x:.3f}")
        leaderboard_df['Accuracy'] = metrics.apply(lambda row: format_with_indicator(row['Accuracy'], row['Accuracy_Change'], '.1%'), axis=1)
        leaderboard_df['Coverage'] = metrics['Comprehensiveness'].map(lambda x: f"{x:.0%}")
        leaderboard_df['Verbosity'] = metrics['Verbosity_Index'].map(lambda x: f"{x:.1%}")
        leaderboard_df['Avg Time'] = metrics['Avg_Time_ms'].map(lambda x: f"{x:,.0f} –º—Å")
        leaderboard_df['Runs'] = metrics['Total_Runs']
        leaderboard_df.set_index('–†–∞–Ω–≥', inplace=True)

        self._save_history(metrics)

        # --- –≠—Ç–∞–ø 5: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Markdown –û—Ç—á–µ—Ç–∞ ---
        timestamp = time.strftime('%Y-%m-%d %H:%M:%S')
        report_md = f"# üèÜ –¢–∞–±–ª–∏—Ü–∞ –õ–∏–¥–µ—Ä–æ–≤ –ë–µ–Ω—á–º–∞—Ä–∫–∞\n\n"
        report_md += f"*–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {timestamp}*\n\n"
        report_md += self._to_markdown_table(leaderboard_df)
        report_md += "\n---\n"

        report_md += "## üéØ –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è –†–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è\n\n"
        report_md += "–†–µ–π—Ç–∏–Ω–≥ —Å—Ç—Ä–æ–∏—Ç—Å—è –Ω–∞ –æ—Å–Ω–æ–≤–µ **Trust Score** ‚Äî —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ –Ω–∞–¥–µ–∂–Ω–æ–π –º–µ—Ç—Ä–∏–∫–µ, –∫–æ—Ç–æ—Ä–∞—è —É—á–∏—Ç—ã–≤–∞–µ—Ç –Ω–µ —Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω–æ—Å—Ç—å, –Ω–æ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ —Å—Ä–∞–≤–Ω–∏–≤–∞—Ç—å –º–æ–¥–µ–ª–∏, –ø—Ä–æ—à–µ–¥—à–∏–µ —Ä–∞–∑–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å–ø—ã—Ç–∞–Ω–∏–π.\n\n"
        report_md += "**Trust Score** ‚Äî —ç—Ç–æ –Ω–∏–∂–Ω—è—è –≥—Ä–∞–Ω–∏—Ü–∞ 95% –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞ –£–∏–ª—Å–æ–Ω–∞. –ü—Ä–æ—â–µ –≥–æ–≤–æ—Ä—è, —ç—Ç–æ **–º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å, –∫–æ—Ç–æ—Ä—É—é –º–æ–∂–Ω–æ –æ–∂–∏–¥–∞—Ç—å –æ—Ç –º–æ–¥–µ–ª–∏ —Å 95% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é**.\n\n"
        report_md += "> _–ü—Ä–∏–º–µ—Ä: –ú–æ–¥–µ–ª—å —Å —Ç–æ—á–Ω–æ—Å—Ç—å—é 100% –Ω–∞ 2 —Ç–µ—Å—Ç–∞—Ö –±—É–¥–µ—Ç –∏–º–µ—Ç—å –Ω–∏–∑–∫–∏–π Trust Score (~0.206), –≤ —Ç–æ –≤—Ä–µ–º—è –∫–∞–∫ –º–æ–¥–µ–ª—å —Å 90% —Ç–æ—á–Ω–æ—Å—Ç–∏ –Ω–∞ 100 —Ç–µ—Å—Ç–∞—Ö –±—É–¥–µ—Ç –∏–º–µ—Ç—å –≤—ã—Å–æ–∫–∏–π Trust Score (~0.825), —á—Ç–æ —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å –µ–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤._\n\n"

        report_md += "### üìñ –ö–∞–∫ —á–∏—Ç–∞—Ç—å —Ç–∞–±–ª–∏—Ü—É –ª–∏–¥–µ—Ä–æ–≤\n\n"
        report_md += "- **–†–∞–Ω–≥**: –ò—Ç–æ–≥–æ–≤–æ–µ –º–µ—Å—Ç–æ –º–æ–¥–µ–ª–∏ –≤ —Ä–µ–π—Ç–∏–Ω–≥–µ (—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ `Trust Score`).\n"
        report_md += "- **–ú–æ–¥–µ–ª—å**: –ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ—Å—Ç–∏—Ä—É–µ–º–æ–π LLM.\n"
        report_md += "- **Trust Score**: **–ì–ª–∞–≤–Ω—ã–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è.** –ß–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ.\n"
        report_md += "- **Accuracy**: –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤. –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã `‚ñ≤ ‚ñº ‚ñ¨` –∏ —á–∏—Å–ª–æ –≤ —Å–∫–æ–±–∫–∞—Ö –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –¥–∏–Ω–∞–º–∏–∫—É —Ç–æ—á–Ω–æ—Å—Ç–∏ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º –æ—Ç—á–µ—Ç–æ–º.\n\n"
        report_md += "**–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–ù–ï –≤–ª–∏—è—é—Ç –Ω–∞ —Ä–∞–Ω–≥):**\n\n"
        report_md += "- **Coverage (–ü–æ–∫—Ä—ã—Ç–∏–µ)**: –ö–∞–∫—É—é –¥–æ–ª—é –∏–∑ **–≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ç–µ—Å—Ç–æ–≤** –ø—Ä–æ—à–ª–∞ –º–æ–¥–µ–ª—å. `100%` –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å –±—ã–ª–∞ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∞ –Ω–∞ –≤—Å–µ—Ö —Ç–∏–ø–∞—Ö –∑–∞–¥–∞—á.\n"
        report_md += "- **Verbosity (–ë–æ–ª—Ç–ª–∏–≤–æ—Å—Ç—å)**: \"–ò–Ω–¥–µ–∫—Å –±–æ–ª—Ç–ª–∏–≤–æ—Å—Ç–∏\" ‚Äî –¥–æ–ª—è \"—à—É–º–∞\" –≤ –æ—Ç–≤–µ—Ç–µ –º–æ–¥–µ–ª–∏. `0%` ‚Äî –∏–¥–µ–∞–ª—å–Ω–æ.\n"
        report_md += "- **Avg Time (–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è)**: –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ—Ç–≤–µ—Ç–∞ –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö.\n"
        report_md += "- **Runs (–ó–∞–ø—É—Å–∫–∏)**: –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∑–∞–¥–∞—á, –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—å—é.\n"

        report_md += "\n## üìä –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º\n\n"
        test_stats = self.all_results.groupby(['model_name', 'category'])['is_correct'].agg(['sum', 'count'])
        test_stats['Accuracy'] = (test_stats['sum'] / test_stats['count'])
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –¥–ª—è –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏: —Å–Ω–∞—á–∞–ª–∞ –ø–æ –∏–º–µ–Ω–∏ –º–æ–¥–µ–ª–∏, –ø–æ—Ç–æ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Ç–æ—á–Ω–æ—Å—Ç–∏
        test_stats.sort_values(by=['model_name', 'Accuracy'], ascending=[True, False], inplace=True)
        test_stats.rename(columns={'sum': '–£—Å–ø–µ—à–Ω–æ', 'count': '–ü–æ–ø—ã—Ç–æ–∫'}, inplace=True)
        test_stats['Accuracy'] = test_stats['Accuracy'].map(lambda x: f"{x:.0%}")
        report_md += self._to_markdown_table(test_stats[['–ü–æ–ø—ã—Ç–æ–∫', '–£—Å–ø–µ—à–Ω–æ', 'Accuracy']])
        report_md += "\n> _–≠—Ç–∞ —Ç–∞–±–ª–∏—Ü–∞ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –≤ —Ä–∞–∑—Ä–µ–∑–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π._"

        return report_md