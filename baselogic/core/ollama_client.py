import json
import logging
import os
from collections.abc import Iterable
from typing import Any, Dict, List, Optional, Union

import requests
from dotenv import load_dotenv

from .interfaces import (
    ProviderClient,
    LLMConnectionError, LLMRequestError, LLMResponseError, LLMTimeoutError
)

log = logging.getLogger(__name__)

def str_to_bool(value: str) -> bool:
    return value.lower() in ("true", "1", "yes", "on")

class OllamaClient(ProviderClient):
    """
    –ß–∏—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è ProviderClient –¥–ª—è –Ω–∞—Ç–∏–≤–Ω–æ–≥–æ API Ollama,
    –∏—Å–ø–æ–ª—å–∑—É—é—â–∞—è —ç–Ω–¥–ø–æ–∏–Ω—Ç /api/chat.
    """
    def __init__(self):
        # –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å:
        self._load_env_file()           # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º .env —Ñ–∞–π–ª
        self.use_params = str_to_bool(os.environ.get("OLLAMA_USE_PARAMS", "false"))

        if self.use_params:
            self._load_ollama_environment() # 2. –î–µ—Ñ–æ–ª—Ç—ã –¥–ª—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö

        self.endpoint = "http://localhost:11434/api/chat"
        self.session = requests.Session()
        self.session.headers.update({"Content-Type": "application/json"})
        log.info("Ollama –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –∏–∑ .env")

    def _load_env_file(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –∏–∑ .env —Ñ–∞–π–ª–∞"""
        try:
            load_dotenv()  # –ó–∞–≥—Ä—É–∂–∞–µ—Ç .env –≤ os.environ
            log.info("‚úÖ .env —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            log.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å .env —Ñ–∞–π–ª: {e}")

    def _load_ollama_environment(self):
        """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–µ—Ñ–æ–ª—Ç–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö"""
        ollama_settings = {
            # –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            'OLLAMA_NUM_PARALLEL': '1',
            'OLLAMA_MAX_LOADED_MODELS': '1',
            'OLLAMA_CPU_THREADS': '6',
            'OLLAMA_FLASH_ATTENTION': 'false',
            'OLLAMA_KEEP_ALIVE': '5m',

            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ GPU/–ø–∞–º—è—Ç–∏
            'OLLAMA_GPU_LAYERS': '999',      # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–µ–≤ –≤ GPU
            'OLLAMA_CONTEXT_SIZE': '4096',   # –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            'OLLAMA_NUM_GPU': '1',           # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ GPU
            'OLLAMA_LOW_VRAM': 'false',      # –†–µ–∂–∏–º —ç–∫–æ–Ω–æ–º–∏–∏ VRAM
            'OLLAMA_USE_NUMA': 'false',      # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ NUMA

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            'OLLAMA_GPU_SPLIT_MODE': '0',    # –†–µ–∂–∏–º —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è GPU
            'OLLAMA_OFFLOAD_KQV': 'true',    # –í—ã–≥—Ä—É–∑–∫–∞ KQV –≤ GPU
        }

        for key, default_value in ollama_settings.items():
            current_value = os.environ.get(key)
            if current_value is None:
                os.environ[key] = default_value
                log.info(f"üîß –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –¥–µ—Ñ–æ–ª—Ç {key}={default_value}")
            else:
                log.info(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑ .env: {key}={current_value}")


    def prepare_payload(self, messages: List[Dict[str, str]], model: str, *, stream: bool = False, **kwargs: Any) -> Dict[str, Any]:
        top_level_args = {'format', 'keep_alive', 'think'}
        payload = {"model": model, "messages": messages, "stream": stream}
        options = {}

        # –ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω —Ä–µ–∂–∏–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏–∑ .env
        if self.use_params:
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
            env_options = {
                'num_ctx': int(os.environ.get('OLLAMA_CONTEXT_SIZE', '4096')),
                'num_gpu': int(os.environ.get('OLLAMA_NUM_GPU', '1')),
                'num_thread': int(os.environ.get('OLLAMA_CPU_THREADS', '6')),
                'low_vram': str_to_bool(os.environ.get('OLLAMA_LOW_VRAM', 'false')),
                'numa': str_to_bool(os.environ.get('OLLAMA_USE_NUMA', 'false')),
                'flash_attn': str_to_bool(os.environ.get('OLLAMA_FLASH_ATTENTION', 'false')),
            }

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ options —Ç–æ–ª—å–∫–æ –Ω–µ-None –∑–Ω–∞—á–µ–Ω–∏—è
            for key, value in env_options.items():
                if value is not None:
                    options[key] = value

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        for key, value in kwargs.items():
            if value is None:
                continue
            if key in top_level_args:
                payload[key] = value
            else:
                options[key] = value  # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–µ—Ä–µ–∑–∞–ø–∏—Å—ã–≤–∞—é—Ç env

        if options:
            payload['options'] = options

        return {k: v for k, v in payload.items() if v is not None}


    def send_request(self, payload: Dict[str, Any]) -> Union[Dict[str, Any], Iterable[Dict[str, Any]]]:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ API, –≥–µ–Ω–µ—Ä–∏—Ä—É—è –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –∏ —Ç–∏–ø–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏—Å–∫–ª—é—á–µ–Ω–∏—è.
        """
        is_stream = payload.get("stream", False)
        # –¢–∞–π–º–∞—É—Ç –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∞—Å—Ç—å—é API Ollama, –ø–æ—ç—Ç–æ–º—É –µ–≥–æ –Ω—É–∂–Ω–æ —É–¥–∞–ª–∏—Ç—å –∏–∑ payload
        timeout = payload.pop('timeout', 180)

        log.info("–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –Ω–∞ %s (stream=%s)...", self.endpoint, is_stream)
        log.info("Payload: %s", json.dumps(payload, indent=2, ensure_ascii=False))

        try:
            resp = self.session.post(self.endpoint, json=payload, stream=is_stream, timeout=timeout)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –æ—à–∏–±–∫–∏ HTTP (4xx, 5xx)
            if not resp.ok:
                # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å –¥–µ—Ç–∞–ª—å–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –∏–∑ —Ç–µ–ª–∞ –æ—Ç–≤–µ—Ç–∞
                try:
                    error_details = resp.json()
                    # Ollama –æ–±—ã—á–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—à–∏–±–∫—É –≤ –∫–ª—é—á–µ 'error'
                    error_message = error_details.get('error', str(error_details))
                except json.JSONDecodeError:
                    error_message = resp.text.strip() # –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç –Ω–µ JSON

                # –°–æ–∑–¥–∞–µ–º –Ω–∞—à–µ –∫–∞—Å—Ç–æ–º–Ω–æ–µ, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
                raise LLMRequestError(
                    message=f"–û—à–∏–±–∫–∞ API: {error_message}",
                    status_code=resp.status_code,
                    response_text=resp.text
                )

            log.info("–ó–∞–ø—Ä–æ—Å –∫ Ollama —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω.")

            # –û–±—Ä–∞–±–æ—Ç–∫–∞ —É—Å–ø–µ—à–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
            if is_stream:
                def stream_generator():
                    try:
                        for line in resp.iter_lines():
                            if line:
                                yield json.loads(line)
                    except requests.exceptions.ChunkedEncodingError as e:
                        raise LLMResponseError(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –ø–æ—Ç–æ–∫–æ–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞: {e}") from e
                    except json.JSONDecodeError as e:
                        raise LLMResponseError(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –∏–∑ –ø–æ—Ç–æ–∫–∞: {e}") from e
                return stream_generator()
            else:
                try:
                    return resp.json()
                except json.JSONDecodeError as e:
                    raise LLMResponseError(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞: {e}") from e

        # --- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã—Ö –æ—à–∏–±–æ–∫ requests ---
        except requests.exceptions.Timeout as e:
            raise LLMTimeoutError(f"–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞ –∫ {self.endpoint} (>{timeout}s)") from e
        except requests.exceptions.ConnectionError as e:
            raise LLMConnectionError(f"–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å {self.endpoint}. –°–µ—Ä–≤–µ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.") from e
        except requests.exceptions.RequestException as e:
            # –û–±—â–∞—è –æ—à–∏–±–∫–∞ –¥–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º requests
            raise LLMConnectionError(f"–°–µ—Ç–µ–≤–∞—è –æ—à–∏–±–∫–∞ Ollama: {e}") from e

    def extract_choices(self, response: Dict[str, Any]) -> List[Dict[str, Any]]:
        return [response] if 'message' in response else []

    def extract_content_from_choice(self, choice: Dict[str, Any]) -> str:
        return choice.get("message", {}).get("content", "")

    def extract_delta_from_chunk(self, chunk: Dict[str, Any]) -> str:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ (content) –∏–ª–∏ –º—ã—à–ª–µ–Ω–∏–µ (thinking) –∏–∑ —á–∞–Ω–∫–∞.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä–æ–∫—É —Å –º—ã—à–ª–µ–Ω–∏–µ–º –≤ —Ç–µ–≥–∞—Ö <think>, –µ—Å–ª–∏ –æ–Ω–æ –µ—Å—Ç—å, –∏–Ω–∞—á–µ content.
        """
        # –ü–æ–ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å message –∏–ª–∏ delta (–¥–ª—è –ø–æ—Ç–æ–∫–æ–≤—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤)
        message = chunk.get("message") or chunk.get("delta") or {}

        # –ò–∑–≤–ª–µ–∫–∞–µ–º thinking –∏ content
        thinking_part = message.get("thinking")
        content_part = message.get("content")

        # –ï—Å–ª–∏ –µ—Å—Ç—å –º—ã—à–ª–µ–Ω–∏–µ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ —Å —Ç–µ–≥–∞–º–∏
        if thinking_part:
            return f"<think>{thinking_part}</think>"

        # –ò–Ω–∞—á–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º content, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å
        return content_part if content_part is not None else ""

    def extract_metadata_from_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –∏–ª–∏ —á–∞–Ω–∫–∞.
        –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤.
        """
        try:
            metadata = {}

            # OpenAI-style usage
            usage_stats = response.get("usage", {})
            if usage_stats:
                metadata.update({
                    "prompt_eval_count": usage_stats.get("prompt_tokens"),
                    "eval_count": usage_stats.get("completion_tokens"),
                    "total_tokens": usage_stats.get("total_tokens"),
                })

            # Ollama-style –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            ollama_fields = [
                "total_duration", "load_duration", "prompt_eval_count",
                "prompt_eval_duration", "eval_count", "eval_duration"
            ]
            for field in ollama_fields:
                if field in response:
                    metadata[field] = response[field]

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è
            additional_fields = ["model", "created", "id", "object", "system_fingerprint"]
            for field in additional_fields:
                if field in response:
                    metadata[field] = response[field]

            # –£–±–∏—Ä–∞–µ–º None –∑–Ω–∞—á–µ–Ω–∏—è
            return {k: v for k, v in metadata.items() if v is not None}

        except Exception as e:
            log.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ –æ—Ç–≤–µ—Ç–∞: {e}")
            return {}

    def extract_metadata_from_chunk(self, chunk: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ —á–∞–Ω–∫–∞. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –µ—Å–ª–∏ —á–∞–Ω–∫ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π,
        –∏–Ω–∞—á–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None.
        """
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —á–∞–Ω–∫ —Ñ–∏–Ω–∞–ª—å–Ω—ã–º (—Å–æ–¥–µ—Ä–∂–∏—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
            if chunk.get("done", False) is True:
                return self.extract_metadata_from_response(chunk)

            # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –¥—Ä—É–≥–∏–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —á–∞–Ω–∫–∞
            if chunk.get("choices", [{}])[0].get("finish_reason") is not None:
                return self.extract_metadata_from_response(chunk)

            # –î–ª—è Ollama - –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            ollama_metadata_fields = [
                "total_duration", "load_duration", "prompt_eval_count",
                "prompt_eval_duration", "eval_count", "eval_duration"
            ]
            if any(field in chunk for field in ollama_metadata_fields):
                return self.extract_metadata_from_response(chunk)

            return None

        except Exception as e:
            log.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –≤ —á–∞–Ω–∫–µ: {e}")
            return None
